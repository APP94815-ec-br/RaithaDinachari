

K8s:






Day1:- 

Containerization: Docker, Rocket (rkt), Container-D, Core OS, CRI-O
Container Orchestration: - Docker Swarm, K8s, Openshift, Mesios

What is Kubernetes?
------------------------------
•	Kubernetes is an orchestration engine and open source platform for managing containerized applications.
•	Responsibilities include container deployment, scaling & descaling of containers & container load.
•	Kubernetes can be considered as a replacement for Docker Swarm.
•	Born in Google, written in Go/Go lang. Donated to CNCF (Cloud native computing foundation) in 2014.
•	Kubernetes v1.0 was released on July 21, 2015.
•	Current stable release v1.18.0.

Kubernetes Features: -
--------------------------------
1.	Automated Scheduling
2.	Self-Healing Capabilities
3.	Automated Roll-outs and Roll-back
4.	Horizontal Scaling and Load balancing
5.	Service Discovery and Load Balancing
6.	Storage Orchestration

K8s is deprecating Docker after 1.20+

Kubernetes Architecture: -
-------------------------------------
Client -> Kubectl or User Interface
Master -> API Server, ETCD, Scheduler, Control Managers (Container Run Time (container-D))
Worker -> Kubelet, Kubeproxy ((Container Run Time (container-D)))








Web UI (Dashboard):- 
Dashboard is a web based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster itself along with its available reso urc es.

Kubectl:- 
Kubectl is a command line configuration tool (CLI) for Kubernetes used to interact with master node of kubernetes. Kubectl has a config file called kubeconfig, this file has the information about server and authentication information to access the API Server.

Master Node: -
---------------------

The master node is responsible for the management of Kubernetes cluster.

Master Components: -

1. API Server: Kube API Server interacts with API, It’s a frontend of the kubernetes control plane.   
    Communication center for developers, sysadmin and other Kubernetes components.

2. ETCD: - etcd is a simple distribute key value store (K8s Database). It stores information of Nodes, Pods,   
    Deployments, Services, ConfigMaps etc.

2. Scheduler: Scheduler watches the pods and assigns the pods to run on specific hosts.

4. Controller manager: - Controller manager runs the controllers in background which runs different tasks   
    in Kubernetes cluster. Performs cluster level functions (replication, keeping track of worker nodes,    
    handling nodes failures…).

Some of the controllers are,
1. Node controller
2. Replication controllers
3. Endpoint controllers 
4. Replicaset controllers 
5. Deployment controller 
6. Daemonsets controller 
7. Jobs controller

Youtube video – K8s deprecating docker.




1. Request Goes to API Server
2. API Server validates the requests
3. API Server Persists (save) the details in ETCD.
4. Scheduler will find Unscheduled POD if any from ETCD, then it will assign nodes to that pod

Worker Nodes: -
-------------------------
Worker nodes are the nodes where the application actually running in kubernetes cluster.

Node Components

1. Kubelet: - 
Kubelet is the primary node agent runs on each nodes and reads the container manifests which ensures that containers are running and healthy.
It makes sure that containers are running in a pod. The kubelet doesn’t manage containers which were not created by Kubernetes.

2. Kube proxy: -
•	kube proxy enables the Kubernetes service abstraction by maintaining network rules on the host and performing connection forwarding.
•	kube proxy maintains network rules on nodes. These network rules allow network communication to your Pods from inside or outside of your cluster.
•	It helps us to have network proxy and load balancer for the services in a single worker node..
•	Service is just a logical concept, the real work is being done by the “kube proxy” pod that is running on each node.
•	It redirect requests from Cluster IP (Virtual IP Address) to Pod IP.

3. Container Runtime
Each node must have a container runtime.

Self-Managed K8s Clusters
-------------------------------------
Minicube -> Single node K8s cluster
Kubeadm -> We can setup multi node k8s cluster using kubeadm.
KubeSpray

Managed K8s Clusters
-------------------------------
1)	Creating EKS
2)	We are responsible for deploying applications and mapping its data.

EKS -> Elastic Kubernetes Service (AWS Cloud)
AKS -> Azure Kubernetes Service (Azure Cloud)
GKE -> Google Kubernetes Engine (Google Cloud)
IKE -> IBM Kubernetes Engine (IBM Cloud)

Self-Managed K8s Cluster
  kubeadm -> We can setup multi node k8s clusters.
  
  
  
Day2: -

Installation

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

Follow the Installation documentation and Install On master and workers.

 

ubuntu@ip-172-31-41-241:~$ kubectl get nodes -v=8
ubuntu@ip-172-31-41-241:~$ kubectl get nodes 


Kubernetes Master and Workers
:

sudo su -

swapoff -a

sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

apt update -y

apt install -y apt-transport-https -y

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -

cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF

apt update -y

apt install -y kubelet kubeadm  containerd kubectl

apt-mark hold kubelet kubeadm kubectl containerd


cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

modprobe overlay
modprobe br_netfilter


cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF


sysctl --system

mkdir -p /etc/containerd

containerd config default | sudo tee /etc/containerd/config.toml

systemctl restart containerd

systemctl daemon-reload 
systemctl start kubelet 
systemctl enable kubelet.service
---------------------
##only in master

kubeadm init

##Your Kubernetes control-plane has initialized successfully!


exit
#in master  as normal user
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#cat ~/.kube/config



kubectl get pods -o wide -n kube-system

kubectl get pods -n kube-system
# to start core dns newtwork cni Pod
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

----------in master  as normal user----------------
kubeadm token create --print-join-command


-----------in nodes as root user----------
copy the output of above command n run in worker nodes


-----------

to check no. of nodes

kubectl get nodes ----in master as normal user 
ubuntu@ip-172-31-4-245:~$ kubectl get nodes
NAME              STATUS   ROLES                  AGE     VERSION
ip-172-31-35-0    Ready    <none>                 7m27s   v1.22.4
ip-172-31-4-245   Ready    control-plane,master   59m     v1.22.4
ip-172-31-43-94   Ready    <none>                 12m     v1.22.4



Install and Set Up kubectl on Linux

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"


sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
kubectl version --client

ubuntu -- user

ubuntu@ip-172-31-47-121:~$ mkdir -p $HOME/.kube  

#cat ~/.kube/config  from master node
ubuntu@ip-172-31-47-121:~$ vi ~/.kube/config


-------

kubectl get ns
kubectl run mavenwebapp --image=harishkumarbr/maven-web-application:1 --port=8080 -n test-ns
kubectl run javawebapp --image=harishkumarbr/java-web-app-1.0 --port=8080 -n test-ns


https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#expose

Containerization --> Docker, Rocket(Rkt),Container-d
Container Orchestration Tools --> Docker Swarm,Kubernetes,OpenShift


Installation
============

Self Managed K8's Cluster
 minikube --> Single Node K8's Cluster.
 kubeadm --> We can setup multi node k8's cluster using kubeadm.


Cloud Managed(Managed Services)
EKS --> Elastic Kubernetes Service(AWS)
AKS --> Azure Kubernetes Service(Azure)
GKE --> Google Kubernetes Engine(GCP)

KOPS --> Kubernetes Operations is a sotware using which we can create production ready
highily available kubenetes services in Cloud like AWS.KOPS will leverage Cloud Sevices like
AWS AutoScaling & Lanuch Configurations to setup K8's Master & Workers. It will Create 2 ASG & Lanuch Configs
one for master and one for worekrs. Thesse Auto Scaling Groups will manage EC2 Instances.

https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-strong-getting-started-strong


Name Spaces

kubectl get namespaces

# Create Name Space Using Imperative Command

kubectl create namespace <nameSpaceName>

ex:
kubectl create test-ns


# Using Declarative Manifest file

apiVersion: v1
kind: Namespace
metadata:
 name: <NameSpaceName>
 lables:           # Labels are key value pairs(Metadata)
   <key>: <value>
   <key>  <value>

# Example   
apiVersion: v1
kind: Namespace
metadata:
  name: test-ns

# Command to apply  
kubectl apply -f <fileName>.yaml


# Create POD Using Command

kubectl run <podName> --image=<imageName> --port=<containerPort> -n <namespaceName>

ex:
# If we don't mention name space it will create in default(current) namespace.
kubectl run javawebapp --image=dockerhandson/java-web-app:1 --port=8080

# List pods from current(default) ns
kubect get pods

# List pods from given  ns
kubect get pods -n <namespaceName>

ex:
kubect get pods -n ftest-ns



Kubernetes Objects Examples:

POD

Replication Controller

Replica Set

DaemonSet

Deployment
Statefullset

Service

PersistentVolume

PersistentVolumeClaim

CofgigMap

Secret ..etc

# POD Manifest

apiVersion: v1
kind: Pod
metadata:
  name: <PodName>
  labels:
    <Key>: <value>
  namespace: <nameSpaceName>
spec:
  containers:
  - name: <NameOfTheCotnainer>
    image: <imagaName>
	ports:
	- containerPort: <portOfContainer>
	
Example:
---	
apiVersion: v1
kind: Pod
metadata:
  name: mavenwebapppod
  labels:
    app:  mavenwebapp
    namespace: test-ns	
spec:
  containers:
  - name: mavenwebappcontainer
    image: dockerhandson/maven-web-application:1
    ports:
    - containerPort: 8080


kubectl apply -f <fileName.yml>

kubectl get all 
kubectl get pods 
kubectl get pods --show-labels
kubectl get pods -o wide
kubectl get pods -o wide --show-labels

kubectl  describe pod <podName>
kubectl  describe pod <podName> -n <namespace>


Note: If we don't mention -n <namespace> it will refer default namespace.
If required we can change name space context.

kubctl config set-context --curent --namespace=<namespace>
ex:
kubectl config set-context --curent --namespace=test-ns

After setting context  by default it will point to that namespace.


Change it to default namespace again if required
ex:
kubectl config set-context --curent --namespace=default

# Multi Container POD
apiVersion: v1
kind: Pod
metadata:
  name:  <PODName>
  namespace: <nameSpaceName>
  labels:
    <labelKey>: <labelValue> 
spec:
  containers:
  - name: <nameOftheCotnainer>
    image: <imageName>
	ports:
	- containerPort: <portNumberOfContainer>
  - name: <nameOftheCotnainer>
    image: <imageName>
    ports:
    - containerPort: <portNumberOfContainer>
	


K8's Service   ---> In Kubernetes Service makes our pods accessable/discoverable with in the cluster or exposing them to internat.
               		service will identify pods using it's labels And Selector. Whenever we create a service a ClusterIP (virtual IP) Address will be allocated for that serivce and DNS 					entry will be created for that IP. So internally we can access using service name(DNS).
			






kubectl run mavenwebapp --image=harishkumarbr/maven-web-application:22 --port=8080 -n test-ns

kubectl get pods -n test-ns

kubectl config current-context

kubectl config set-context --namespace=test-ns --current


kubectl config set-context --namespace=default --current


kubectl get nodes   -n test-ns -o wide

kubectl describe pods mavenwebapp -n test-ns



curl -Lv 10.36.0.1:8080/maven-web-application
			
	
Service
========
apiVersion: v1
kind: Service
metadata:
  name: <serviceName>
  namespace: <nameSpace>
spec:
  type: <ClusterIP/NodePort>
  selector:
     <key>: <value>
  ports:
  - port: <servciePort>	# default It to 80
    targetPort: <containerPort> 
	

With in Cluster ClusterIP
==========================
apiVersion: v1
kind: Service
metadata:
  name: mavenwebappservice
  namespace: test-ns  
spec:
  type: ClusterIP
  selector:
     run: mavenwebapp
  ports:
  - port: 80
    targetPort: 8080
	

kubectl get svc -n test-ns
kubectl get service -n test-ns

kubectl expose pod <PodName> --port=80 target-Port=8080 --type=ClusterIP -n test-ns
	kubectl  get ep -n test-ns
	



+=======create one more pod


apiVersion: v1
kind: Pod
metadata:
  name:  <PODName>
  namespace: <nameSpaceName>
  labels:
    <labelKey>: <labelValue> 
spec:
  containers:
  - name: <nameOftheCotnainer>
    image: <imageName>
	ports:
	- containerPort: <portNumberOfContainer>
 
	-----------------------------
apiVersion: v1
kind: Pod
metadata:
  name: pythonapp
  namespace: test-ns
  labels:
    app: pythonapp

spec:
  containers:
  - name: pythonapp
    image: dockerhandson/python-flask-app:1
    ports:
     - containerPort: 5000
---
apiVersion: v1
kind: Service
metadata:
  name: pythonapp
  namespace: test-ns
spec:
  type: ClusterIP
  selector:
     app: pythonapp
  ports:
  - port: 80
    targetPort: 5000

	
----------------------------------	
	
	kubectl exec -i -t pythonapp /bin/bash -n test-ns
    
    curl -L mavenwebapp/maven-web-application

	kubectl edit servcie mavenwebapp  -n test-ns
Out side of Cluster Node Port
========================
apiVersion: v1
kind: Service
metadata:
  name: javawebappservice
spec:
  type: NodePort
  selector:
     app: javawebapp
  ports:
  - port: 80
    targetPort: 8080
    
    
	nodePort: 30033 # This Optional if u don't mention nodePort.Kuberetes will assign.
	

kubectl apply -f <file.yml>

kubectl get svc 
kubectl get all

kubectl  describe service <serviceName>
kubectl  describe service <serviceName> -n <namespace>
kubectl  describe service <serviceName> -o wide


What is node port range?
30000-32767


kubectl get all --all-namespaces
kubectl get all -n <namespace>
kubectl get pods -n <namespace>
kubectl get pods -n <namespace> - o wide

kubectl get svc -n <namespace>

ACCESS OUTSIDE USING NODEIP:NODEPORT.


With in the cluster one application(POD) can access other applications(PODS) using Service name.

What is FQDN?
Fully Qualified Domain name. 
If one POD need access to service & which are in differnent names space we have to use FQDN of the serivce.
Syntax: <serivceName>.<namespace>.svc.cluster.local
ex: mavenwebappservice.test-ns.svc.cluster.local




POD --> Pod is the smallest building block which we can deploy in k8s.Pod represents running process.Pod contains one or more containers.These container will share same network,storage and any other specifications.Pod will have unique IP Address in k8s cluster.

Pods
 SingleContainerPods --> Pod will have only one container.
 
 MultiContainerPods(SideCar) --> POD will two or more contianers.
 
We should not create pods directly for deploying applications.If pod is down it wont be rescheduled.

We have to create pods with help of controllers.Which manages POD life cycle.




					


Controllers
===========

ReplicationController
ReplicaSet
DaemonSet
Deploymnet
StatefullSet





# Replication Conrtoller
apiVersion: v1
kind: ReplicationController
metadata:
  name: <replicationControllerName>
  namespace: <nameSpaceName>
spec:
  replicas: <noOfReplicas>
  selector:
    <key>: <value>
  template: # POD Template
    metadata:
	  name: <PODName>
	  labels:
	    <key>: <value>
	spec:
	- containers:
	  - name: <nameOfTheContainer>
	    image: <imageName>
		ports:
		- containerPort: <containerPort>
  -----
  
Example:
========
apiVersion: v1
kind: ReplicationController
metadata:
  name: javawebapprc
spec:
  replicas: 1
  selector:
    app: javawebapp
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappservice
spec:
  type: NodePort
  selector:
     app: javawebapp
  ports:
  - port: 80
    targetPort: 8080		

# Another Appplication
apiVersion: v1
kind: ReplicationController
metadata:
  name: pythonrc
spec:
  replicas: 1
  template: # Pod template
    metadata:
      name: pythonapppod
      labels:
        app: pythonapp
    spec:
      containers:
      - name: pythonappcontainer
        image: dockerhandson/python-app:1
        ports:
        - containerPort: 5000
---
apiVersion: v1
kind: Service
metadata:
  name: pythonsvc
spec:
  type: NodePort
  selector:
    app: pythonapp
  ports:
  - port: 80
    targetPort: 5000
	
    
    
    
    
    
    -
    
    ++-------------------
    
apiVersion: v1
kind: ReplicationController
metadata:
  name: javawebapprc
  namespace: test-ns
  #lables:
    #app: javawebapp
spec:
  replicas: 1
  selector:
    app: javawebapp

  template:
    metadata:
      name: javawebapp
      lables:
        app: javawebapp
    spec:
      containers:
       - name: javawebapp
        image: harishkumarbr/java-web-app-docker:10
        ports:
         - containerPort: 8080
          name: accessport
     # - name: javawebapp
      #  image: harishkumarbr/java-web-app-docker:10
       # ports:
        # - containerPort: 8080        
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
  namespace: test-ns
spec:
  selector:
    app: javawebapp
  type: NodePort
  ports:
   - port: 80
     targetport: 8080
     nodePort: 30790
     
     
	
kubectl apply -f <filename.yml>
kubectl get rc 
kubectl get rc -n <namespace>
kubectl get all
kubectl scale rc <rcName> --replicas <noOfReplicas>

kubectl describe rc <rcName>
kubectl delete rc <rcName>




Replicaset:


apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: pythonapprs
  namespace: test-ns
spec:
  replicas: 1
  selector: ## mandatory 
    matchLabels:
      app: pythonapp
  template:
    metadata:
      name: pythonapppod
      labels:
        app: pythonapp
    spec:
      containers:
      - name: pythonappcontainer
        image: dockerhandson/python-flask-app:1
        ports:
        - containerPort: 5000
---
apiVersion: v1
kind: Service
metadata:
  name: pythonapp
  namespace: test-ns
spec:
  type: NodePort
  selector:
     app: pythonapp
  ports:
  - port: 80
    targetPort: 5000
    
    
    
    DaemonSet
   

piVersion: apps/v1
kind: DaemonSet
metadata:
  name: pythonappds
  namespace: test-ns
spec:
  replicas: 1
  selector: ## mandatory 
    matchLabels:
      app: pythonapp
  template:
    metadata:
      name: pythonapppod
      labels:
        app: pythonapp
    spec:
      containers:
      - name: pythonappcontainer
        image: dockerhandson/python-flask-app:1
        ports:
        - containerPort: 5000
---
apiVersion: v1
kind: Service
metadata:
  name: pythonapp
  namespace: test-ns
spec:
  type: NodePort
  selector:
     app: pythonapp
  ports:
  - port: 80
    targetPort: 5000
       
    -------------------
    
    Deployment**********
    
    piVersion: apps/v1
kind: Deployment
metadata:
  name: pythonapprs
  namespace: test-ns
spec:
  replicas: 1
  strategy:
    type: ReCreate
  selector: ## mandatory 
    matchLabels:
      app: pythonapp
  template:
    metadata:
      name: pythonapppod
      labels:
        app: pythonapp
    spec:
      containers:
      - name: pythonappcontainer
        image: dockerhandson/python-flask-app:1
        ports:
        - containerPort: 5000
---
apiVersion: v1
kind: Service
metadata:
  name: pythonapp
  namespace: test-ns
spec:
  type: NodePort
  selector:
     app: pythonapp
  ports:
  - port: 80
    targetPort: 5000
    



ReplicaSet:

What is difference b/w replicaset and replication controller?

It's next gernation of replication controller. Both manages the pod replicas. But only difference as now is
selector support.

RC --> Supports only equality based selectors.

key == value(Equal Condition)
selector:
    app: javawebapp

RS --> Supports eqaulity based selectors and also set based selectors.


key == value(Equal Condition)

Set Based
key in (value1,value2,value3)
key notin (value1) 

selector:
   matchLabels: # Equality Based
     key: value
   matchExpressions: # Set Based
   - key: app
     operator: IN
	 values:
	 - javawebpp
	 - javawebapplication
	 
# Mainfest File RS

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: <RSName>
spec:
  replicas: <noOfPODReplicas>
  selector:  # To Match POD Labels.
    matchLabels:  # Equality Based Selector
	  <key>: <value>
    matchExpressions:  # Set Based Selector 
	- key: <key>
	  operator: <in/not in>
	  values:
	  - <value1>
	  - <value2>
  template:
    metadata:
	  name: <PODName>
	  labels:
	    <key>: <value>
	spec:
	- containers:
	  - name: <nameOfTheContainer>
	    image: <imageName>
		ports:
		- containerPort: <containerPort>


Example:

apiVersion: apps/v1
kind: ReplicaSet
metadata: 
  name: javawebapprs
spec: 
  replicas: 1
  selector: 
    matchLabels: 
      app: javawebapp
  template: 
    metadata: 
	  name: javawebapppod
      labels: 
        app: javawebapp
    spec: 
      containers: 
      - image: dockerhandson/java-web-app:1
        name: javawebappcontainer
        ports: 
        - containerPort: 8080


kubectl get rs 
kubectl get rs -n <namespace>
kubectl get all
kubectl scale rs <rsName> --replicas <noOfReplicas>

kubectl describe rs <rsName>
kubectl delete rs <rsName>






What is difference b/w kubectl create and kubectl apply ?

Create will Create an Object if it's not already created. Apply will perfrom create if object is not created earlier.If it's already
created it will update.			   
			   
			   
			   	

kubectl apply (create & update)

kubectl create -f <fileName.yml>

kubectl update -f <fileName.yml>


Change/Switch Context(NameSpace)
=================================

# View kubectl context
kubectl config view | grep namespace


# Change/Switch namespace

kubectl config set-context --current  --namespace=<namespace>




apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: <RSName>
spec:
  selector:  # To Match POD Labels.
    matchLabels:  # Equality Based Selector
	  <key>: <value>
    matchExpressions:  # Set Based Selector 
	- key: <key>
	  operator: <in/not in>
	  values:
	  - <value1>
	  - <value2>
  template:
    metadata:
	  name: <PODName>
	  labels:
	    <key>: <value>
	spec:
	- containers:
	  - name: <nameOfTheContainer>
	    image: <imageName>
		ports:
		- containerPort: <containerPort>


apiVersion: apps/v1
kind: DaemonSet
metadata: 
  name: mavenwebappds
spec: 
  selector: 
    matchLabels: 
      app: mavenwebapp
  template: 
    metadata: 
	  name: mavenwebapppod
      labels: 
        app: mavenwebapp
    spec: 
      containers: 
      - image: dockerhandson/maven-web-app
        name: mavenwebappcontainer
        ports: 
        - containerPort: 8080
		
kubectl get ds 
kubectl get ds -n <namespace>
kubectl get all


kubectl describe ds <dsName>
kubectl delete ds <dsName>





# Deployment ReCreate
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: javawebapp
  strategy:
    type: Recreate    
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:1
        ports:
        - containerPort: 8080

kubectl get deployment
kubectl get rs
kubectl get pods
kubectl rollout status deployment <deploymentName>
kubectl rollout history  deployment <deploymentName>
kubectl rollout history  deployment <deploymentName> --revision=1  
kubectl scale deployment <deploymentName> --replicas <noOfReplicas>

We can update deployment using yml or using command
	
	
# Update Deployment Image using command	

kubectl set image deployment <deploymentName> <containerName>=<imageNameWithVersion> --record		
ex:	
kubectl set image deployment javawebappdeployment javawebappcontainer=dockerhandson/java-web-app:2 --record		

Roll back to previous revison
kubectl rollout undo  deployment <deploymentName> --to-revision=1


# Rolling Update
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: javawebapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  minReadySeconds: 30
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:1
        ports:
        - containerPort: 8080








--------------------






apiVersion: apps/v1
kind: Deployment
metadata:
  name: mavenappdev
  namespace: test-ns
spec:
 #revisionHistoryLimit: 6
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingupdate:
      maxSurge: 1
      maxUnavailable: 50%
  minReadySeconds: 30
  selector:
    matchLabels:
      app: mavenwebapp
  template:
    metadata:
      name: mavenwebapp
      namespace: test-ns
      labels:
        app: mavenwebapp
    spec:
      containers:
       - name: mavenwebapp
         image: harishkumarbr/maven-web-application:16
         ports:
         - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: mavenwebappservice
  namespace: test-ns
spec:
  type: NodePort
  selector:
     app: mavenwebapp
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30791






        


kubectl get deployment
kubectl get rs
kubectl get pods
kubectl rollout status deployment <deploymentName>
kubectl rollout history  deployment <deploymentName>
kubectl rollout history  deployment <deploymentName> --revision 1  
kubectl rollout undo  deployment <deploymentName> --to-revision 1  
kubectl scale deployment <deploymentName> --replicas <noOfReplicas>
	
# Update Deployment Image using command	

kubectl set image deployment <deploymentName> <containerName>=<imageNameWithVersion> --record


	


POD AutoScaler
==============
What is difference b/w Kubernetes AutoScaling(POD AutoScaling) & AWS AutoScaling?


POD AutoScaling --> Kuberenets POD AutoScaling Will make sure u have minimum number pod replicas available at any time & based the observed CPU/Memory utilization on pods it can scale PODS.  HPA Will Scale up/down pod replicas of Deployment/ReplicaSet/ReplicationController based on observerd CPU & Memory utilization base the target specified.


AWS AutoScaling --> It will make sure u have enough number of nodes(Servers). Always it will maintian minimum number of nodes. Based the observed CPU/Memory utilization of node it can scale nodes.


Note: Deploy metrics server as k8s addon which will fetch metrics. Follow bellow link to deploy metrics Server.
====
https://github.com/MithunTechnologiesDevOps/metrics-server




---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpadeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      name: hpapod
  template:
    metadata:
      labels:
        name: hpapod
    spec:
      containers:
        - name: hpacontainer
          image: k8s.gcr.io/hpa-example
          ports:
          - name: http
            containerPort: 80
          resources:
            requests:
              cpu: "100m"
              memory: "64Mi"
            limits:
              cpu: "100m"
              memory: "256Mi"
---
apiVersion: v1
kind: Service
metadata:
  name: hpaclusterservice
  labels:
    name: hpaservice
spec:
  ports:
  - port: 80
    targetPort: 80
  selector:
    name: hpapod
    type: NodePort
---
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: hpadeploymentautoscaler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: hpadeployment
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 40
  - type: Resource
    resource:
     name: memory
     target:
      type: Utilization
      averageUtilization: 40
	  

# Create temp POD using below command interatively and increase the load on demo app by accessing the service.

kubectl run -i --tty load-generator --rm  --image=busybox /bin/sh


# Access the service to increase the load.

while true; do wget -q -O- http://hpaclusterservice; done






Volumes:

Kubernetes Supports different types of volumes.

hostPath
nfs
emptydir
configMap
Secret

awsElasticBlockStore
googlePersistantdisk
azureFile
azuredisk

persistantVolume
persistantVolumeClaim



## Mongo db POD with Host Path Volume##

## Spring Boot App  
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
        - name: MONGO_DB_HOSTNAME
          value: mongo 
---
apiVersion: v1
kind: Service
metadata:
  name: springapp
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
    type: NodePort
---
# Mongo db pod with volumes(HostPath)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodbdeployment
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: hostpathvol
         hostPath:
           path: /tmp/mongodb 
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: hostpathvol
           mountPath: /data/db   
---
apiVersion: v1
kind: Service
metadata:
  name: mongo
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort: 27017
	

	

Configuration of NFS Server
===========================

Step 1: 

Create one Server for NFS

Install NFS Kernel Server in 
Before installing the NFS Kernel server, we need to update our system’s repository index 
with that of the Internet through the following apt command as sudo:

$ sudo apt-get update

The above command lets us install the latest available version of a software through the Ubuntu repositories.

Now, run the following command in order to install the NFS Kernel Server on your system:

$ sudo apt install nfs-kernel-server


Step 2: Create the Export Directory

sudo mkdir -p /mnt/share/

# As we want all clients to access the directory, we will remove restrictive permissions.
sudo chown nobody:nogroup /mnt/share/
sudo chmod 777 /mnt/share/

Step 3: Assign server access to client(s) through NFS export file

sudo vi /etc/exports


#/mnt/share/ <clientIP or Clients CIDR>(rw,sync,no_subtree_check,no_root_squash)
 #Ex:
/mnt/share/ *(rw,sync,no_subtree_check,no_root_squash)

# * means any server 


Step 4: Export the shared directory

$ sudo exportfs -a



sudo systemctl restart nfs-kernel-server

Step 5: Open firewall for the client (s) PORT 2049




Configuring the Client Machines(Kubernetes Nodes)
================================================

Step 1: Install NFS Common
Before installing the NFS Common application, 
we need to update our system’s repository index with that of the Internet through the following apt command as sudo:

$ sudo apt-get update


$ sudo apt-get install nfs-common


Deploy mongdb pods using NFS Volume in k8s


#Mongo db POD with NFS Volume ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec:
 replicas: 2
 selector:
   matchLabels:
     app: springapp
 template:
   metadata:
     name: springapppod
     labels:
       app: springapp
   spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mognodbrs
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      name: mongodbpod
      labels:
        app: mongo
    spec:
      containers:
      - name: mongodbcontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
        volumeMounts:
        - name: mongodbNFS
          mountPath: /data/db
      volumes:
      - name: mongodbNFS
        nfs:
          server: 172.31.38.98  # Update NFS Server IP And Path Based on your setup
          path: /mnt/share
---
apiVersion: v1
kind: Service
metadata:
  name: mongo
spec:
  type: ClusterIP
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017
	
-=-----------------------
nfs:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
  namespace: test-ns
spec:
 replicas: 2
 selector:
   matchLabels:
     app: springapp
 template:
   metadata:
     name: springapppod
     labels:
       app: springapp
   spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30791
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      name: mongodbpod
      labels:
        app: mongo
    spec:
      containers:
      - name: mongodbcontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
        volumeMounts:
        - name: mongodbnfs
          mountPath: /data/db
      volumes:
      - name: mongodbnfs
        nfs:
          server: 172.31.47.3 # Update NFS Server IP And Path Based on your setup
          path: /mnt/share
---
apiVersion: v1
kind: Service
metadata:
  name: mongo
  namespace: test-ns
spec:
  type: ClusterIP
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017
++++++++++++++




PV --> It's a piece of storage(hostPath,nfs,ebs,azurefile,azuredisk) in k8s cluster. PV exists independently from 
from pod life cycle which is consuming.

Persistent Volumes are provisioned in two ways, Statically or Dynamically.

1) Static Volumes (Manual Provisionging)
    As a k8's Administrator will create a PV manullay so that pv's can be avilable for PODS which requires.
	Create a PVC so that PVC will be attached PV. We can use PVC with PODS to get an access to PV. 
	
2) Dynamic Volumes (Dynamic Provisioning)
     It's possible to have k8's provsion(Create) volumes(PV) as required. Provided we have configured storageClass.
	 So when we create PVC if PV is not available Storage Class will Create PV dynamically.
   

PVC
If pod requires access to storage(PV),it will get an access using PVC. PVC will be attached to PV.



PersistentVolume – the low level representation of a storage volume.
PersistentVolumeClaim – the binding between a Pod and PersistentVolume.
Pod – a running container that will consume a PersistentVolume.
StorageClass – allows for dynamic provisioning of PersistentVolumes.


PV Will have Access Modes

ReadWriteOnce – the volume can be mounted as read-write by a single node
ReadOnlyMany – the volume can be mounted read-only by many nodes
ReadWriteMany – the volume can be mounted as read-write by many nodes

In the CLI, the access modes are abbreviated to:

RWO - ReadWriteOnce
ROX - ReadOnlyMany
RWX - ReadWriteMany


Claim Policies

A Persistent Volume can have several different claim policies associated with it including

Retain – When the claim is deleted, the volume remains.
Recycle – When the claim is deleted the volume remains but in a state where the data can be manually recovered.
Delete – The persistent volume is deleted when the claim is deleted.
The claim policy (associated at the PV and not the PVC) is responsible for what happens to the data on when the claim has been deleted.


Commands

kubectl get pv
kubectl get pvc
kubectl get storageclass
kubectl describe pvc <pvcName>
kubectl describe pv <pvName>



If StorageClass is  not configued
=================================
1) Create a PV manually if not already available

2) Claim the PV by creating PVC

3) Use that PVC in your pod manfiset


If StorageClass is  configued
=============================

1) Claim the PV by creating PVC

2) Use that PVC in your pod manfiset




Find Sample PV & PVC Yml from below Git Hub

https://github.com/MithunTechnologiesDevOps/Kubernates-Manifests/tree/master/pv-pvc

Static Volumes
1) Create PV

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
spec:
  storageClassName: manual
  capacity:
	storage: 1Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
	path: "/kube"

2) Create PVC

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-hostpath
spec:
  storageClassName: manual
  accessModes:
  - ReadWriteOnce
  resources:
	requests:
	  storage: 100Mi
		   
3) Use PVC with POD in POD manifest.

# Mongo db pod with PVC
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodbrs
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: mongodb-pvc
         persistentVolumeClaim:
           claimName: pvc-hostpath   
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: mongodb-pvc
           mountPath: /data/db



Commands://
=========

kubectl get pv
kubectl get pvc

kubectl describe pv <pvName>
kubectl describe pvc <pvcName>



kubectl get storageclass

Note: Configure Storage Class for Dynamic Volumes based on infra sturcture. Make that one as default storage class.

NFS Provisioner
Prerequisiets:
1) NFS Server
2) Insall nfs client softwares in all k'8s nodes.

Find NFS Provisioner below.

https://raw.githubusercontent.com/MithunTechnologiesDevOps/Kubernates-Manifests/master/pv-pvc/nfsstorageclass.yml

Get yml from above link.

$ curl https://raw.githubusercontent.com/MithunTechnologiesDevOps/Kubernates-Manifests/master/pv-pvc/nfsstorageclass.yml >> nfsstorageclass.yml

And update Your NFS Server IP Address(2 Places you need update IP Addrees) And path of nfs share. Apply

kubectl apply -f nfsstorageclass.yml

Dynamic Volumes

Refer below link where we are creating PVC & PODS :

https://raw.githubusercontent.com/MithunTechnologiesDevOps/Kubernates-Manifests/master/SpringBoot-Mongo-DynamicPV.yml

1) Create PVC(If we don't mention storageclass name it will use default storage class which is configured.) It will create PV.

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-nfs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
	  
2) Use PVC with POD in POD manifest.

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodbrs
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: mongodb-pvc
         persistentVolumeClaim:
           claimName: mongodb-nfs-pvc   
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: mongodb-pvc
           mountPath: /data/db
		   
# Complete Manifest Where in single yml we defined Deployment & Service for SpringApp & PVC(with NFS Dynamic StorageClass),ReplicaSet & Service For Mongo.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
        - name: MONGO_DB_HOSTNAME
          value: mongo 
---
apiVersion: v1
kind: Service
metadata:
  name: springapp
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30032
  type: NodePort
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodbpvc 
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Mi
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodbrs
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: pvc
         persistentVolumeClaim:
           claimName: mongodbpvc     
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: pvc
           mountPath: /data/db   
---
apiVersion: v1
kind: Service
metadata:
  name: mongo
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort: 27017	
    
    
    
    
    
    
    
++++++++++++++++++++
Jenkins image and volume:


apiVersion: apps/v1
kind: Deployment
metadata:
  name: jenkinsappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: jenkinsapp
  template:
    metadata:
      labels:
        app: jenkinsapp
    spec:
      containers:
      - name: jenkinsappcontainer
        image: jenkins
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            cpu: "200m"
            memory: "256Mi"
        ports:
        - containerPort: 8080
        volumeMounts:
          name: jenkinsvol
          mountPath: /var/jenkins_home
      volume:
      - name:  jenkinsvol
        persistantVolumeClaim:
          claimName: jenkins_pvc        
 ---
apiVersion: v1
kind: Service
metadata:
  name: jenkinsappsvc
spec:
  type: NodePort
  selector:
    app: jenkinsapp
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: jenkins_pvc
  namespace: test-ns
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 256Mi    
    
   
    
    
    
    |++++++++++++++++++++++

    
    
    
    
    
    
    
    
    
    
    
    
Config Maps & Secrets
======================

We can create ConfigMap & Secretes in Cluster using command or also using yml.


ConfigMap Using Command
======================
kubectl create configmap springappconfig --from-literal=mongodbusername=devdb


Or Using yml

---
apiVersion: v1
kind: ConfigMap
metadata:      # We can define multiple key value pairs.
  name: springappconfig
data:
    : devdb
  
  
Secret Using Command: 

kubectl create secret generic springappsecret --from-literal=mongodbpassword=devdb@123


Using Yml:

apiVersion: v1
kind: Secret
metadata:
  name: springappsecret
type: Opaque
stringData:   # We can define multiple key value pairs.
  mongodbpassword: devdb@123    
  

In Production Cluster we can create with differnet values.

kubectl create configmap springappconfig --from-literal=mongodbusername=proddb 

apiVersion: v1
kind: ConfigMap
metadata:
  name: springappconfig
data:            # We can define multiple key value pairs.
  mongodbusername: proddb

kubectl create secret generic springappsecret --from-literal=mongodbpassword=proddb@123


Using Yml:

apiVersion: v1
kind: Secret
metadata:
  name: springappsecret
type: Opaque
stringData:   # We can define multiple key value pairs.
  mongodbpassword: prodb@123
  

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo:1
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongosvc
        env:
        - name: MONGO_DB_USERNAME
          valueFrom:
            configMapKeyRef:
              name: springappconfig
              key: mongodbusername
        - name: MONGO_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: springappsecret
              key: mongodbpassword
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            cpu: "200m"
            memory: "256Mi"
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodeployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongocontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            configMapKeyRef:
              name: springappconfig
              key: mongodbusername
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: springappsecret
              key:  mongodbpassword
        volumeMounts:
        - name: mongodbhostvol
          mountPath: /data/db
      volumes:
      - name: mongodbhostvol
        hostPath:
          path: /tmp/mongo
---
apiVersion: v1
kind: Service
metadata:
  name: mongosvc
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017




ConigMap As Volume Example
=========================

# ConfigMap with file data

apiVersion: v1
kind: ConfigMap
metadata:
  name: javawebappconfig
data:
  tomcat-users.xml: |
    <?xml version='1.0' encoding='utf-8'?>
      <tomcat-users xmlns="http://tomcat.apache.org/xml"
                      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                      xsi:schemaLocation="http://tomcat.apache.org/xml tomcat-users.xsd"
                      version="1.0">
       <user username="tomcat" password="tomcat" roles="admin-gui,manager-gui"/>
    </tomcat-users>
	
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebdeployment
spec:
  replicas: 2
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: javawebapp
  template:
    metadata:
      name: javawebappod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:3
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: tomcatusersconfig
          mountPath: "/usr/local/tomcat/conf/tomcat-users.xml"
          subPath: "tomcat-users.xml"
      volumes:
      - name: tomcatusersconfig
        configMap:
          name: javawebappconfig
          items:
          - key: "tomcat-users.xml"
            path: "tomcat-users.xml"
			
Pull an Image from a Private Registry
=====================================

kubectl create secret docker-registry regcred --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>


<your-registry-server> is your Private Docker Registry FQDN. Use https://index.docker.io/v1/ for DockerHub.
<your-name> is your Docker username.
<your-pword> is your Docker password.
<your-email> is your Docker email.

ex: 
Docker Hub: --docker-server is optional in case of docker hub

kubectl create secret docker-registry regcred --docker-server=https://index.docker.io/v1/ --docker-username=dockerhandson --docker-password=password

ECR
kubectl create secret docker-registry regcred --docker-server=https://567763916643.dkr.ecr.ap-south-1.amazonaws.com --docker-username=AWS --docker-password=password
				


Create a Pod or deployment or ds ,rc,statefull set that uses your Secret(Image Pull Secrets under pod Spec)

apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: regcred		

--------


Liveness Probe & Ready ness Probes
==================================  				

apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2
  revisionHistoryLimit: 10
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: javawebapp
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:1
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 1
            memory: 1Gi
        livenessProbe:
          httpGet:
            path: /java-web-app
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
		  timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /java-web-app
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
		  timeoutSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
 name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 8080  

-------------

	
####################################
# Mongo Database as statefullSet
####################################

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongod
spec:
  serviceName: mongodb-service
  replicas: 3
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: mongod-container
        image: mongo
        command:
          - "mongod"
          - "--bind_ip"
          - "0.0.0.0"
          - "--replSet"
          - "MainRepSet"
        resources:
          requests:
             cpu: 200m
             memory: 200Mi
          limits:
             cpu: 500m
             memory: 512Mi
        ports:
        - containerPort: 27017
          volumeMounts:
          - name: mongodb-persistent-storage-claim
            mountPath: /data/db
  volumeClaimTemplates:
  - metadata:
      name: mongodb-persistent-storage-claim
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: mongodb-service
  labels:
    name: mongo
spec:
  ports:
  - port: 27017
    targetPort: 27017
  clusterIP: None
  selector:
    app: mongo
	


# Setup Mongodb Reple Set And Added Members And Create the Administrator for the MongoDB

# Go inside one of the pod

kubectl exec -it mongod-0  bash

# Connect to mongo shell by using below command

mongo

# Initiate mongod db reple set and add members

rs.initiate( { _id: "MainRepSet", version: 1,
members: [ 
 { _id: 0, host: "mongod-0.mongodb-service.default.svc.cluster.local:27017" }, 
 { _id: 1, host: "mongod-1.mongodb-service.default.svc.cluster.local:27017" }, 
 { _id: 2, host: "mongod-2.mongodb-service.default.svc.cluster.local:27017" } ] } );
 
# Create root user name and password 
db.getSiblingDB("admin").createUser( {user : "devdb",pwd  : "devdb123",roles: [ { role: "root", db: "admin" } ] } ); 	



# Deploy Spring App which connects to mongodb

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
        env:
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb123
        - name: MONGO_DB_HOSTNAME
          value: mongodb-service 
---
apiVersion: v1
kind: Service
metadata:
  name: springapp
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
  type: NodePort


Commands
========
kubectl get pv
kubectl get pvc
kubectl get statefulset

kubectl delete statefulset <statefulsetName>







Node Selector  Node Affinity And Taints,Tolerations
===================================================

kubectl get nodes

kubectl get nodes --show-labels

kubectl describe node <nodeId>

# Add Label to Node
kubectl label nodes <nodeId/Name> node=workerone


# Node Selector
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: javawebapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  minReadySeconds: 60
  template:
    metadata:
      name: javawebapp
      labels:
        app: javawebapp
    spec:
      nodeSelector:
        name: workerone
      containers:
      - name: javawebapp
        image: dockerhandson/java-web-app:3
        ports:
        - containerPort: 8080
---		
# requiredDuringSchedulingIgnoredDuringExecution(HardRule)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: javawebapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  minReadySeconds: 60
  template:
    metadata:
      name: javawebapp
      labels:
        app: javawebapp
    spec:
      affinity:
        nodeAffinity:
         requiredDuringSchedulingIgnoredDuringExecution:
           nodeSelectorTerms:
           - matchExpressions:
             - key: "node"
               operator: In
               values:
               - workerone
      containers:
      - name: javawebapp
        image: dockerhandson/java-web-app:3
        ports:
        - containerPort: 8080
---		
# preferredDuringSchedulingIgnoredDuringExecution(Soft Rule)   
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: javawebapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  minReadySeconds: 60
  template:
    metadata:
      name: javawebapp
      labels:
        app: javawebapp
    spec:
      affinity:
        nodeAffinity:
         preferredDuringSchedulingIgnoredDuringExecution:
         - weight: 1
           preference:
            matchExpressions:
            - key: name
              operator: In
              values:
              - workerone
      containers:
      - name: javawebapp
        image: dockerhandson/java-web-app:3
        ports:
        - containerPort: 8080
		
Pod Affinity
------------

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginxdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginxpod
      labels:
        app: nginx
    spec:
      containers:
      - name: nginxcontainer
        image: nginx
        ports:
        - containerPort: 80
---		
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: javawebapp
  strategy:
    type: Recreate
  template:
    metadata:
      name: javawebappod
      labels:
        app: javawebapp
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:4
        ports:
        - containerPort: 8080

Pod AntiAffinity
----------------

apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: javawebapp
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: javawebapp
        image: dockerhandson/java-web-app:3
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 1
            memory: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 8080
	

#########
EKS Setup
#########

1) Create IAM Role For EKS Cluster.
      EKS – Cluster 
2) Create Dedicated VPC For EKS Cluster. Using CloudFormation. 
     https://amazon-eks.s3.us-west-2.amazonaws.com/cloudformation/2020-08-12/amazon-eks-vpc-private-subnets.yaml 
3) Create EKS Cluster.
4) Create IAM Role For EKS Worker Nodes.
        AmazonEKSWorkerNodePolicy
        AmazonEKS_CNI_Policy
        AmazonEC2ContainerRegistryReadOnly -- > 
5) Create Worker Nodes.
6) Create An Instance (If Not Exists) Install AWS CLI , IAM Authenticator And kubectl. Configure AWS CLI using Root or IAM User Access Key & Secret Key. Or Attach IAM With Required       Policies.
      aws eks update-kubeconfig --name <ClusterName> --region <RegionName> 


####Setup K8s Client Machine #####

### Install Kubectl In Linux====

1) Install Download the latest kubectl release with the command:

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"



2) Make the kubectl binary executable. 

     chmod +x ./kubectl
	 
3) Move the binary in to your PATH.

      sudo mv ./kubectl /usr/local/bin/kubectl
4) Test to ensure the version you installed is up-to-date:

kubectl version --client	 


### Install aws CLI In Linux====

1) Download AWS CLI ZIP
    
	curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"

2) Download & Install Unzip
    sudo yum install unzip -y

3) Extract Zip 
    unzip awscliv2.zip
	
4) Install
	sudo ./aws/install -i /usr/local/aws-cli -b /usr/local/bin
	
5) Verify
  aws --version	
	
	
######## Configure AWS CLI using ACCESS Key & Secret Key ########

aws configure
access key and secret key from aws security 


 aws eks list-clusters


##### Get KubeConfig file #####

aws eks update-kubeconfig --name <ClusterName> --region <RegionName> 
aws eks update-kubeconfig --name EKS-Learn --region ap-south-1

##### Verify Kubectl #####
kubectl get nodes
kubectl get pods		


http://learning.com/maven-web-application/


------------------



===== Cluster Auto Scaler Deployment in EKS========


1) Create AWS policy with below Actions .


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "autoscaling:DescribeAutoScalingGroups",
                "autoscaling:DescribeAutoScalingInstances",
                "autoscaling:DescribeLaunchConfigurations",
                "autoscaling:DescribeTags",
                "autoscaling:SetDesiredCapacity",
                "autoscaling:TerminateInstanceInAutoScalingGroup",
                "ec2:DescribeLaunchTemplateVersions"
            ],
            "Resource": "*",
            "Effect": "Allow"
        }
    ]
}	  

2) Attach policy to IAM Role which is used in EKS Node Group.
	
3) Deploy ClusterAutoScaler using below yml(Make sure u update - --node-group-auto-discovery values with your cluster name under cluster-autoscaler deployment commands.)



apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["update"]
  - apiGroups: [""]
    resources: ["endpoints"]
    resourceNames: ["cluster-autoscaler"]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["watch", "list", "get", "update"]
  - apiGroups: [""]
    resources:
      - "pods"
      - "services"
      - "replicationcontrollers"
      - "persistentvolumeclaims"
      - "persistentvolumes"
    verbs: ["watch", "list", "get"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["batch", "extensions"]
    resources: ["jobs"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create"]
  - apiGroups: ["coordination.k8s.io"]
    resourceNames: ["cluster-autoscaler"]
    resources: ["leases"]
    verbs: ["get", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create","list","watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
    verbs: ["delete", "get", "update", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8085'
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
        - image: k8s.gcr.io/cluster-autoscaler:v1.14.7
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/EKS_Demo # Update Your ClusterName here
          env:
          - name: AWS_REGION
            value: ap-south-1 # Update Your Region Name here in which u have EKS Cluster
          volumeMounts:
          - name: ssl-certs
            mountPath: /etc/ssl/certs/ca-certificates.crt
            readOnly: true
          imagePullPolicy: "Always"
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"



			 
========= RBAC With EKS IAM========================



As a Admin:
=============

1) Create IAM User With Polociy to List & Read EKS Cluster to get Kube Config File in AWS IAM Console.

2) Edit aws-auth to add userarn to aws-auth config map

  kubectl edit configmap aws-auth -n kube-system

apiVersion: v1
data:
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::921483055369:role/EKS_Node_Role
      username: system:node:{{EC2PrivateDNSName}}
  mapUsers: |
    - userarn: arn:aws:iam::935840844891:user/Mithun          # Update your user arn here
      username: Mithun                                        # Update your user name.
kind: ConfigMap
metadata:
  creationTimestamp: "2020-10-19T03:35:20Z"
  name: aws-auth
  namespace: kube-system
  resourceVersion: "792449"
  selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth
  uid: 8135dcd1-90e6-4dfb-872f-636601475aca
	  
	  


## Create Role/ClusterRole & RoleBinding & ClusterRoleBinding#####


kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: readonly
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get","list","create","delete","update"]
- apiGroups: ["apps"]
  resources: ["deployments","replicasets","daemonsets"]
  verbs: ["get","list"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: full_access_role_binding
  namespace: default
subjects:
- kind: User
  name: Balaji                           # Map with username
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: readonly
  apiGroup: rbac.authorization.k8s.io


++++++++++++++


kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: full_access
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: full_access_cluster_role_binding
  namespace: default
subjects:
- kind: User
  name: Balaji                           # Map with username
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: full_access
  apiGroup: rbac.authorization.k8s.io



Clinet Side:
===========
1) Install Kubectl & AWS CLI.

2) Configure AWS CLI(With Access Key & Secret Key of IAM User which you created in AWS IAM)

3) Get Kube-config file

   aws eks update-kubeconfig --name <EKSClusteName> --region <regionName>

4) try to access the cluster resources using kubectl		


+++++++++++
Helm:

$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
$ chmod 700 get_helm.sh
$ ./get_helm.sh

#adding repo , 

helm repo add name url

helm repo add bitnami https://charts.bitnami.com/bitnami


$ helm install metricsserver bitnami/metrics-server -n kube-system
	
					

helm show values bitnami/metrics-server
helm template bitnami/metrics-server    

helm template javawebapp --set autoscaling=true
 helm install javappp javawebapp
 
 helm upgrade javappp javawebapp --set image.tag=2


 helm rollback javappp




   
Resource Quotas:
===============

When several users or teams share a cluster with a fixed number of nodes, there is a concern that one team could use more than its fair share of resources.

Resource quotas are a tool for administrators to address this concern.

A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per namespace. It can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources(CPU,Memory) that may be consumed by resources in that namespace.

Resource quotas work like this:

1) The administrator creates one ResourceQuota for each namespace.

2) Users create resources (pods, services, etc.) in the namespace, and the quota system tracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota.

3) If creating or updating a resource violates a quota constraint, the request will fail with HTTP status code 403 FORBIDDEN with a message explaining the constraint that would have been violated.

4) If quota is enabled in a namespace for compute resources like cpu and memory, users must specify requests or limits for those values; otherwise, the quota system may reject pod creation. 

Hint: Use the LimitRange admission controller to force defaults for pods that make no compute resource requirements.



apiVersion: v1
kind: Namespace
metadata:
  name: testns
---
#Resource Quota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: testns-qs-quota
  namespace: testns
spec:
  hard:
    requests.cpu: "0.25"
    requests.memory: 256Mi
    limits.cpu: "1"
    limits.memory: 1Gi
    pods: 2	
---
# LimitRange
apiVersion: v1
kind: LimitRange
metadata:
  name: testns-limit-range
  namespace: testns
spec:
  limits:
  - default:
      cpu: 200m
      memory: 512Mi
    defaultRequest:
      cpu: 100m
      memory: 256Mi
    type: Container
	
	
    
    
    
  1)	Kubectl writes to the API server (kubectl run mywebserver --image=nginx)
2)	API server will authenticate and authorize. Upon validation, it will write it to etcd.
3)	Upon write to etcd, API Server will invoke the scheduler.
4)	Scheduler decides which node the pod should run and return data to API server. API will in-turn write it back to etcd.
5)	API Server will invoke the kubelet in the node decided by the scheduler.
6)	Kubelet communicates to the docker daemon via Docker socket to create the container.
7)	Kubelet will update the status of the POD back to the API server.
8)	API server will write the status details back to etcd.
  
 

  
========================

Containerization --> Docker, Rocket(Rkt),Container-d
Container Orchestration Tools --> Docker Swarm,Kubernetes,OpenShift


Installation
============

Self Managed K8's Cluster
 minikube --> Single Node K8's Cluster.
 kubeadm --> We can setup multi node k8's cluster using kubeadm.


Cloud Managed(Managed Services)
EKS --> Elastic Kubernetes Service(AWS)
AKS --> Azure Kubernetes Service(Azure)
GKE --> Google Kubernetes Engine(GCP)

KOPS --> Kubernetes Operations is a sotware using which we can create production ready
highily available kubenetes services in Cloud like AWS.KOPS will leverage Cloud Sevices like
AWS AutoScaling & Lanuch Configurations to setup K8's Master & Workers. It will Create 2 ASG & Lanuch Configs
one for master and one for worekrs. Thesse Auto Scaling Groups will manage EC2 Instances.

https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-strong-getting-started-strong


Name Spaces

kubectl get namespaces

# Create Name Space Using Imperative Command

kubectl create namespace <nameSpaceName>

ex:
kubectl create test-ns


# Using Declarative Manifest file

apiVersion: v1
kind: Namespace
metadata:
 name: <NameSpaceName>
 lables:           # Labels are key value pairs(Metadata)
   <key>: <value>
   <key>  <value>

# Example   
apiVersion: v1
kind: Namespace
metadata:
  name: test-ns

# Command to apply  
kubectl apply -f <fileName>.yaml


# Create POD Using Command

kubectl run <podName> --image=<imageName> --port=<containerPort> -n <namespaceName>

ex:
# If we don't mention name space it will create in default(current) namespace.
kubectl run javawebapp --image=dockerhandson/java-web-app:1 --port=8080

# List pods from current(default) ns
kubect get pods

# List pods from given  ns
kubect get pods -n <namespaceName>

ex:
kubect get pods -n ftest-ns



Kubernetes Objects Examples:

POD

Replication Controller

Replica Set

DaemonSet

Deployment
Statefullset

Service

PersistentVolume

PersistentVolumeClaim

CofgigMap

Secret ..etc

# POD Manifest

apiVersion: v1
kind: Pod
metadata:
  name: <PodName>
  labels:
    <Key>: <value>
  namespace: <nameSpaceName>
spec:
  containers:
  - name: <NameOfTheCotnainer>
    image: <imagaName>
	ports:
	- containerPort: <portOfContainer>
	
Example:
---	
apiVersion: v1
kind: Pod
metadata:
  name: mavenwebapppod
  labels:
    app:  mavenwebapp
  namespace: test-ns	
spec:
  containers:
  - name: mavenwebappcontainer
    image: dockerhandson/maven-web-application:1
    ports:
    - containerPort: 8080


kubectl apply -f <fileName.yml>

kubectl get all 
kubectl get pods 
kubectl get pods --show-labels
kubectl get pods -o wide
kubectl get pods -o wide --show-labels

kubectl  describe pod <podName>
kubectl  describe pod <podName> -n <namespace>


Note: If we don't mention -n <namespace> it will refer default namespace.
If required we can change name space context.

kubctl config set-context --curent --namespace=<namespace>
ex:
kubectl config set-context --curent --namespace=test-ns

After setting context  by default it will point to that namespace.


Change it to default namespace again if required
ex:
kubectl config set-context --curent --namespace=default

# Multi Container POD
apiVersion: v1
kind: Pod
metadata:
  name:  <PODName>
  namespace: <nameSpaceName>
  labels:
    <labelKey>: <labelValue> 
spec:
  containers:
  - name: <nameOftheCotnainer>
    image: <imageName>
	ports:
	- containerPort: <portNumberOfContainer>
  - name: <nameOftheCotnainer>
    image: <imageName>
    ports:
    - containerPort: <portNumberOfContainer>
	


K8's Service   ---> In Kubernetes Service makes our pods accessable/discoverable with in the cluster or exposing them to internat.
               		service will identify pods using it's labels And Selector. Whenever we create a service a ClusterIP (virtual IP) Address will be allocated for that serivce and DNS 					entry will be created for that IP. So internally we can access using service name(DNS).
						
	
Service
========
apiVersion: v1
kind: Service
metadata:
  name: <serviceName>
  namespace: <nameSpace>
spec:
  type: <ClusterIP/NodePort>
  selector:
     <key>: <value>
  ports:
  - port: <servciePort>	# default It to 80
    targetPort: <containerPort> 
	

With in Cluster ClusterIP
==========================
apiVersion: v1
kind: Service
metadata:
  name: mavenwebappservice
  namespace: test-ns  
spec:
  type: ClusterIP
  selector:
     app: mavenwebapp
  ports:
  - port: 80
    targetPort: 8080
	

Out side of Cluster Node Port
========================
apiVersion: v1
kind: Service
metadata:
  name: javawebappservice
spec:
  type: NodePort
  selector:
     app: javawebapp
  ports:
  - port: 80
    targetPort: 8080
	nodePort: 30033 # This Optional if u don't mention nodePort.Kuberetes will assign.
	

kubectl apply -f <file.yml>

kubectl get svc 
kubectl get all

kubectl  describe service <serviceName>
kubectl  describe service <serviceName> -n <namespace>
kubectl  describe service <serviceName> -o wide


What is node port range?
30000-32767


kubectl get all --all-namespaces
kubectl get all -n <namespace>
kubectl get pods -n <namespace>
kubectl get pods -n <namespace> - o wide

kubectl get svc -n <namespace>

ACCESS OUTSIDE USING NODEIP:NODEPORT.


With in the cluster one application(POD) can access other applications(PODS) using Service name.

What is FQDN?
Fully Qualified Domain name. 
If one POD need access to service & which are in differnent names space we have to use FQDN of the serivce.
Syntax: <serivceName>.<namespace>.svc.cluster.local
ex: mavenwebappservice.test-ns.svc.cluster.local




POD --> Pod is the smallest building block which we can deploy in k8s.Pod represents running process.Pod contains one or more containers.These container will share same network,storage and any other specifications.Pod will have unique IP Address in k8s cluster.

Pods
 SingleContainerPods --> Pod will have only one container.
 
 MultiContainerPods(SideCar) --> POD will two or more contianers.
 
We should not create pods directly for deploying applications.If pod is down it wont be rescheduled.

We have to create pods with help of controllers.Which manages POD life cycle.
	


Controllers
===========

ReplicationController
ReplicaSet
DaemonSet
Deploymnet
StatefullSet




# Replication Conrtoller
apiVersion: v1
kind: ReplicationController
metadata:
  name: <replicationControllerName>
  namespace: <nameSpaceName>
spec:
  replicas: <noOfReplicas>
  selector:
    <key>: <value>
  template: # POD Template
    metadata:
	  name: <PODName>
	  labels:
	    <key>: <value>
	spec:
	- containers:
	  - name: <nameOfTheContainer>
	    image: <imageName>
		ports:
		- containerPort: <containerPort>
  	
Example:
========
apiVersion: v1
kind: ReplicationController
metadata:
  name: javawebapprc
spec:
  replicas: 1
  selector:
    app: javawebapp
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappservice
spec:
  type: NodePort
  selector:
     app: javawebapp
  ports:
  - port: 80
    targetPort: 8080		

# Another Appplication
apiVersion: v1
kind: ReplicationController
metadata:
  name: pythonrc
spec:
  replicas: 1
  template: # Pod template
    metadata:
      name: pythonapppod
      labels:
        app: pythonapp
    spec:
      containers:
      - name: pythonappcontainer
        image: dockerhandson/python-app:1
        ports:
        - containerPort: 5000
---
apiVersion: v1
kind: Service
metadata:
  name: pythonsvc
spec:
  type: NodePort
  selector:
    app: pythonapp
  ports:
  - port: 80
    targetPort: 5000
	
	
kubectl apply -f <filename.yml>
kubectl get rc 
kubectl get rc -n <namespace>
kubectl get all
kubectl scale rc <rcName> --replicas <noOfReplicas>

kubectl describe rc <rcName>
kubectl delete rc <rcName>






ReplicaSet:

What is difference b/w replicaset and replication controller?

It's next gernation of replication controller. Both manages the pod replicas. But only difference as now is
selector support.

RC --> Supports only equality based selectors.

key == value(Equal Condition)
selector:
    app: javawebapp

RS --> Supports eqaulity based selectors and also set based selectors.


key == value(Equal Condition)

Set Based
key in (value1,value2,value3)
key notin (value1) 

selector:
   matchLabels: # Equality Based
     key: value
   matchExpressions: # Set Based
   - key: app
     operator: IN
	 values:
	 - javawebpp
	 - javawebapplication
	 
# Mainfest File RS

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: <RSName>
spec:
  replicas: <noOfPODReplicas>
  selector:  # To Match POD Labels.
    matchLabels:  # Equality Based Selector
	  <key>: <value>
    matchExpressions:  # Set Based Selector 
	- key: <key>
	  operator: <in/not in>
	  values:
	  - <value1>
	  - <value2>
  template:
    metadata:
	  name: <PODName>
	  labels:
	    <key>: <value>
	spec:
	- containers:
	  - name: <nameOfTheContainer>
	    image: <imageName>
		ports:
		- containerPort: <containerPort>


Example:

apiVersion: apps/v1
kind: ReplicaSet
metadata: 
  name: javawebapprs
spec: 
  replicas: 1
  selector: 
    matchLabels: 
      app: javawebapp
  template: 
    metadata: 
	  name: javawebapppod
      labels: 
        app: javawebapp
    spec: 
      containers: 
      - image: dockerhandson/java-web-app:1
        name: javawebappcontainer
        ports: 
        - containerPort: 8080


kubectl get rs 
kubectl get rs -n <namespace>
kubectl get all
kubectl scale rs <rsName> --replicas <noOfReplicas>

kubectl describe rs <rsName>
kubectl delete rs <rsName>






What is difference b/w kubectl create and kubectl apply ?

Create will Create an Object if it's not already created. Apply will perfrom create if object is not created earlier.If it's already
created it will update.			   
			   
			   
			   	

kubectl apply (create & update)

kubectl create -f <fileName.yml>

kubectl update -f <fileName.yml>


Change/Switch Context(NameSpace)
=================================

# View kubectl context
kubectl config view | grep namespace


# Change/Switch namespace

kubectl config set-context --current  --namespace=<namespace>




apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: <RSName>
spec:
  selector:  # To Match POD Labels.
    matchLabels:  # Equality Based Selector
	  <key>: <value>
    matchExpressions:  # Set Based Selector 
	- key: <key>
	  operator: <in/not in>
	  values:
	  - <value1>
	  - <value2>
  template:
    metadata:
	  name: <PODName>
	  labels:
	    <key>: <value>
	spec:
	- containers:
	  - name: <nameOfTheContainer>
	    image: <imageName>
		ports:
		- containerPort: <containerPort>


apiVersion: apps/v1
kind: DaemonSet
metadata: 
  name: mavenwebappds
spec: 
  selector: 
    matchLabels: 
      app: mavenwebapp
  template: 
    metadata: 
	  name: mavenwebapppod
      labels: 
        app: mavenwebapp
    spec: 
      containers: 
      - image: dockerhandson/maven-web-app
        name: mavenwebappcontainer
        ports: 
        - containerPort: 8080
		
kubectl get ds 
kubectl get ds -n <namespace>
kubectl get all


kubectl describe ds <dsName>
kubectl delete ds <dsName>





# Deployment ReCreate
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: javawebapp
  strategy:
    type: Recreate    
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:1
        ports:
        - containerPort: 8080

kubectl get deployment
kubectl get rs
kubectl get pods
kubectl rollout status deployment <deploymentName>
kubectl rollout history  deployment <deploymentName>
kubectl rollout history  deployment <deploymentName> --revision 1  
kubectl scale deployment <deploymentName> --replicas <noOfReplicas>

We can update deployment using yml or using command
	
	
# Update Deployment Image using command	

kubectl set image deployment <deploymentName> <containerName>=<imageNameWithVersion> --record		
ex:	
kubectl set image deployment javawebappdeployment javawebappcontainer=dockerhandson/java-web-app:2 --record		

Roll back to previous revison
kubectl rollout undo  deployment <deploymentName> --to-revision 1


# Rolling Update
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: javawebapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  minReadySeconds: 30
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:1
        ports:
        - containerPort: 8080 


kubectl get deployment
kubectl get rs
kubectl get pods
kubectl rollout status deployment <deploymentName>
kubectl rollout history  deployment <deploymentName>
kubectl rollout history  deployment <deploymentName> --revision 1  
kubectl rollout undo  deployment <deploymentName> --to-revision 1  
kubectl scale deployment <deploymentName> --replicas <noOfReplicas>
	
# Update Deployment Image using command	

kubectl set image deployment <deploymentName> <containerName>=<imageNameWithVersion> --record




	


POD AutoScaler
==============
What is difference b/w Kubernetes AutoScaling(POD AutoScaling) & AWS AutoScaling?


POD AutoScaling --> Kuberenets POD AutoScaling Will make sure u have minimum number pod replicas available at any time & based the observed CPU/Memory utilization on pods it can scale PODS.  HPA Will Scale up/down pod replicas of Deployment/ReplicaSet/ReplicationController based on observerd CPU & Memory utilization base the target specified.


AWS AutoScaling --> It will make sure u have enough number of nodes(Servers). Always it will maintian minimum number of nodes. Based the observed CPU/Memory utilization of node it can scale nodes.


Note: Deploy metrics server as k8s addon which will fetch metrics. Follow bellow link to deploy metrics Server.
====
https://github.com/MithunTechnologiesDevOps/metrics-server




---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpadeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      name: hpapod
  template:
    metadata:
      labels:
        name: hpapod
    spec:
      containers:
        - name: hpacontainer
          image: k8s.gcr.io/hpa-example
          ports:
          - name: http
            containerPort: 80
          resources:
            requests:
              cpu: "100m"
              memory: "64Mi"
            limits:
              cpu: "100m"
              memory: "256Mi"
---
apiVersion: v1
kind: Service
metadata:
  name: hpaclusterservice
  labels:
    name: hpaservice
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    name: hpapod
  type: NodePort
---
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: hpadeploymentautoscaler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: hpadeployment
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 40
  - type: Resource
    resource:
     name: memory
     target:
      type: Utilization
      averageUtilization: 40
	  

# Create temp POD using below command interatively and increase the load on demo app by accessing the service.

kubectl run -i --tty load-generator --rm  --image=busybox /bin/sh


# Access the service to increase the load.

while true; do wget -q -O- http://hpaclusterservice; done







Volumes:

Kubernetes Supports different types of volumes.

hostPath
nfs
emptydir
configMap
Secret

awsElasticBlockStore
googlePersistantdisk
azureFile
azuredisk

persistantVolume
persistantVolumeClaim



## Mongo db POD with Host Path Volume##

## Spring Boot App  
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
        - name: MONGO_DB_HOSTNAME
          value: mongo 
---
apiVersion: v1
kind: Service
metadata:
  name: springapp
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
  type: NodePort
---
# Mongo db pod with volumes(HostPath)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodbdeployment
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: hostpathvol
         hostPath:
           path: /tmp/mongodb 
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: hostpathvol
           mountPath: /data/db   
---
apiVersion: v1
kind: Service
metadata:
  name: mongo
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort: 27017
	

	

Configuration of NFS Server
===========================

Step 1: 

Create one Server for NFS

Install NFS Kernel Server in 
Before installing the NFS Kernel server, we need to update our system’s repository index with that of the Internet through the following apt command as sudo:

$ sudo apt-get update

The above command lets us install the latest available version of a software through the Ubuntu repositories.

Now, run the following command in order to install the NFS Kernel Server on your system:

$ sudo apt install nfs-kernel-server


Step 2: Create the Export Directory

sudo mkdir -p /mnt/share/

# As we want all clients to access the directory, we will remove restrictive permissions.
sudo chown nobody:nogroup /mnt/share/
sudo chmod 777 /mnt/share/

Step 3: Assign server access to client(s) through NFS export file

sudo vi /etc/exports


#/mnt/share/ <clientIP or Clients CIDR>(rw,sync,no_subtree_check,no_root_squash)
 #Ex:
/mnt/share/ *(rw,sync,no_subtree_check,no_root_squash)




Step 4: Export the shared directory

$ sudo exportfs -a



sudo systemctl restart nfs-kernel-server

Step 5: Open firewall for the client (s) PORT 2049




Configuring the Client Machines(Kubernetes Nodes)
================================================

Step 1: Install NFS Common
Before installing the NFS Common application, we need to update our system’s repository index with that of the Internet through the following apt command as sudo:

$ sudo apt-get update


$ sudo apt-get install nfs-common


Deploy mongdb pods using NFS Volume in k8s


#Mongo db POD with NFS Volume ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec:
 replicas: 2
 selector:
   matchLabels:
     app: springapp
 template:
   metadata:
     name: springapppod
     labels:
       app: springapp
   spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mognodbrs
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      name: mongodbpod
      labels:
        app: mongo
    spec:
      containers:
      - name: mongodbcontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
        volumeMounts:
        - name: mongodbhostpath
          mountPath: /data/db
      volumes:
      - name: mongodbhostpath
        nfs:
          server: 172.31.38.98  # Update NFS Server IP And Path Based on your setup
          path: /mnt/share
---
apiVersion: v1
kind: Service
metadata:
  name: mongo
spec:
  type: ClusterIP
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017





PV --> It's a piece of storage(hostPath,nfs,ebs,azurefile,azuredisk) in k8s cluster. PV exists independently from 
from pod life cycle whihc is consuming.

Persistent Volumes are provisioned in two ways, Statically or Dynamically.

1) Static Volumes (Manual Provisionging)
    As a k8's Administrator will create a PV manullay so that pv's can be avilable for PODS which requires.
	Create a PVC so that PVC will be attached PV. We can use PVC with PODS to get an access to PV. 
	
2) Dynamic Volumes (Dynamic Provisioning)
     It's possible to have k8's provsion(Create) volumes(PV) as required. Provided we have configured storageClass.
	 So when we create PVC if PV is not available Storage Class will Create PV dynamically.
   

PVC
If pod requires access to storage(PV),it will get an access using PVC. PVC will be attached to PV.



PersistentVolume – the low level representation of a storage volume.
PersistentVolumeClaim – the binding between a Pod and PersistentVolume.
Pod – a running container that will consume a PersistentVolume.
StorageClass – allows for dynamic provisioning of PersistentVolumes.


PV Will have Access Modes

ReadWriteOnce – the volume can be mounted as read-write by a single node
ReadOnlyMany – the volume can be mounted read-only by many nodes
ReadWriteMany – the volume can be mounted as read-write by many nodes

In the CLI, the access modes are abbreviated to:

RWO - ReadWriteOnce
ROX - ReadOnlyMany
RWX - ReadWriteMany


Claim Policies

A Persistent Volume can have several different claim policies associated with it including

Retain – When the claim is deleted, the volume remains.
Recycle – When the claim is deleted the volume remains but in a state where the data can be manually recovered.
Delete – The persistent volume is deleted when the claim is deleted.
The claim policy (associated at the PV and not the PVC) is responsible for what happens to the data on when the claim has been deleted.


Commands

kubectl get pv
kubectl get pvc
kubectl get storageclass
kubectl describe pvc <pvcName>
kubectl describe pv <pvName>



If Storage is calss not configued
=================================
1) Create a PV manually if not already available

2) Claim the PV by creating PVC

3) Use that PVC in your pod manfiset


If Storage is calss configued
=============================

1) Claim the PV by creating PVC

2) Use that PVC in your pod manfiset




Find Sample PV & PVC Yml from below Git Hub

https://github.com/MithunTechnologiesDevOps/Kubernates-Manifests/tree/master/pv-pvc

Static Volumes
1) Create PV

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
spec:
  storageClassName: manual
  capacity:
	storage: 1Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
	path: "/kube"

2) Create PVC

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-hostpath
spec:
  storageClassName: manual
  accessModes:
  - ReadWriteOnce
  resources:
	requests:
	  storage: 100Mi
		   
3) Use PVC with POD in POD manifest.

# Mongo db pod with PVC
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodbrs
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: mongodb-pvc
         persistentVolumeClaim:
           claimName: pvc-hostpath   
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: mongodb-pvc
           mountPath: /data/db



Commands://
=========

kubectl get pv
kubectl get pvc

kubectl describe pv <pvName>
kubectl describe pvc <pvcName>



kubectl get storageclass

Note: Configure Storage Class for Dynamic Volumes based on infra sturcture. Make that one as default storage class.

NFS Provisioner
Prerequisiets:
1) NFS Server
2) Insall nfs client softwares in all k'8s nodes.

Find NFS Provisioner below.

https://raw.githubusercontent.com/MithunTechnologiesDevOps/Kubernates-Manifests/master/pv-pvc/nfsstorageclass.yml

Get yml from above link.

$ curl https://raw.githubusercontent.com/MithunTechnologiesDevOps/Kubernates-Manifests/master/pv-pvc/nfsstorageclass.yml >> nfsstorageclass.yml

And update Your NFS Server IP Address(2 Places you need update IP Addrees) And path of nfs share. Apply

kubectl apply -f nfsstorageclass.yml

Dynamic Volumes

Refer below link where we are creating PVC & PODS :

https://raw.githubusercontent.com/MithunTechnologiesDevOps/Kubernates-Manifests/master/SpringBoot-Mongo-DynamicPV.yml

1) Create PVC(If we don't mention storageclass name it will use defautl storage class which is configured.) It will create PV.

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-nfs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
	  
2) Use PVC with POD in POD manifest.

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodbrs
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: mongodb-pvc
         persistentVolumeClaim:
           claimName: mongodb-nfs-pvc   
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: mongodb-pvc
           mountPath: /data/db
		   
# Complete Manifest Where in single yml we defined Deployment & Service for SpringApp & PVC(with NFS Dynamic StorageClass),ReplicaSet & Service For Mongo.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
        - name: MONGO_DB_HOSTNAME
          value: mongo 
---
apiVersion: v1
kind: Service
metadata:
  name: springapp
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30032
  type: NodePort
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodbpvc 
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Mi
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodbrs
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: pvc
         persistentVolumeClaim:
           claimName: mongodbpvc     
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: pvc
           mountPath: /data/db   
---
apiVersion: v1
kind: Service
metadata:
  name: mongo
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort: 27017





Config Maps & Secrets
======================

We can create ConfigMap & Secretes in Cluster using command or also using yml.


ConfigMap Using Command
======================
kubectl create configmap springappconfig --from-literal=mongodbusername=devdb


Or Using yml

---
apiVersion: v1
kind: ConfigMap
metadata:      # We can define multiple key value pairs.
  name: springappconfig
data:
  mongodbusername: devdb
  
  
Secret Using Command: 

kubectl create secret generic springappsecret --from-literal=mongodbpassword=devdb@123


Using Yml:

apiVersion: v1
kind: Secret
metadata:
  name: springappsecret
type: Opaque
stringData:   # We can define multiple key value pairs.
  mongodbpassword: devdb@123    
  

In Production Cluster we can create with differnet values.

kubectl create configmap springappconfig --from-literal=mongodbusername=proddb 

apiVersion: v1
kind: ConfigMap
metadata:
  name: springappconfig
data:            # We can define multiple key value pairs.
  mongodbusername: proddb

kubectl create secret generic springappsecret --from-literal=mongodbpassword=proddb@123


Using Yml:

apiVersion: v1
kind: Secret
metadata:
  name: springappsecret
type: Opaque
stringData:   # We can define multiple key value pairs.
  mongodbpassword: prodb@123
  

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo:1
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongosvc
        env:
        - name: MONGO_DB_USERNAME
          valueFrom:
            configMapKeyRef:
              name: springappconfig
              key: mongodbusername
        - name: MONGO_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: springappsecret
              key: mongodbpassword
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            cpu: "200m"
            memory: "256Mi"
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodeployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongocontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            configMapKeyRef:
              name: springappconfig
              key: mongodbusername
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: springappsecret
              key:  mongodbpassword
        volumeMounts:
        - name: mongodbhostvol
          mountPath: /data/db
      volumes:
      - name: mongodbhostvol
        hostPath:
          path: /tmp/mongo
---
apiVersion: v1
kind: Service
metadata:
  name: mongosvc
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017




ConigMap As Volume Example
=========================

# ConfigMap with file data

apiVersion: v1
kind: ConfigMap
metadata:
  name: javawebappconfig
data:
  tomcat-users.xml: |
    <?xml version='1.0' encoding='utf-8'?>
      <tomcat-users xmlns="http://tomcat.apache.org/xml"
                      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                      xsi:schemaLocation="http://tomcat.apache.org/xml tomcat-users.xsd"
                      version="1.0">
       <user username="tomcat" password="tomcat" roles="admin-gui,manager-gui"/>
    </tomcat-users>
	
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebdeployment
spec:
  replicas: 2
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: javawebapp
  template:
    metadata:
      name: javawebappod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:3
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: tomcatusersconfig
          mountPath: "/usr/local/tomcat/conf/tomcat-users.xml"
          subPath: "tomcat-users.xml"
      volumes:
      - name: tomcatusersconfig
        configMap:
          name: javawebappconfig
          items:
          - key: "tomcat-users.xml"
            path: "tomcat-users.xml"
			
Pull an Image from a Private Registry
=====================================

kubectl create secret docker-registry regcred --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>


<your-registry-server> is your Private Docker Registry FQDN. Use https://index.docker.io/v1/ for DockerHub.
<your-name> is your Docker username.
<your-pword> is your Docker password.
<your-email> is your Docker email.

ex: 
Docker Hub: --docker-server is optional in case of docker hub

kubectl create secret docker-registry regcred --docker-server=https://index.docker.io/v1/ --docker-username=dockerhandson --docker-password=password

ECR
kubectl create secret docker-registry regcred --docker-server=https://567763916643.dkr.ecr.ap-south-1.amazonaws.com --docker-username=AWS --docker-password=password
				


Create a Pod or deployment or ds ,rc,statefull set that uses your Secret(Image Pull Secrets under pod Spec)

apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: regcred
  


Liveness Probe & Ready ness Probes
==================================  				

apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2
  revisionHistoryLimit: 10
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: javawebapp
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:1
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 1
            memory: 1Gi
        livenessProbe:
          httpGet:
            path: /java-web-app
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
		  timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /java-web-app
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
		  timeoutSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
 name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 8080  






	
####################################
# Mongo Database as statefullSet
####################################

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongod
spec:
  serviceName: mongodb-service
  replicas: 3
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      terminationGracePeriodSeconds: 10
      containers:
        - name: mongod-container
          image: mongo
          command:
            - "mongod"
            - "--bind_ip"
            - "0.0.0.0"
            - "--replSet"
            - "MainRepSet"
          resources:
            requests:
              cpu: 200m
              memory: 200Mi
            limits:
              cpu: 500m
              memory: 512Mi
          ports:
            - containerPort: 27017
          volumeMounts:
            - name: mongodb-persistent-storage-claim
              mountPath: /data/db
  volumeClaimTemplates:
  - metadata:
      name: mongodb-persistent-storage-claim
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: mongodb-service
  labels:
    name: mongo
spec:
  ports:
  - port: 27017
    targetPort: 27017
  clusterIP: None
  selector:
    app: mongo
	


# Setup Mongodb Reple Set And Added Members And Create the Administrator for the MongoDB

# Go inside one of the pod

kubectl exec -it mongod-0  bash

# Connect to mongo shell by using below command

mongo

# Initiate mongod db reple set and add members

rs.initiate( { _id: "MainRepSet", version: 1,
members: [ 
 { _id: 0, host: "mongod-0.mongodb-service.default.svc.cluster.local:27017" }, 
 { _id: 1, host: "mongod-1.mongodb-service.default.svc.cluster.local:27017" }, 
 { _id: 2, host: "mongod-2.mongodb-service.default.svc.cluster.local:27017" } ] } );
 
# Create root user name and password 
db.getSiblingDB("admin").createUser( {user : "devdb",pwd  : "devdb123",roles: [ { role: "root", db: "admin" } ] } ); 	



# Deploy Spring App which connects to mongodb

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
        env:
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb123
        - name: MONGO_DB_HOSTNAME
          value: mongodb-service 
---
apiVersion: v1
kind: Service
metadata:
  name: springapp
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
  type: NodePort


Commands
========
kubectl get pv
kubectl get pvc
kubectl get statefulset

kubectl delete statefulset <statefulsetName>







Node Selector  Node Affinity And Taints,Tolerations
===================================================

kubectl get nodes

kubectl get nodes --show-labels

kubectl describe node <nodeId>

# Add Label to Node
kubectl label nodes <nodeId/Name> node=workerone


# Node Selector
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: javawebapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  minReadySeconds: 60
  template:
    metadata:
      name: javawebapp
      labels:
        app: javawebapp
    spec:
      nodeSelector:
        name: workerone
      containers:
      - name: javawebapp
        image: dockerhandson/java-web-app:3
        ports:
        - containerPort: 8080
---		
# requiredDuringSchedulingIgnoredDuringExecution(HardRule)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: javawebapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  minReadySeconds: 60
  template:
    metadata:
      name: javawebapp
      labels:
        app: javawebapp
    spec:
      affinity:
        nodeAffinity:
         requiredDuringSchedulingIgnoredDuringExecution:
           nodeSelectorTerms:
           - matchExpressions:
             - key: "node"
               operator: In
               values:
               - workerone
      containers:
      - name: javawebapp
        image: dockerhandson/java-web-app:3
        ports:
        - containerPort: 8080
---		
# preferredDuringSchedulingIgnoredDuringExecution(Soft Rule)   
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: javawebapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  minReadySeconds: 60
  template:
    metadata:
      name: javawebapp
      labels:
        app: javawebapp
    spec:
      affinity:
        nodeAffinity:
         preferredDuringSchedulingIgnoredDuringExecution:
         - weight: 1
           preference:
            matchExpressions:
            - key: name
              operator: In
              values:
              - workerone
      containers:
      - name: javawebapp
        image: dockerhandson/java-web-app:3
        ports:
        - containerPort: 8080
		
Pod Affinity
------------

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginxdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginxpod
      labels:
        app: nginx
    spec:
      containers:
      - name: nginxcontainer
        image: nginx
        ports:
        - containerPort: 80
---		
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: javawebapp
  strategy:
    type: Recreate
  template:
    metadata:
      name: javawebappod
      labels:
        app: javawebapp
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:4
        ports:
        - containerPort: 8080

Pod AntiAffinity
----------------

apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: javawebapp
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: javawebapp
        image: dockerhandson/java-web-app:3
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 1
            memory: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 8080
	




# Taint a node
kubectl taint nodes <nodeId/Name> <key>=<value>:<effect>

Example:
=======
kubectl taint nodes ip-172-31-34-69 node=HatesPods:NoSchedule

# Tolerating above taint.		
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: javawebapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  minReadySeconds: 60
  template:
    metadata:
      name: javawebapp
      labels:
        app: javawebapp
    spec:
      tolerations:
      - effect: NoSchedule
		operator: "Exists"
        key: node
      containers:
      - name: javawebapp
        image: dockerhandson/java-web-app:3
        ports:
        - containerPort: 8080
		



#########
EKS Setup
#########

1) Create IAM Role For EKS Cluster.
      EKS – Cluster 
2) Create Dedicated VPC For EKS Cluster. Using CloudFormation. 
     https://amazon-eks.s3.us-west-2.amazonaws.com/cloudformation/2020-08-12/amazon-eks-vpc-private-subnets.yaml 
3) Create EKS Cluster.
4) Create IAM Role For EKS Worker Nodes.
        AmazonEKSWorkerNodePolicy
        AmazonEKS_CNI_Policy
        AmazonEC2ContainerRegistryReadOnly -- > 
5) Create Worker Nodes.
6) Create An Instance (If Not Exists) Install AWS CLI , IAM Authenticator And kubectl. Configure AWS CLI using Root or IAM User Access Key & Secret Key. Or Attach IAM With Required       Policies.
      aws eks update-kubeconfig --name <ClusterName> --region <RegionName> 


####Setup K8s Client Machine #####

### Install Kubectl In Linux====

1) Install Download the latest kubectl release with the command:

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"



2) Make the kubectl binary executable. 

     chmod +x ./kubectl
	 
3) Move the binary in to your PATH.

      sudo mv ./kubectl /usr/local/bin/kubectl
4) Test to ensure the version you installed is up-to-date:

kubectl version --client	 


### Install aws CLI In Linux====

1) Download AWS CLI ZIP
    
	curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"

2) Download & Install Unzip
    sudo yum install unzip -y

3) Extract Zip 
    unzip awscliv2.zip
	
4) Install
	sudo ./aws/install -i /usr/local/aws-cli -b /usr/local/bin
	
5) Verify
  aws --version	
	
	
######## Configure AWS CLI using ACCESS Key & Secret Key ########

aws configure


##### Get KubeConfig file #####

aws eks update-kubeconfig --name <ClusterName> --region <RegionName> 

##### Verify Kubectl #####
kubectl get nodes
kubectl get pods




===== Cluster Auto Scaler Deployment in EKS========


1) Create AWS policy with below Actions .


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "autoscaling:DescribeAutoScalingGroups",
                "autoscaling:DescribeAutoScalingInstances",
                "autoscaling:DescribeLaunchConfigurations",
                "autoscaling:DescribeTags",
                "autoscaling:SetDesiredCapacity",
                "autoscaling:TerminateInstanceInAutoScalingGroup",
                "ec2:DescribeLaunchTemplateVersions"
            ],
            "Resource": "*",
            "Effect": "Allow"
        }
    ]
}	  

2) Attach policy to IAM Role which is used in EKS Node Group.
	
3) Deploy ClusterAutoScaler using below yml(Make sure u update - --node-group-auto-discovery values with your cluster name under cluster-autoscaler deployment commands.)



apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["update"]
  - apiGroups: [""]
    resources: ["endpoints"]
    resourceNames: ["cluster-autoscaler"]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["watch", "list", "get", "update"]
  - apiGroups: [""]
    resources:
      - "pods"
      - "services"
      - "replicationcontrollers"
      - "persistentvolumeclaims"
      - "persistentvolumes"
    verbs: ["watch", "list", "get"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["batch", "extensions"]
    resources: ["jobs"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create"]
  - apiGroups: ["coordination.k8s.io"]
    resourceNames: ["cluster-autoscaler"]
    resources: ["leases"]
    verbs: ["get", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create","list","watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
    verbs: ["delete", "get", "update", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8085'
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
        - image: k8s.gcr.io/cluster-autoscaler:v1.14.7
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/EKS_Demo # Update Your ClusterName here
          env:
          - name: AWS_REGION
            value: ap-south-1 # Update Your Region Name here in which u have EKS Cluster
          volumeMounts:
          - name: ssl-certs
            mountPath: /etc/ssl/certs/ca-certificates.crt
            readOnly: true
          imagePullPolicy: "Always"
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"



			 
========= RBAC With EKS IAM========================



As a Admin:
=============

1) Create IAM User With Polociy to List & Read EKS Cluster to get Kube Config File in AWS IAM Console.

2) Edit aws-auth to add userarn to aws-auth config map

  kubectl edit configmap aws-auth -n kube-system

apiVersion: v1
data:
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::921483055369:role/EKS_Node_Role
      username: system:node:{{EC2PrivateDNSName}}
  mapUsers: |
    - userarn: arn:aws:iam::935840844891:user/Mithun          # Update your user arn here
      username: Mithun                                        # Update your user name.
kind: ConfigMap
metadata:
  creationTimestamp: "2020-10-19T03:35:20Z"
  name: aws-auth
  namespace: kube-system
  resourceVersion: "792449"
  selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth
  uid: 8135dcd1-90e6-4dfb-872f-636601475aca
	  
	  


## Create Role/ClusterRole & RoleBinding & ClusterRoleBinding#####


kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: readonly
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get","list","create","delete","update"]
- apiGroups: ["apps"]
  resources: ["deployments","replicasets","daemonsets"]
  verbs: ["get","list"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: full_access_role_binding
  namespace: default
subjects:
- kind: User
  name: Balaji                           # Map with username
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: readonly
  apiGroup: rbac.authorization.k8s.io




Clinet Side:
===========
1) Install Kubectl & AWS CLI.

2) Configure AWS CLI(With Access Key & Secret Key of IAM User which you created in AWS IAM)

3) Get Kube-config file

   aws eks update-kubeconfig --name <EKSClusteName> --region <regionName>

4) try to access the cluster resources using kubectl			
					




===============

	 
POD and Service

apiVersion: v1
kind: Pod
metadata:
  name: mavenPod
  namespace: test-ns
  labels:
    app: mavenapp
spec:
  containers:
  - name: mavenContainer
    image: harishkumarbr/maven-web-app:1
    ports:
    - containerPort: 8080
---    
apiVersion: v1
kind: Service
metadata:
  name: mavenService:
  namespace: test-ns
spec:
  type: NodePort/ClusterIP
  selector:
    app: mavenapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 32019  
    # Optional
    
    
 +++++++++++++++++++++++++++++++++++++++++++   
 ++++++++++++++++++++++++++++++++++++++++++++++
 RC and Service
 
apiVersion: v1
kind: ReplicationController
metadata:
  name:mavenRC
  namespace: test-ns
  #labels:
   # app: mavenapp
spec:
  replicas: 1
  selector:
    app: mavenapp
  template:
    metadata:
      name: mavenPod
      namespace: test-ns
      labels:
        app: mavenapp
    spec:
    - name: mavenContainer
      image: harishkumarbr/maven-web-app:1
      ports:
      - containerPort: 8080
---
apiVersion: v1
kind:  Service
metadata:
  name: mavenServiceRC
spec:            
  type: NodePort/ClusterIP
  selector:
    app: mavenapp
  ports:
  - ports: 80
    targetPort: 8080
    NodePort: 310917
 # optional
+++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++

# RS

apiVersion: apps/v1
kind:  ReplicaSet
metadata:
  name: mavenRS
  namespace: test-ns
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mavenapp
  template:
    metadata:
      name: mavenPod
      namespace: test-ns
      labels:
        app:mavenapp
    spec:
      containers:
      - name: mavenContainer
        image: harishkumarbr/maven-web-app:1
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: mavenServiceRS
  namespace: test-ns
spec:
  type: NodePort
  selector:
   app: mavenapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 310454
+++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++    
   # Deployment Recreate
   
apiVersion: apps/v1
kind: Deployment
metadata:
  name:mavenDeploymentRecreate
  namespace: test-ns
spec:
  replicas: 2
  strategy:
    type: Recreate 
  selector:
    matchLabels:
      app: mavenapp
  template:
    metadata:
      name: mavenPod
      namespace: test-ns
      labels:
        app: mavenapp
    spec:
      containers:
      - name: mavenContainer
        image: harishkumarbr/maven-web-app:1
        ports:
        - containerPort: 8080
    
      
---        
apiVersion: v1
kind: Service
metadata:
  name: mavenServiceDeploymentRecreate
  namespace: test-ns
spec:
  type: NodePort
  selector:
   app: mavenapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 310454        
        
++++++++++++++++++++++++++
++++++++++++++++++++++++++


Deployment with Rolling Update

apiVersion: apps/v1
kind: Deployment
metadata:
 name: mavenDeploymentRollingUpdate
 namespace: test-ns
spec:
  replicas: 2
  strategy: 
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  minReadySeconds: 1    
  selector:
    matchLabels:
      app: mavenapp
  template:
    metadata:
     name: mavenPod
     namespace: test-ns
     labels:
       app: mavenapp
    spec:
      containers:    
      - name: mavenContainer
        image: harishkumarbr/maven-web-app:1
        ports:
        - containerPort: 8080
---
# service
apiVersion: v1
kind: Service
metadata:
  name: mavenServiceDeploymentRollingUpdate
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: mavenapp
  port:
  - ports: 80
    targetPort: 8080
 

++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++

#HPA Deployment

apiVersion: apps/v1
kind: Deployment
metadata:
  name: HPADeployment
  namespace: test-ns
spec:
  replicas: 2
  strategy: RollingUpdate
  selector:
    matchLabels:
      app: mavenapp
  template:
    metadata:
      name: HPAPod
      namespace: test-ns
      labels:
        app: mavenapp
    spec:
      containers:
      - name: mavenContainer
        image: harishkumarbr/maven-web-app:1
        ports:
        - containerPort: 8080      
        resources:
          requests:
            cpu: "200m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "600Mi"        
---        
apiVersion: v1
kind: Service
metadata:
  name: HPADeploymentService
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: mavenapp
  port:
  - ports: 80
    targetPort: 8080  
---
apiVersion: autoscaling/v2beta2
kind: HorizonatalPodCluster
metadata:
  name: HPA
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: HPADeployment
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 40
  - type: Resource
    resource:
      name: memory
      target:
      type: Utilization
      averageUtilization: 40    
      
  +++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++  
  # volume with HostPath
#Spring APP
apiVersion: apps/v1
kind: Deployment
metadata:
  name: SpringAppDeploymentHostPath
  namespace: test-ns
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
    template:
      metadata:
        name: springPod
        labels:
          app: springapp
      spec:
        containers:
        - name: SpringContainer
          image: harishkumarbr/spring-boot-mongo:1
          port:
          - containerPort: 8080
          env:
          - name: MONGO_DB_HOSTNAME
            value: mongodbService
          - name: MONGO_USER_NAME
            value: devdb
          - name: MONGO_PASSWORD
            value: devdb@123
---
#APP Service
apiVersion: v1
kind: Service
metadata:
 name:mongoservice
 namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
# Mongodb
apiVersion: v1
kind: ReplicaSet
metadata:
  name: MongodbRS
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      name: mongodbPod
      namespace: test-ns
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodbContainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123        
        resources:
          requests:
            cpu: "200m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "600Mi"  
        volumeMounts:
        - name: volumeHostPath
          mountPath: /data/db
      volumes:
      - name: volumeHostPath
        hostPath:
          path: /tmp/dbstore      
---
#mongodb Service
apiVersion: v1
kind: Service
metadata:
  name: mongodbService
  namespace: test-ns
spec:
  type: ClusterIP # default
  selector:
    app: mongodb
  ports:
  - port: 27017  
    targetPort: 27017
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
    # volume with NFS
    
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappDeploymentNFS
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      name: springPod
      labels:
        app: springapp
    spec:
      containers:
      - name: springappContainer
        image: harishkumarbr/spring-boot-mongo:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_USER_NAME
          value: devdb
        - name: MONGO_PASSWORD
          value: devdb@123  
---
#SpringService
apiVersion: v1
kind: Service
metadata:
  name: springappService
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 312001
---
# mongobsrs
apiVersion: v1
kind: ReplicaSet
metadata:
  name: mongodbRS
  namespace: test-ns
spec:
  replicas: 1
  selector:
    app: mongoapp
  template:
    metadata:
      name: mongoappPod
      labels:
        app: mongoapp
    spec:
      containers:
      - name: mongoContainer
        image: mongo
        ports:
        - containerPort: 27017
        volumeMounts:
        - name: volumenfs
          mountPath: /data/db
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123                  
      volumes:
      - name: volumenfs
        nfs:
          server: 10.23.32.123  #Ip of nfs server
          path: /mnt/share # path in nfs server        
---
#mongodbService
apiVersion: v1
kind: Service
metadata:
  name: mongo
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017
---     
    
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#PV 
#PVC     with hostpath
        
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappDeploymentpvc
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappContainer
        image: harishkumarbr/spring-boot-mongo:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_USER_NAME
          value: devdb
        - name: MONGO_PASSWORD
          value: devdb@123         
---
apiVersion: v1
kind: Service
namespace: test-ns
metadata:
  name: springappService
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongoContainer
        image: mongo
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123 
        volumeMounts:
        - name: pvc_hostpath
          mountPath: /data/db   
      volumes:
      - name: pvc_hostpath
        persistentVolumeClaim:
          claimName: mypvc-hostpath
          

---
apiVersion: v1
kind: Service
metadata:
  name: mongo
  namespace: test-ns
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc-hostpath
  namespace: test-ns
spec:
  resources:
    requests:
      storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mypv-hostpath
  namespace: test-ns
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /tmp/dbstore
  #persistentVolumeReclaimPolicy: Recycle default = delete
  #storageClassName: slow
  #mountOptions:
   # - hard
    #- nfsvers=4.1
  #nfs:
   # path: /tmp/dbstore
    #server: 172.17.0.2
      
 ++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++++++++++++++++++++++++++++++++++++++++++++++++++++++
#PV
#PVC with nfs
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappDeploymentpvc
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappContainer
        image: harishkumarbr/spring-boot-mongo:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_USER_NAME
          value: devdb
        - name: MONGO_PASSWORD
          value: devdb@123         
---
apiVersion: v1
kind: Service
namespace: test-ns
metadata:
  name: springappService
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongoContainer
        image: mongo
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123 
        volumeMounts:
        - name: pvc_nfs
          mountPath: /data/db   
      volumes:
      - name: pvc_nfs
        persistentVolumeClaim:
          claimName: mypvc-nfs
          

---
apiVersion: v1
kind: Service
metadata:
  name: mongo
  namespace: test-ns
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc-nfs
  namespace: test-ns
spec:
  resources:
    requests:
      storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mypv-nfs
  namespace: test-ns
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  #hostPath:
   # path: /tmp/dbstore
  #persistentVolumeReclaimPolicy: Recycle default = delete
  #storageClassName: slow
  #mountOptions:
   # - hard
    #- nfsvers=4.1
  nfs:
    path: /tmp/dbstore
    server: 172.17.0.2
 	 
+++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++     
# PVC + StorageClass (PV wil auto created once confogured StorageClass) 
 
#SpringApp
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappDeployment
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappContainer
        image: harishkumarbr/spring-boot-mongo:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_USER_NAME
          value: devdb
        - name: MONGO_PASSWORD
          value: devdb@123        
---        
#APP Service
apiVersion: v1
kind: Service
metadata:
  name: springappService
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 30123 # 30000 - 32767
---
#Mongo DB App
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodbdeployment
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongoContainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
        volumeMounts:
        - name: pvc_mongo
          mountPath: /data/db
      volumes:
      - name: pvc_mongo
        persistentVolumeClaim:
          claimName: pvc_storage_pv
---
# PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc_storage_pv
  namespace: test-ns
spec:
  resources:
    requests:
      storage: 100mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
---
#mongo service
apiVersion: v1
kind: Service
metadata:
  name: mongo
  namespace: test-ns
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017 
 +++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++
# ConfigMap and Secrets + APP , Service , Mongo DB + Service + PVC +  



#Config Map
apiVersion: v1
kind: ConfigMap
metadata:
  name: springAppConfigMap
  namespace: test-ns
data:
  userName: devdb
---
#Secrets
apiVersion: v1
kind: Secret
metadata:
  name: springAppSecret
  namespace: test-ns
type: Opaque
stringData:
  password: devdb@123
---
#SpringApp
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappDeployment
  namespace: test-ns
spec:
  replicas: 2
  strategy: Recreate
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappContainer
        image: harishkumarbr/spring-boot-mongo:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_USER_NAME
          valueFrom:
            configMapKeyRef:
              name: springAppConfigMap
              key: userName
          #value: devdb
        - name: MONGO_PASSWORD
          valueFrom:
            secretKeyRef:
              name: springAppSecret
              key: password
        #  value: devdb@123        
---        
#APP Service
apiVersion: v1
kind: Service
metadata:
  name: springappService
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 30123 # 30000 - 32767
---
#Mongo DB App
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodbdeployment
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongoContainer
        image: mongo
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          #value: devdb
          valueFrom:
            configMapKeyRef:
              name: springAppConfigMap
              key: userName
        - name: MONGO_INITDB_ROOT_PASSWORD
          #value: devdb@123
          valueFrom:
            secretKeyRef:
              name: springAppSecret
              key: password
        volumeMounts:
        - name: pvc_mongo
          mountPath: /data/db
      volumes:
      - name: pvc_mongo
        persistentVolumeClaim:
          claimName: pvc_storage_pv
---
# PV
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc_storage_pv
  namespace: test-ns
spec:
  resources:
    requests:
      storage: 100mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
---
#mongo service
apiVersion: v1
kind: Service
metadata:
  name: mongo
  namespace: test-ns
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017
 +++++++++++++++++++++++++++++++++++++++++++++++
 +++++++++++++++++++++++++++++++++++++++++++++++
 
 #liveness Probe and Readiness Probe  + ConfigMap + Secrets + PVC
apiVersion: apps/v1 
kind: Deployment
metadata:
  name: springappdeployment-liveness-readiness
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappPod
        image: harishkumarbr/spring-boot-mongo:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_USER_NAME
          valueFrom:
            configMapKeyRef:
              name: springAppConfigMap
              key: userName
          #value: devdb
        - name: MONGO_PASSWORD
          valueFrom:
            secretKeyRef:
              name: springAppSecret
              key: password
        #  value: devdb@123        
        livenessProbe:
          httpGet:
            path: /java-web-app
            port: 8080
            initialDelaySeconds: 60
            periodSeconds: 30 # time interval
            timeoutSeconds: 10 #  response time 
        readinessProbe:
          httpGet:
            path: /java-web-app
            port: 8080
            initialDelaySeconds: 40
            periodSeconds: 30
            timeoutSeconds: 5
---
 # app service
apiVersion: v1
kind: Service
metadata:
  name: springappService
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
#Mongo DB App
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodbdeployment
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongoContainer
        image: mongo
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          #value: devdb
          valueFrom:
            configMapKeyRef:
              name: springAppConfigMap
              key: userName
        - name: MONGO_INITDB_ROOT_PASSWORD
          #value: devdb@123
          valueFrom:
            secretKeyRef:
              name: springAppSecret
              key: password
        volumeMounts:
        - name: pvc_mongo
          mountPath: /data/db
      volumes:
      - name: pvc_mongo
        persistentVolumeClaim:
          claimName: pvc_storage_pv
---
# PV
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc_storage_pv
  namespace: test-ns
spec:
  resources:
    requests:
      storage: 100mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
---
#mongo service
apiVersion: v1
kind: Service
metadata:
  name: mongo
  namespace: test-ns
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017
---
#Config Map
apiVersion: v1
kind: ConfigMap
metadata:
  name: springAppConfigMap
  namespace: test-ns
data:
  userName: devdb
---
#Secrets
apiVersion: v1
kind: Secret
metadata:
  name: springAppSecret
  namespace: test-ns
type: Opaque
stringData:
  password: devdb@123       

++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++
              
 ## Stateful set
 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongodb
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongodb
  serviceName: mongodb-Service
  replicas: 2
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongoContainer
        image: mongo
        ports:
        - containerPort: 27017
        volumeMounts:
        - name: persistent-volume-stateful
          mountPath: /data/db
  volumeClaimTemplates:
  - metadata:
      name: persistent-volume-stateful
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
---
#mongodb Service
apiVersion: v1
kind: Service
metadata:
  name: mongodb-Service
  namespace: test-ns
  labels:
    name: mongodb
spec:
  selector:
    app: mongodb
  ClusterIP: None # Headless Service  
  ports:
  - port: 27017
    targetPort: 27017
---
#Spring APP

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappDeploymentStatefulSet
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappContainer
        image: harishkumarbr/spring-boot-mongo:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongodb-Service
        - name: MONGO_USER_NAME
          valueFrom:
            configMapKeyRef:
              name: springAppConfigMap
              key: userName
          #value: devdb
        - name: MONGO_PASSWORD
          valueFrom:
            secretKeyRef:
              name: springAppSecret
              key: password
        #  value: devdb@123         
---
apiVersion: v1
kind: Service
metadata:
  name: springappService
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 32323
              
---
#Config Map
apiVersion: v1
kind: ConfigMap
metadata:
  name: springAppConfigMap
  namespace: test-ns
data:
  userName: devdb
---
#Secrets
apiVersion: v1
kind: Secret
metadata:
  name: springAppSecret
  namespace: test-ns
type: Opaque
stringData:
  password: devdb@123       

++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++

## Node Selector

apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappDeploymentNodeSelector
spec:
  selector:
    matchLabels:
      app: Javawebapp
  template:
    metadata:
      labels:
        app: Javawebapp
    spec:
      replicas: 2
      nodeSelector:
        name: workerNode
      containers:
      - name: JavawebappContainer
        image: harishkumarbr/java-web-app:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: JavawebappService
spec:
  type: NodePort
  selector:
    app: Javawebapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 31092

++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++

          
# Node affinity -requiredDuringSchdedulingIgnoredDuringExecution
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappDeploymentNodeAffinity
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  minReadySeconds: 60
  selector:
    matchLabels:
      app: Javawebapp
  template:
    metadata:
      labels:
        app: Javawebapp
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchdedulingIgnoredDuringExecution:
            nodeSelectorsTerms:
            - matchExpressions:
              - key: "node"
                operator: In
                values:
                - workernode
      containers:
      - name: JavawebappContainer
        image: harishkumarbr/java-web-app:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: JavawebappService
spec:
  selector:
    app: Javawebapp
  ports:
  - port: 80
    targetPort: 8080
++++++++++++++++++++++
+++++++++++++++++++++

 ## Node affinity -prefferedDuringSchedulingIgnoredDurindExecution
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappDeploymentNodeAffinity
  namespace: test-ns
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  minReadySeconds: 60
  selector:
    matchLabels:
      app: Javawebapp
  template:
    metadata:
      labels:
        app: Javawebapp
    spec:
      affinity:
        nodeAffinity:
          prefferedDuringSchedulingIgnoredDurindExecution:
          - weight: 1
            preference:
            - matchExpressions:
              - key: "node"
                operator: In
                values:
                - workerone
      containers:
      - name: JavawebappContainer
        image: harishkumarbr/java-web-app:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: JavawebappService
  namespace: test-ns
spec:
  selector:
    app: Javawebapp
  ports:
  - port: 80
    targetPort: 8080
+++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++

# Pod Affinity and Pod AntiAffinity

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginxDeployment
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: nginxApp
  template:
    metadata:
      labels:
        app: nginxApp
    spec:
      containers:
      - name: nginxContainer
        image: nginx
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginxAppService
  namespace: test-ns
spec:
  selector:
    app: nginxApp
  ports:
  - port: 80
    targetPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappDeploymentPodAffinity
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: Javawebapp
  template:
    metadata:
      labels:
        app: Javawebapp
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx
            topologyKey: topology.kubernetes.io/zone
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: security
                operator: In
                values:
                - maven
          topologyKey: topology.kubernetes.io/zone            
      containers:
      - name: JavawebappContainer
        image: harishkumarbr/java-web-app:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
---        
apiVersion: v1
kind: Service
metadata:
  name: JavawebappService
  namespace: test-ns
spec:
  selector:
    app: Javawebapp
  ports:
  - port: 80
    targetPort: 8080   
++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++


#Pod AntiAffinity

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginxDeployment
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: nginxApp
  template:
    metadata:
      labels:
        app: nginxApp
    spec:
      containers:
      - name: nginxContainer
        image: nginx
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginxAppService
  namespace: test-ns
spec:
  selector:
    app: nginxApp
  ports:
  - port: 80
    targetPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappDeploymentPodAffinity
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: Javawebapp
  template:
    metadata:
      labels:
        app: Javawebapp
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx
            topologyKey: topology.kubernetes.io/zone         
      containers:
      - name: JavawebappContainer
        image: harishkumarbr/java-web-app:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
---        
apiVersion: v1
kind: Service
metadata:
  name: JavawebappService
  namespace: test-ns
spec:
  selector:
    app: Javawebapp
  ports:
  - port: 80
    targetPort: 8080
---
+++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++
       
# taint :


apiVersion: apps/v1
kind: Deployment
metadata:
  name: Javawebapptaints
spec:
  selector:
    matchLabels:
      app: Javawebapp
  template:
    metadata:
      labels:
        app: Javawebapp
    spec:
      tolerations:
      - effect: "NoSchedule"
        operator: "Exists"
      containers:
      - name: JavawebappContainer
        image: harishkumarbr/java-web-app:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: JavawebappService
spec:
  selector:
    app: Javawebapp
  ports:
  - port: 80
    targetPort: 8080
 +++++++++++++++++++++++++++
+++++++++++++++++++++++++++ 


