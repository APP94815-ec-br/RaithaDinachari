







How to troubleshoot linux servers or what are the ways to troubleshoot?

Ping the server where application is deployed.
Telnet the application url/ IP of server to check if the port is enable of not.
Ssh to the server and test the application using curl or wget with localhost and port.
Check the Harddisk space by df -h
Check the CPU utilization --> using top command
Check ram utilization --> by free command
Check the process if it's running or not by using ps -ef 
check if port is listning or not using netstat
Check the logs of your application or software.


EFK- helm:


Elastic stack

collect , process 






For  DevOps
	
	
	
	I would like to enhance my skills and thought to learn new things for career growth and 
	to compete with the existing IT world




AWS  
		  EC2
		  VPC-(Virtual private cloud)	
		  Subnet
		  Route table
		  Internet gateway
		  VPC peering	
		  Elastic Ips
		  NAt instance										
		  NAT gateway
		  S3 (Storage)
		  IAM		
		  Load BAlancer
          Autoscaling					
		  Route 53
		  Cloudwatch,SQS,SNS,SES
          Database
		  SSM (Patching)
		  AWS Lambda		
		  
Terraform  (Infrastructure as code)
GIT -Source Code Management Tool	
Maven - Build Artifacts
Jenkins - CI/CD
Docker  -Container Service   --H@r!$h1234  Hari@dec*2019
Kubernetes 	Orcestration tool for containers
Ansible	   - Configuration Management tool - Playbooks
Monitoring tools
   Nagios
   DataDog(EC2,Docker, Kubernetes)
   Promethus and Grafana(For Kubernetes)	

=========================



DevOps

Career RoadMap
----------

------

DevOps

1.Plan
2.Code
3.Build.
4.Deploy
5.Test
6.Release
7.Config/Operations
8.Monitoring 


Step a) 1 , 2 . Learn Programming languages
 
  Java,Python*

Step b) 3, 4 .  Learn Building Tools
		
   Build  -- Maven, Gradle
   SCM    --  Git* ,SVN ,Github*, BitBucket
   CI CD  -- Jenkins
   Container -- Docker ( Container Platform) ,Kubernetes (Container Management Platform)
   
Step c) 5. Learn how to run  tests from Command CI/CD
	  
Step d) 6. Learn cloud Service Platforms .
      
	   AWS, Azure , Google Cloud Platform
	   
Step e) 7. Learn Config Management Tools  	   
	  
	   Puppet,Chef, Ansible
	   
Step f) 8. Learn Logging/Monitoring Tools

         Nagios

----------------------------------------------------
DevOps

Programming language 
    Python
	 
Source Control
   Git Tool   -  Versioning 
   Github.com  - Repository
   
Operating Systems

   Linux
         BASH
		         Basic Linux Commands
				LS    APT/YUM
				GREP   LSOF
				UNAME  SS
				UPTIME	NETSTAT
				TOP     MOUNT
				MAN     TOP
				
				  /bin  /boot /dev  /etc /home /var /opt /proc  /mnt /sbin				  
				
         SSH				
   
Networking   

         DNS Name Resolution 
		 SUBNETTING
		 GATEWAYS
		 DHCP/NAT
		 HTTP
		 
		

    Farewalls                                 Load balancing                           Proxy Server
	
	Incoming/Outgoing                           Round Robin 							Traffic Flow
	Stateful/Non-Stateful						Weighted Round Robin                     Forward/Reverse
	Layer 3-7 Firewalls							Least Connections
  

Cloud Providers
	
     AWS    Azure**   Google Cloud
  
  
Infrastructure as Code

Containers
 
    Docker**
	
    Orchestration --	Kubernetes
	
	
Infrastructure Provisioning

    Terraform

Configuration Management
   
    Ansible** ,Puppet  Chef , Saltstack
	
	
CI/CD pipelines

   Deploy to-- 	
   
        Jenkins** ,  Gitlab , github Actions 
   
   
Log and Monitoring Management

    Nagios , ---   
 -------------------------------
 
 
  SQL
  shell  scripts /  python scripts
  
  Jenkins 5 
  
  CI/CD Pipelines
   pluggings
   
 github =
  git 
 build trigger
 
 Ansible
 
 AWS -
 
 Docker / K8s -- Container
 
 Yaml file
 
 ========================================
 
 DevOps
 

 K8s
 Docker
 Ansible
 AWS
***Jenkins ******
 Git
 Github
 Maven/npm
 Nexus
 STM/Webserver/Apache/Weblogic
 
 
 
 
 
+++++++++++++++++++

sed  - Stream editor:

sed 's/mango/apple/g'  filename 

sed  -i 's/mango/apple/g'  filename  ignore case

sed 's/mango//g'  filename 

sed '/mango/d'  filename   -- deleting line which matches mango

sed '/^$/d'  filename -- deleting empty lines

sed '1d'  filename -- deleting 1st time

sed '1,2d'  filename deleting 1st 2 time

sed 's/\t/ /g'  filename   -- replcace tabs to space

sed 's/ /\t/g'  filename   -- replcace space to tabs

sed -n 12, 18p   filename   ----- view 12 - 18 lines

sed 12,18d filename  --  to view all except 12 to 18 lines

sed G filename  - emptyline after 1 line

sed '8!s/mango/orange/g' filename    -- replace mango to orange in all lines except 8th line

sed 's/mango/orange/g' filename   -- replace mango to orange

in vi editor

:%s/mango/orange/


session 6

VPC:

10.0.0.0/24

32-24=8 
 2^8 = 256
 
 
    1 subnet  10.0.0.0/24  ---10.0.0.255/25
        
		10.0.0.0 /24
		10.0.0.1/24
		..
		10.0.0.255/25 ===> 256
		

		256 - 5 ==> 251 total 251 Ips
		
 -----------------------------------------/16  --- /28  -----
   2 subnets 256/2= 128
                         2^7 
						 32-7 =25
						 
		subnet 1 =
     		         10.0.0.0/25    ---  10.0.0.126/25
					
																		10.0.0.1/25
																		10.0.0.2/25
																		..
																		10.0.0.127/25
															
																			  128 - 5 = 123 -total IPs available for this Subnet 1	
					
        subnet 2 = 
		
		             10.0.0.128/25 -- 10.0.0.255/25
			  
																		10.0.0.128/25
																		10.0.0.129/25
																		
																		..
																		10.0.0.255/25
																		
																			   128 - 5 = 123 -total IPs available for this Subnet	2

one time maybe set subnet.
----------------------------------------------------------
VPC 
 10.0.0.0/24
 
	3  subnets 256/3 =
	
	               128 Ips 
	
	subnet 1 =
     		         10.0.0.0/25    ---  10.0.0.126/25
					
																			10.0.0.1/25
																			10.0.0.2/25
																			..
																			10.0.0.127/25
																
																		   128 - 5 = 123 -total IPs available for this Subnet 1	
						  
	subnet 2 
		
																		128/2 = 64 Ips 
																				   2^6 
																					32-6 =26

 					10.0.0.128/26	-- 10.0.0.192/26
					
																			10.0.0.129/26
																			..
																			10.0.0.192/26
																			
																			64 - 5 = Total 61 Ips
	subnet 3   64 Ips
		         10.0.0.193/26  -- 10.0.0.255/26
				 
																			 10.0.0.194/26
																			 ..
																			 10.0.0.255/26
																			 
																			64 - 5 = Total 61 Ips	
																			
																			
DHCP

Dynamic Host Configuration Protocol

Dynamically gives uniquekly Private Ips in that subnet ranges

																			
					
					
Session 7

v0.15	-Version of Terraform


terraform plan -target aws_s3_bucket.b		 #	aws_s3_bucket.b -==name of resource

	 Unique
	 
	 
	 +++++++++++++++++++++++
	 
POD and Service

apiVersion: v1
kind: Pod
metadata:
  name: mavenPod
  namespace: test-ns
  labels:
    app: mavenapp
spec:
  containers:
  - name: mavenContainer
    image: harishkumarbr/maven-web-app:1
    ports:
    - containerPort: 8080
---    
apiVersion: v1
kind: Service
metadata:
  name: mavenService:
  namespace: test-ns
spec:
  type: NodePort/ClusterIP
  selector:
    app: mavenapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 32019  
    # Optional
    
    
 +++++++++++++++++++++++++++++++++++++++++++   
 ++++++++++++++++++++++++++++++++++++++++++++++
 RC and Service
 
apiVersion: v1
kind: ReplicationController
metadata:
  name:mavenRC
  namespace: test-ns
  #labels:
   # app: mavenapp
spec:
  replicas: 1
  selector:
    app: mavenapp
  template:
    metadata:
      name: mavenPod
      namespace: test-ns
      labels:
        app: mavenapp
    spec:
    - name: mavenContainer
      image: harishkumarbr/maven-web-app:1
      ports:
      - containerPort: 8080
---
apiVersion: v1
kind:  Service
metadata:
  name: mavenServiceRC
spec:            
  type: NodePort/ClusterIP
  selector:
    app: mavenapp
  ports:
  - ports: 80
    targetPort: 8080
    NodePort: 310917
 # optional
+++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++

# RS

apiVersion: apps/v1
kind:  ReplicaSet
metadata:
  name: mavenRS
  namespace: test-ns
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mavenapp
  template:
    metadata:
      name: mavenPod
      namespace: test-ns
      labels:
        app:mavenapp
    spec:
      containers:
      - name: mavenContainer
        image: harishkumarbr/maven-web-app:1
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: mavenServiceRS
  namespace: test-ns
spec:
  type: NodePort
  selector:
   app: mavenapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 310454
+++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++    
   # Deployment Recreate
   
apiVersion: apps/v1
kind: Deployment
metadata:
  name:mavenDeploymentRecreate
  namespace: test-ns
spec:
  replicas: 2
  strategy:
    type: Recreate 
  selector:
    matchLabels:
      app: mavenapp
  template:
    metadata:
      name: mavenPod
      namespace: test-ns
      labels:
        app: mavenapp
    spec:
      containers:
      - name: mavenContainer
        image: harishkumarbr/maven-web-app:1
        ports:
        - containerPort: 8080
    
      
---        
apiVersion: v1
kind: Service
metadata:
  name: mavenServiceDeploymentRecreate
  namespace: test-ns
spec:
  type: NodePort
  selector:
   app: mavenapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 310454        
        
++++++++++++++++++++++++++
++++++++++++++++++++++++++


Deployment with Rolling Update

apiVersion: apps/v1
kind: Deployment
metadata:
 name: mavenDeploymentRollingUpdate
 namespace: test-ns
spec:
  replicas: 2
  strategy: 
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  minReadySeconds: 1    
  selector:
    matchLabels:
      app: mavenapp
  template:
    metadata:
     name: mavenPod
     namespace: test-ns
     labels:
       app: mavenapp
    spec:
      containers:    
      - name: mavenContainer
        image: harishkumarbr/maven-web-app:1
        ports:
        - containerPort: 8080
---
# service
apiVersion: v1
kind: Service
metadata:
  name: mavenServiceDeploymentRollingUpdate
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: mavenapp
  port:
  - ports: 80
    targetPort: 8080
 

++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++

#HPA Deployment

apiVersion: apps/v1
kind: Deployment
metadata:
  name: HPADeployment
  namespace: test-ns
spec:
  replicas: 2
  strategy: RollingUpdate
  selector:
    matchLabels:
      app: mavenapp
  template:
    metadata:
      name: HPAPod
      namespace: test-ns
      labels:
        app: mavenapp
    spec:
      containers:
      - name: mavenContainer
        image: harishkumarbr/maven-web-app:1
        ports:
        - containerPort: 8080      
        resources:
          requests:
            cpu: "200m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "600Mi"        
---        
apiVersion: v1
kind: Service
metadata:
  name: HPADeploymentService
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: mavenapp
  port:
  - ports: 80
    targetPort: 8080  
---
apiVersion: autoscaling/v2beta2
kind: HorizonatalPodCluster
metadata:
  name: HPA
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: HPADeployment
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 40
  - type: Resource
    resource:
      name: memory
      target:
      type: Utilization
      averageUtilization: 40    
      
  +++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++  
  # volume with HostPath
#Spring APP
apiVersion: apps/v1
kind: Deployment
metadata:
  name: SpringAppDeploymentHostPath
  namespace: test-ns
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
    template:
      metadata:
        name: springPod
        labels:
          app: springapp
      spec:
        containers:
        - name: SpringContainer
          image: harishkumarbr/spring-boot-mongo:1
          port:
          - containerPort: 8080
          env:
          - name: MONGO_DB_HOSTNAME
            value: mongodbService
          - name: MONGO_USER_NAME
            value: devdb
          - name: MONGO_PASSWORD
            value: devdb@123
---
#APP Service
apiVersion: v1
kind: Service
metadata:
 name:mongoservice
 namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
# Mongodb
apiVersion: v1
kind: ReplicaSet
metadata:
  name: MongodbRS
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      name: mongodbPod
      namespace: test-ns
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodbContainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123        
        resources:
          requests:
            cpu: "200m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "600Mi"  
        volumeMounts:
        - name: volumeHostPath
          mountPath: /data/db
      volumes:
      - name: volumeHostPath
        hostPath:
          path: /tmp/dbstore      
---
#mongodb Service
apiVersion: v1
kind: Service
metadata:
  name: mongodbService
  namespace: test-ns
spec:
  type: ClusterIP # default
  selector:
    app: mongodb
  ports:
  - port: 27017  
    targetPort: 27017
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
    # volume with NFS
    
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappDeploymentNFS
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      name: springPod
      labels:
        app: springapp
    spec:
      containers:
      - name: springappContainer
        image: harishkumarbr/spring-boot-mongo:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_USER_NAME
          value: devdb
        - name: MONGO_PASSWORD
          value: devdb@123  
---
#SpringService
apiVersion: v1
kind: Service
metadata:
  name: springappService
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 312001
---
# mongobsrs
apiVersion: v1
kind: ReplicaSet
metadata:
  name: mongodbRS
  namespace: test-ns
spec:
  replicas: 1
  selector:
    app: mongoapp
  template:
    metadata:
      name: mongoappPod
      labels:
        app: mongoapp
    spec:
      containers:
      - name: mongoContainer
        image: mongo
        ports:
        - containerPort: 27017
        volumeMounts:
        - name: volumenfs
          mountPath: /data/db
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123                  
      volumes:
      - name: volumenfs
        nfs:
          server: 10.23.32.123  #Ip of nfs server
          path: /mnt/share # path in nfs server        
---
#mongodbService
apiVersion: v1
kind: Service
metadata:
  name: mongo
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017
---     
    
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#PV 
#PVC     with hostpath
        
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappDeploymentpvc
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappContainer
        image: harishkumarbr/spring-boot-mongo:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_USER_NAME
          value: devdb
        - name: MONGO_PASSWORD
          value: devdb@123         
---
apiVersion: v1
kind: Service
namespace: test-ns
metadata:
  name: springappService
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongoContainer
        image: mongo
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123 
        volumeMounts:
        - name: pvc_hostpath
          mountPath: /data/db   
      volumes:
      - name: pvc_hostpath
        persistentVolumeClaim:
          claimName: mypvc-hostpath
          

---
apiVersion: v1
kind: Service
metadata:
  name: mongo
  namespace: test-ns
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc-hostpath
  namespace: test-ns
spec:
  resources:
    requests:
      storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mypv-hostpath
  namespace: test-ns
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /tmp/dbstore
  #persistentVolumeReclaimPolicy: Recycle default = delete
  #storageClassName: slow
  #mountOptions:
   # - hard
    #- nfsvers=4.1
  #nfs:
   # path: /tmp/dbstore
    #server: 172.17.0.2
      
 ++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++++++++++++++++++++++++++++++++++++++++++++++++++++++
#PV
#PVC with nfs
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappDeploymentpvc
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappContainer
        image: harishkumarbr/spring-boot-mongo:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_USER_NAME
          value: devdb
        - name: MONGO_PASSWORD
          value: devdb@123         
---
apiVersion: v1
kind: Service
namespace: test-ns
metadata:
  name: springappService
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongoContainer
        image: mongo
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123 
        volumeMounts:
        - name: pvc_nfs
          mountPath: /data/db   
      volumes:
      - name: pvc_nfs
        persistentVolumeClaim:
          claimName: mypvc-nfs
          

---
apiVersion: v1
kind: Service
metadata:
  name: mongo
  namespace: test-ns
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc-nfs
  namespace: test-ns
spec:
  resources:
    requests:
      storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mypv-nfs
  namespace: test-ns
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  #hostPath:
   # path: /tmp/dbstore
  #persistentVolumeReclaimPolicy: Recycle default = delete
  #storageClassName: slow
  #mountOptions:
   # - hard
    #- nfsvers=4.1
  nfs:
    path: /tmp/dbstore
    server: 172.17.0.2
 	 
+++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++     
# PVC + StorageClass (PV wil auto created once confogured StorageClass) 
 
#SpringApp
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappDeployment
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappContainer
        image: harishkumarbr/spring-boot-mongo:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_USER_NAME
          value: devdb
        - name: MONGO_PASSWORD
          value: devdb@123        
---        
#APP Service
apiVersion: v1
kind: Service
metadata:
  name: springappService
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 30123 # 30000 - 32767
---
#Mongo DB App
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodbdeployment
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongoContainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
        volumeMounts:
        - name: pvc_mongo
          mountPath: /data/db
      volumes:
      - name: pvc_mongo
        persistentVolumeClaim:
          claimName: pvc_storage_pv
---
# PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc_storage_pv
  namespace: test-ns
spec:
  resources:
    requests:
      storage: 100mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
---
#mongo service
apiVersion: v1
kind: Service
metadata:
  name: mongo
  namespace: test-ns
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017 
 +++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++
# ConfigMap and Secrets + APP , Service , Mongo DB + Service + PVC +  



#Config Map
apiVersion: v1
kind: ConfigMap
metadata:
  name: springAppConfigMap
  namespace: test-ns
data:
  userName: devdb
---
#Secrets
apiVersion: v1
kind: Secret
metadata:
  name: springAppSecret
  namespace: test-ns
type: Opaque
stringData:
  password: devdb@123
---
#SpringApp
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappDeployment
  namespace: test-ns
spec:
  replicas: 2
  strategy: Recreate
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappContainer
        image: harishkumarbr/spring-boot-mongo:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_USER_NAME
          valueFrom:
            configMapKeyRef:
              name: springAppConfigMap
              key: userName
          #value: devdb
        - name: MONGO_PASSWORD
          valueFrom:
            secretKeyRef:
              name: springAppSecret
              key: password
        #  value: devdb@123        
---        
#APP Service
apiVersion: v1
kind: Service
metadata:
  name: springappService
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 30123 # 30000 - 32767
---
#Mongo DB App
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodbdeployment
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongoContainer
        image: mongo
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          #value: devdb
          valueFrom:
            configMapKeyRef:
              name: springAppConfigMap
              key: userName
        - name: MONGO_INITDB_ROOT_PASSWORD
          #value: devdb@123
          valueFrom:
            secretKeyRef:
              name: springAppSecret
              key: password
        volumeMounts:
        - name: pvc_mongo
          mountPath: /data/db
      volumes:
      - name: pvc_mongo
        persistentVolumeClaim:
          claimName: pvc_storage_pv
---
# PV
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc_storage_pv
  namespace: test-ns
spec:
  resources:
    requests:
      storage: 100mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
---
#mongo service
apiVersion: v1
kind: Service
metadata:
  name: mongo
  namespace: test-ns
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017
 +++++++++++++++++++++++++++++++++++++++++++++++
 +++++++++++++++++++++++++++++++++++++++++++++++
 
 #liveness Probe and Readiness Probe  + ConfigMap + Secrets + PVC
apiVersion: apps/v1 
kind: Deployment
metadata:
  name: springappdeployment-liveness-readiness
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappPod
        image: harishkumarbr/spring-boot-mongo:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_USER_NAME
          valueFrom:
            configMapKeyRef:
              name: springAppConfigMap
              key: userName
          #value: devdb
        - name: MONGO_PASSWORD
          valueFrom:
            secretKeyRef:
              name: springAppSecret
              key: password
        #  value: devdb@123        
        livenessProbe:
          httpGet:
            path: /java-web-app
            port: 8080
            initialDelaySeconds: 60
            periodSeconds: 30 # time interval
            timeoutSeconds: 10 #  response time 
        readinessProbe:
          httpGet:
            path: /java-web-app
            port: 8080
            initialDelaySeconds: 40
            periodSeconds: 30
            timeoutSeconds: 5
---
 # app service
apiVersion: v1
kind: Service
metadata:
  name: springappService
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
#Mongo DB App
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodbdeployment
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongoContainer
        image: mongo
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          #value: devdb
          valueFrom:
            configMapKeyRef:
              name: springAppConfigMap
              key: userName
        - name: MONGO_INITDB_ROOT_PASSWORD
          #value: devdb@123
          valueFrom:
            secretKeyRef:
              name: springAppSecret
              key: password
        volumeMounts:
        - name: pvc_mongo
          mountPath: /data/db
      volumes:
      - name: pvc_mongo
        persistentVolumeClaim:
          claimName: pvc_storage_pv
---
# PV
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc_storage_pv
  namespace: test-ns
spec:
  resources:
    requests:
      storage: 100mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
---
#mongo service
apiVersion: v1
kind: Service
metadata:
  name: mongo
  namespace: test-ns
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017
---
#Config Map
apiVersion: v1
kind: ConfigMap
metadata:
  name: springAppConfigMap
  namespace: test-ns
data:
  userName: devdb
---
#Secrets
apiVersion: v1
kind: Secret
metadata:
  name: springAppSecret
  namespace: test-ns
type: Opaque
stringData:
  password: devdb@123       

++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++
              
 ## Stateful set
 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongodb
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: mongodb
  serviceName: mongodb-Service
  replicas: 2
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongoContainer
        image: mongo
        ports:
        - containerPort: 27017
        volumeMounts:
        - name: persistent-volume-stateful
          mountPath: /data/db
  volumeClaimTemplates:
  - metadata:
      name: persistent-volume-stateful
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
---
#mongodb Service
apiVersion: v1
kind: Service
metadata:
  name: mongodb-Service
  namespace: test-ns
  labels:
    name: mongodb
spec:
  selector:
    app: mongodb
  ClusterIP: None # Headless Service  
  ports:
  - port: 27017
    targetPort: 27017
---
#Spring APP

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappDeploymentStatefulSet
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappContainer
        image: harishkumarbr/spring-boot-mongo:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongodb-Service
        - name: MONGO_USER_NAME
          valueFrom:
            configMapKeyRef:
              name: springAppConfigMap
              key: userName
          #value: devdb
        - name: MONGO_PASSWORD
          valueFrom:
            secretKeyRef:
              name: springAppSecret
              key: password
        #  value: devdb@123         
---
apiVersion: v1
kind: Service
metadata:
  name: springappService
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 32323
              
---
#Config Map
apiVersion: v1
kind: ConfigMap
metadata:
  name: springAppConfigMap
  namespace: test-ns
data:
  userName: devdb
---
#Secrets
apiVersion: v1
kind: Secret
metadata:
  name: springAppSecret
  namespace: test-ns
type: Opaque
stringData:
  password: devdb@123       

++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++

## Node Selector

apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappDeploymentNodeSelector
spec:
  selector:
    matchLabels:
      app: Javawebapp
  template:
    metadata:
      labels:
        app: Javawebapp
    spec:
      replicas: 2
      nodeSelector:
        name: workerNode
      containers:
      - name: JavawebappContainer
        image: harishkumarbr/java-web-app:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: JavawebappService
spec:
  type: NodePort
  selector:
    app: Javawebapp
  ports:
  - port: 80
    targetPort: 8080
    NodePort: 31092

++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++

          
# Node affinity -requiredDuringSchdedulingIgnoredDuringExecution
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappDeploymentNodeAffinity
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  minReadySeconds: 60
  selector:
    matchLabels:
      app: Javawebapp
  template:
    metadata:
      labels:
        app: Javawebapp
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchdedulingIgnoredDuringExecution:
            nodeSelectorsTerms:
            - matchExpressions:
              - key: "node"
                operator: In
                values:
                - workernode
      containers:
      - name: JavawebappContainer
        image: harishkumarbr/java-web-app:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: JavawebappService
spec:
  selector:
    app: Javawebapp
  ports:
  - port: 80
    targetPort: 8080
++++++++++++++++++++++
+++++++++++++++++++++

 ## Node affinity -prefferedDuringSchedulingIgnoredDurindExecution
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappDeploymentNodeAffinity
  namespace: test-ns
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  minReadySeconds: 60
  selector:
    matchLabels:
      app: Javawebapp
  template:
    metadata:
      labels:
        app: Javawebapp
    spec:
      affinity:
        nodeAffinity:
          prefferedDuringSchedulingIgnoredDurindExecution:
          - weight: 1
            preference:
            - matchExpressions:
              - key: "node"
                operator: In
                values:
                - workerone
      containers:
      - name: JavawebappContainer
        image: harishkumarbr/java-web-app:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: JavawebappService
  namespace: test-ns
spec:
  selector:
    app: Javawebapp
  ports:
  - port: 80
    targetPort: 8080
+++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++

# Pod Affinity and Pod AntiAffinity

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginxDeployment
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: nginxApp
  template:
    metadata:
      labels:
        app: nginxApp
    spec:
      containers:
      - name: nginxContainer
        image: nginx
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginxAppService
  namespace: test-ns
spec:
  selector:
    app: nginxApp
  ports:
  - port: 80
    targetPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappDeploymentPodAffinity
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: Javawebapp
  template:
    metadata:
      labels:
        app: Javawebapp
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx
            topologyKey: topology.kubernetes.io/zone
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: security
                operator: In
                values:
                - maven
          topologyKey: topology.kubernetes.io/zone            
      containers:
      - name: JavawebappContainer
        image: harishkumarbr/java-web-app:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
---        
apiVersion: v1
kind: Service
metadata:
  name: JavawebappService
  namespace: test-ns
spec:
  selector:
    app: Javawebapp
  ports:
  - port: 80
    targetPort: 8080   
++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++


#Pod AntiAffinity

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginxDeployment
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: nginxApp
  template:
    metadata:
      labels:
        app: nginxApp
    spec:
      containers:
      - name: nginxContainer
        image: nginx
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginxAppService
  namespace: test-ns
spec:
  selector:
    app: nginxApp
  ports:
  - port: 80
    targetPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappDeploymentPodAffinity
  namespace: test-ns
spec:
  selector:
    matchLabels:
      app: Javawebapp
  template:
    metadata:
      labels:
        app: Javawebapp
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx
            topologyKey: topology.kubernetes.io/zone         
      containers:
      - name: JavawebappContainer
        image: harishkumarbr/java-web-app:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
---        
apiVersion: v1
kind: Service
metadata:
  name: JavawebappService
  namespace: test-ns
spec:
  selector:
    app: Javawebapp
  ports:
  - port: 80
    targetPort: 8080
---
+++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++
       
# taint :


apiVersion: apps/v1
kind: Deployment
metadata:
  name: Javawebapptaints
spec:
  selector:
    matchLabels:
      app: Javawebapp
  template:
    metadata:
      labels:
        app: Javawebapp
    spec:
      tolerations:
      - effect: "NoSchedule"
        operator: "Exists"
      containers:
      - name: JavawebappContainer
        image: harishkumarbr/java-web-app:1
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: JavawebappService
spec:
  selector:
    app: Javawebapp
  ports:
  - port: 80
    targetPort: 8080
 +++++++++++++++++++++++++++
+++++++++++++++++++++++++++ 






















+




















LINUX:	



cd
mkdir
ls
touch
rmdir
rm -rf



find . -type f -empty
find ~ -type d -empty

find . -iname harish.txt


umask
0022 -- root
0002 -- normal user

r w x
4 2 1
0777 - dir
0666 - file

chmod 
chmod -R 400 devOps

chown
chown root devOps.txt
chmod -R  root devOps


sudo su 
sudo su -

session 4:

chgrp

chgrp root devOps.txt
chgrp -R  root devOps



chown ec2-user:ec2-user devOps.txt
chown -R ec2-user:ec2-user devOps


cp

cp source destination --> file
cp -r source destination  --> directory

cp *.txt devOps


mv

mv oldname newname


file 1.txt  -------> to decide the content of file

wc

ln /var/logs/a.txt  /tmp/b.txt

ln -s logs/1.txt /tmp/b.txt

vi
vim

session 5

vi ~/.ssh/config

Host *
     ServerAliveInternal 30
	 ServerAliveCountMax 2
	 
nano test.txt

echo

cat
head
tail
sed

cat -n filename.txt
 
+++++++++++++++++++

sed  - Stream editor:

sed 's/mango/apple/g'  filename 

sed  -i 's/mango/apple/g'  filename  ignore case

sed 's/mango//g'  filename 

sed '/mango/d'  filename   -- deleting line which matches mango

sed '/^$/d'  filename -- deleting empty lines

sed '1d'  filename -- deleting 1st time

sed '1,2d'  filename deleting 1st 2 time

sed 's/\t/ /g'  filename   -- replcace tabs to space

sed 's/ /\t/g'  filename   -- replcace space to tabs

sed -n 12, 18p   filename   ----- view 12 - 18 lines

sed 12,18d filename  --  to view all except 12 to 18 lines

sed G filename  - emptyline after 1 line

sed '8!s/mango/orange/g' filename    -- replace mango to orange in all lines except 8th line

sed 's/mango/orange/g' filename   -- replace mango to orange

in vi editor

:%s/mango/orange/


less

more
sort
|
tr

grep -i harish devOps.txt

grep  harish devOps.txt 1.txt



who
who -H

last

w

uptime

lscpu

 cat /proc/cpuinfo
 
 
 users
 
 whoami
 
 whereis cat
 whereis java
 
 which ls
 
 date
 
 df -kh
 
 du -sh ~
 
 hostname
 
 hostname -i
 ifconfig
 ip a
 
 
 man
 man ls
 
 info mkdir
 
 mkdir --help
 
 
 whatis
 
 whatis mkdir
 
 
 
 service
 systemctl list-unit-files
 
 service sshd status
 systemctl status sshd
 
 
 chkconfig sshd on
 
 ps
 
ps -ef
 
kill

top

sar

zip -r  devOps.zip devOps
unzip
tar


#useradd harish
#passwd harish

chage harish

groupadd devOps

usermod -g groupname username

usermod -L username --------> lock user

usermod -U username ----->unlock

id username
groups username

usermod -aG g1 g2 g3 username


lid -g groupname

sudo su - username
sudo -iu username
 su - username
 
 visudo
   vi /etc/sudoers  
 
 userdel
 groupdel
 
 
 userdel -r username
 
 
 Crontable
 
 crontab
 
	
ssh

ssh userName@ip
or

ssh userName@hostname

with pasword
vi /etc/ssh/sshd_config
PasswordAuthentication yes

service sshd restart

scp filename userName@ip:/tmp/
or
scp filename userName@hostname:/tmp/

df -h  -- harddisk utlization 
du -sh


free -kh - ram size

mail


wget

curl -o

tee 

ls |tee  output.txt
-- display output  in console adn aslo so store in the file 

script
 
 script -a  output.txt
 exit

cal

    
    telnet
    ping
    history
    
netstat tulnpa
watch

shutdown

++++++++++++++++++++

cat /etc/shells

echo $$0
echo $SHELL
echo ps -p $$


#! /bin/bash

./filename.sh
. filename
sh filename.sh

/bin/sh
/bin/bash


sh -x filename.sh   

set -x


set +x

comments:
#

<<H

H


/*



*/



Display System defined Variables:

env
print env

comgen -v

pwd -- command

PWD -- variable name 


echo $ HISTSIZE

export HISTSIZE=200


set HISTSIZE permanent::specific user

vi ~/.bash_profile
export HISTSIZE=200



set HISTSIZE permanent::all user  run in root

vi /etc/profile
export HISTSIZE=200

echo $LOGNAME


a=10
b=10.2
c=harish
d=Tech Mahindra


sh dbbackup.sh arg1 arg2  .. args10

$0

$1   1st argument
$2
$3 
..
..
${10}

$#  number of Arguments

$*  print ALL arguments
$@  or $*
$$
$?



if [ $# -eq 2]
then
..
..

fi

String

var="Malnad College of Engineering"

echo ${#var}   -- length of the String

echo ${var:20}  

echo ${var 1:20}

echo ${var:20:14}

echo ${var:0:14}

echo ${var:-8}


Arthematic Opertaion:

expr 1 + 2
expr 2 - 3
expr 2 \* 9
expr 4 / 2

sum =`expr 1 + 2`

echo "addtion is :" `expr 2 + 3`
echo "search:" `$USER` 

-------------------------

sh commandLineArthematicOperation.sh 4 8



echo "Enter values of a is: " $1 
echo "Enter values of b is: " $2

echo "addtion of a and b :"` expr $1 + $2` 

echo "multiplication of a and b : "` expr $1 \* $2` 


--------------------------




echo "Please enter DevOps Tools.."
read -a  devopsTool

echo "entered tools are :"${devopsTool[*]}
echo "enter the position to cehck the respective value"
read n
b=`expr n - 1`
echo "entered tools  in" $n"st position:"${devopsTool[$b]}

echo "entered tools  in 1st position:"${devopsTool[0]}
echo "entered tools  in "$n"th position:"${devopsTool[`expr $n - 1`]}
# Array

--------------------------
echo " name is"
read

echo "stored in "$REPLAY
----------------------
#online printing
 
read -p "enter name:"  userName
 
read -ps "enter passsword:" passsword
echo
echo "enter username and password is " $userName $password

-----------

echo "Enter values of a"
read a 

echo "Enter values of b"
read b

echo "Enter values of a is: " $a 
echo "Enter values of b is: " $b

echo "sum  of a and b"` expr $a + $b` 

--------------------------------------

Input and Output direction symbols:

>  redirect std o/p
>> append std o/p
<  std input

0 std i/p
1 std Output
2 std error


ls > list.txt

cat < list.txt


sh hello.sh >output.log   #  0nly output in output.log file, error in console

sh hello.sh >output.log 2>&1  #both error  and output in output.log file


sh hello.sh 2>error.log 1>output.log # error in error.log , output in output.log


Control Statements:

if [ condition]
then
..
..
fi

----
if [ condition ]
then
...
...

else

fi

-----
if [ condition ] ; then
.....
.....
else
....
...
fi
-------------
a=20
b=30

if [ $a -gt $b ]
then
echo "$a is greater than $b"
else
echo "$b is greater than $c"
fi

----------------------

a=20
b=30
c=50
if [[ ($a -gt $b) && ($b -gt $c) ]]
then
echo "$a is greater than $b and $c"
elif [[ ($b -gt $c) && ($b -gt $a) ]]
echo "$b is greater than $c and $a"
else 
echo "$c is greater than $a and $b"
fi
------------------

sh search.sh

echo "enter the file name to search"
read filename
if [[ -f $filename ]]
then
echo " $filename is present in current directory"
echo "displaying the content in $filename"
cat $filename
else
echo " $filename is  not present in current directory"
echo "Do you want to create it"
echo "print yes or y or Y or Yes  ---> for creating it"
read value
if [[ $value -eq 'yes' || $value -eq 'Yes' || $value -eq 'Y' || $value -eq 'y' ]]
then
touch $filename
echo "$filname is creating.."
fi
fi

------------
if [[ -d $directory ]]


------------
if [[ -r $filename ]]

------------
if [[ -w $filename ]]

------------
if [[ -x $filename ]]

----------------------------------------
Loops

for  (( initilisation ; condition : inc/dec ))
do
..
....
..

done
-----
echo "demo of for loop"
for (( a=1;a<=5;a++ ))
do 
echo "$a"
done

echo "demo done"
----

for i in {100 .. 1}

--------------------------
while demo
initilisation
while ( condition )
do
...
....
inc/dec
done
----------------
echo "while loop demo starts"
a=5
while ( $a -ge 0 )

echo "time left "$a "Second"

a=`expr $a - 1`
done
echo "done While Demo"

----------------------

-le
-lt
-gt
-ge
-eq
------------------

switch case:

#instead of nested if else we can use switch case:
sh sonar.sh

echo "Enter option to to start ,stop ,restart"
read option

case $option in
start) 
echo "starting sonarQube"
        echo "starting......." ;;
stop) 
 echo "Stoping sonarQube"
        echo "Stoping......."
		;;
restart) 
echo "Restarting sonarQube"
        echo "Restarting......."
*)
echo "enter valid option as start or stop or restart
  usage of $0 script to start or stop or restart"
  ;;
easc

-------

sh sonar.sh start|stop|restart


case $option in
start) 
echo "starting sonarQube"
        echo "starting......." ;;
stop) 
 echo "Stoping sonarQube"
        echo "Stoping......."
		;;
restart) 
echo "Restarting sonarQube"
        echo "Restarting......."
*)
echo "enter valid option as start or stop or restart
  usage of sh $0 script to start or stop or restart"
  ;;
easc
---------------------------
convertScript.sh

 echo "enter number only as 1 to 5 "
 read number
 case $number in
 1) echo "you have typed $number is one"
 ;;
 2) echo "you have typed $number is two"
 ;;
 3) echo "you have typed $number is three"
 ;;
 4) echo "you have typed  $number is four"
 ;;
 5) echo "you have typed  $number is five"
 ;;
 *)
 echo "enter valid number from 1- 5 only"
 sh convertScript.sh
 ;;
 easc


------------------------------------------------------------

functions :

functionName(){



}

functionName


----------

sh function.sh
echo "function demo"
greeting(){
echo "Hi All,"
echo ""
echo  "
Have a nice day."
echo "
Regards,
Harish "
}

echo "callling function"

greeting

echo "called functions"
---------------------
sh utilities.sh

add()
{
echo " sum"
}

sub(){
echo "sub"
}
----------------
sh sample.sh

multi()

{ echo "multiplication"
}
-----------------
sh a.sh

source ./utilities.sh
source ./sample.sh
add
sub
echo "calling add , sub , multi functions in utilities.sh script $0 script"
------------
sh b.sh

source ./utilities.sh
source ./sample.sh
add
sub

echo "calling add , sub , multi functions in utilities.sh script in $0 script"

+++++++++++++++++++++++++++++++=



Git:

Enterprise Edition

Gitlab
Github
BitBucket(Stash)


login

username: harishkumarbr
email :harishkumarbr8@gmail.com
Password: H@r!$h1234


1: create Organization
   name of Organization: APP94815-ec-br
   email : ServiceId/Functional Mai ID
   
   https://github.com/APP94815-ec-br
2: Create a Repository

    Repository name:
	
	Public /Private 
	
https://github.com/APP94815-ec-br/Wallmart -- public repo
	

https://github.com/APP94815-ec-br/Amazon.git -- Private repo


3: Create Teams

https://github.com/orgs/APP94815-ec-br/teams/raithadinachari-teams

    Add Members to team
	Add Team to Repository
	

4: Provide the Repository access to the Team	

  	https://github.com/orgs/APP94815-ec-br/teams/raithadinachari-teams/repositories
	
	write
	read
	
	
----------
git

cd wallmart

git init

git config --global user.name ""
$ git config --global user.name "harishkumarbr"
$ git config --global user.email "harishkumarbr8@gmail.com"
git config --global --list


git status

git add .
git add filename1 filename2
git add *.java
git add *

git commit -m "some msgs"

git commit -m "some mgs" filename1 filename2


git remote add aliasname url

git remote add RD https://github.com/APP94815-ec-br/RaithaDinachari.git

git remote add origin https://github.com/APP94815-ec-br/RaithaDinachari.git



git remote -v
	

git push RD master
git push origin master

gti push origin main

PAT:

personal access token

git commit -a  -m "updated"	

git log
git log -2
git log --online


git show <commit_id>

# only file name:
git show --pretty="" --name-only <commit_id>

git remote remove aliasname  
git remote remove RD










tag:


git tag tagName

git push aliasname tagName

git tag -d tagName

stash:

git stash

git stash list

stash@{0}  -->latets
stash@{1}
stash@{2}


git stash apply
git stash apply stash@{1}

git stash drop
git stash drop stash@{1}

git stash pop
git stash pop stash@{1}


  cheery-pick:

git commit -a -m "msg1"
git commit -a -m "msg2"
git  checkout branchName

git cheery-pick <commit_id>

      Clone:


git clone <url>


git pull origin master


git fetch origin master  + git merge origin/master


ssh-keygen
.
.

##configuring public key in github


ls -la ~/.ssh

id_rsa
id_rsa.pub


cat ~/.ssh/id_rsa.pub


ssh -T git@github.com  


git@github.com:harishgowdabr/maven-web-application.git


ssh-keygen
shh-copy-id



git  API:

sshkeygenandupload.sh file by bhaskar

Branching strategy



git commit -a --aemnd -m  ""

git branch -m oldbranch newbranch

git branch -m newbranch

master>git checkout feature 

git checkout -b newbranch master



Branching

git branch

git branch branchName

git branch -d branchName

git checkout branchName

git diff branchName

git merge branchName

Merge Conflicts

git checkout -b newBranchName

git push AliasName branchName1 branchName2

git push AliasName --all

git push AliasName :branchName # branch is deleted in Remote repo

git push AliasName branchName -d


PR - Pull Request



Tags:

git tag

git tag tagName

   git tag wallmartv1.0.0

git tag -d tagName

git push AliasName --tags

git push AliasName tagName


git pull AliasName branchName 



git fetch AliasName branchName















-----------------------


Maven



Build Tools:

Maven:

Java ,JDK


javac hello.java

java hello


javac --version ------------> JDK

JRE ---

java :

java -version 

mvn -version

boot 
bin
conf - settings.xml***
lib
pom.xml

mvn clean package

wallmart.xml

mvn -f wallmart.xml  clean package



maven repo:

maven local repositories

maven central repositories

maven remote repositories

~/.m2/repository


life cycles


Life Cycle      GoalName
clean            clean:

site              site

default             validate:
					compile
					test	
					package
					install
					deploy
						


mvn clean package -Dskiptests
mvn clean package -Dmaven.test.skip=true

settings.xml

<localRepository>path</localRepository>


mvn clean package  -pl moduleName
----------------------------------------------------------

Tomcat:



Tomcat is an OS, Java Based, Web App server..


Application Servers
-------------------

1.x to 7.x --> JBoss
8.x to latest version --> Wildfly


JBoss/Wildfly --> RedHat --> IBM
Weblogic --> BEA --> Oracle
WebSphere Application Server (WAS) --> IBM


Jboss/Wildlfy --> Enterprise APp servers
bin/ :

startup.sh
shutdown.sh
startup.bat

sh catalina.sh start
sh catalina.sh stop
version.bat


conf/:

tomcat-users.xml  
server.xml-- line number 69

lib/:
jar files

logs/:

catalina.out  , manager.log ,  


webapps/:
all deployed apps,

5 default apps

work/


tmp/



--------Installation
Java 



curl -o apache-tomcat-9.0.56.zip https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.56/bin/apache-tomcat-9.0.56.zip

or

wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.56/bin/apache-tomcat-9.0.56.zip


unzip apache-tomcat-9.0.56.

cd /opt/apache-tomcat-9.0.56/bin


chmod u+x *.sh


ln -s /opt/apache-tomcat-9.0.56/bin/startup.sh /usr/bin/startTomcat
ln -s /opt/apache-tomcat-9.0.56/bin/shutdown.sh /usr/bin/stopTomcat





roles:

manager-gui
admin-gui
manager-script

in config
server.xml

change port line number  69



in config
tomcat-users.xml
	  <user username="admin" password="admin" roles="manager-script"/>
 <user username="admin" password="admin" roles="manager-gui,admin-gui"/



  In Webapps - Manager -- META-INF--context.xml --valve commented
         HostManager -valve commented 
		 
		 		 



Web Servers
-----------

Apache HTTP server
Nginx (Engine X)

IBM HTTP Server (IHS)

IIS




------Installation HTTP Server--

yum install httpd -y


systemctl start httpd

cd /var/www/html/

index.html


/etc/httpd/conf 
httpd.conf -- change port number	 line number 45




----------SonarQube------------------------

Open source ,Code quality Management tool

source code validation , review , coverage , analysis 


Code Review:
  Standards 
Code Coverage:
   unit test case
   
 Sonar
Java 

supports many lanaguage

 Duplicate code
 comments
 Architecture and design
	
Sonar Installation:

java 1.8
sonar 7.8

	Directories
	
	bin ,conf,logs , lib , 
	/opt/sonarqube-7.8/conf
	sonar.properties --change of port  
                   #sonar.web.port=9000

	
	/bin ---linux..
	
	sonar.sh should run in normal user
	-
	useradd sonar
	
	Give the sudo access to sonar user
visudo

sonar   ALL=(ALL)       NOPASSWD: ALL

Change the owner and group permissions to /opt/sonarqube-7.8/ directory.
chown -R sonar:sonar /opt/sonarqube-7.8/
chmod -R 775 /opt/sonarqube-7.8/

su - sonar
cd /opt/sonarqube-7.8/bin/linux-x86-64/

./sonar.sh start

	
--
administration 	--Security --Force user authentication - enable

Architecture:

Sonarqube Scanner -- Identify and reports genaration

SonarQube Server

 Compute Engine 
   its catagories below
			verneralibites
			Bugs
			Code Smells
			
  H2 -- store reports
  
  web server :
     display reports  in dashboard
	
   there is a search Engine in dashboard fot quick response 
   
   
   mvn  sonar:sonar # to run sonarqube 
   plugging:GoalName
   
   plugging extarts sonar scanner
   
   sudo su - sonar
   
   cd sonar/7.8/bin /linux
   
   sh.sonar.sh start
   if u run this in root user 
  switch to sonar user
check logs   
   /opt/sonar7.8/logs
    rm -rf /opt/sonar7.8/temp
	
cd 	opt/sonar7.8/bin/linux-x86-64/
	sh sonar.sh start
	
	Executing sonar reports:
	
	Maven - Java

pom.xml

<properties>
          SonarQube Server Details
 <sonar.host.url>http://13.234.66.155:9000/</sonar.host.url>
		<sonar.login>admin</sonar.login>
		<sonar.password>admin</sonar.password>

</properties>
	
	
	mvn clean sonar:sonar package
	
	Administration --Security--users-Token
	
	Token:
	
	7039883a8c91a3ca3ad24493e12ea32590421a12
	
	<properties>
 --- SonarQube Server Details
      <sonar.host.url>http://13.234.66.155:9000/</sonar.host.url>
		<sonar.login>7039883a8c91a3ca3ad24493e12ea32590421a12</sonar.login>
		
		
		mvn sonar:sonar
		
		
project --
Issues --
Rules--
Quality Profile-  Group of  Rules which are going to apply while executing the sonarqube report
sonar way is the default QP
Quality gates  --- collection of condition which are going to apply while executing the sonarqube report to mark as pass or 
               fail.
        
Administration -- 
    
		security  -
     		users 
		
		     users will cretaed under sonar-users group by default
		
		security  -
       		groups
						sonar-users
						sonar -administrators
						
						
		project -- to delete unknown projects
       
	   System--
              info of sonarqube 


SonarQube , Fortify 
------------
Mysql -- 3306 -port

0-----------------------------------------------------------
Nexus:
jfrog Artifactory 

SonarType Nexus
Nexus is an OSS, Java Based, Artifactory Repo.

It can be used to store the build artifacts(packages)

and retrieve the build artifcats whenever we required.		  
		
		
		java 1.8
		nexus3.36.2
		
/opt/sonatype-work/nexus3/log -- logs of nexus server
sonar.log

/opt/nexus3.36.2/etc
nexus-default.properties
   application-port=8081  -- to change port

   nexus3.36.2/bin
     nexus -------file to start nexus  server
	 
	 http://3.110.191.155:8081/
	 
	 CAN CEHCK Password HERE 
	 
	 /opt/sonatype-work/nexus3/admin.password
	 
	 admin
	 admin
	 
Server administration and configuration
  Repository
      Create Repository  (maven2 hosted - 2repo)
             version policy   --Release 
              version policy  -- Snapshot	


http://3.110.191.155:8081/repository/Walmart-release/

http://3.110.191.155:8081/repository/Walmart-snapshot/

			  
	 Nexus repo:
	 
	 In POM.xml
<distributionManagement>
	
	    <repository>
	      <id>nexus</id>
	      <name>Mithun Technologies Releases Nexus Repository</name>
	      <url>http://3.110.165.154:8081/repository/Walmart-release/</url>
	    </repository>
	    
	    <snapshotRepository>
	      <id>nexus</id>
	      <name>Mithun Technologies Snapshot Nexus Repository </name>
	      <url>http://3.110.165.154:8081/repository/Walmart-snapshot/</url>
	    </snapshotRepository>
	    
	</distributionManagement>


Nexus Credentials
-----------------
in  maven HD conf/settings.xml 

<servers>
..
...
.
.
 <server>
      <id>nexus</id>
      <username>admin</username>
      <password>admin</password>
    </server>
	


  </servers>
  
  
  mvn clean sonar:sonar deploy
  

error: if we deploy same version then we will get below error:


status: 400 Repository does not allow updating assets: Walmart-release


if we want redeploy

repositories -release  -Hosted -Deployment polices - Allow redeploy
   
-----
   Maven proxy
   
  http://3.110.165.154:8081/repository/Wallmart-Proxy-repo/
   
   maven2 proxy 
   mvn clean package
      search dependencies in maven local repo ~/.m2/repository  1st then proxy and create build .war  </target>
   and 
 it will store the dependencies in remote repo 
   mvn clean deploy
      search dependencies in maven local repo ~/.m2/repository  1st then proxy and create build .war  </target> and 
	      stores .war in remote  repo 
	     and also 
 it will store the dependencies in remote repo 
 
   
   In settings.xml
   
     <mirrors>
    <mirror>
      <id>nexus</id>
      <mirrorOf>*</mirrorOf>
      <name>Proxy repo</name>
      <url>http://3.110.165.154:8081/repository/Wallmart-Proxy-repo/</url>
 
    </mirror>
  </mirrors>

   
   In POM.XML
   
   <repositories>

  <repository>
   <id>nexus</id>
   <name>Proxy Repo</name>
   <url>http://3.110.165.154:8081/repository/Wallmart-Proxy-repo/</url>
  </repository>
  
</repositories>
   
   
   
---------
Remote Repository:

creating  remote repo for common lib to share in company 


maven2 hosted

once updated some   dependencies jar then take that tag and mention in POM.XML

<dependency>
  <groupId>com.mss</groupId>
  <artifactId>mail</artifactId>
  <version>1.0.0</version>
</dependency>


Need to configure remote repo url in POM.XML

<repositories>
			<repository>
			      <id>nexus</id>
			      <name>Remote  Repo</name>
			      <url>http://3.110.165.154:8081/repository/mss-remote-repo/</url>
			</repository>  
	</repositories>
	
	aslo
	
	change in settings.xml
	
	     <mirrors>
    <mirror>
      <id>nexus</id>
      <mirrorOf>*</mirrorOf>
      <name>Remote repo</name>
      <url>http://3.110.165.154:8081/repository/mss-remote-repo/</url>
 
    </mirror>
  </mirrors>
	-----------
	we have to give any one of them  proxy or remote repo details in POM.xml and settings.xml
	
	else we need to use  both in  in POM.xml and settings.xml
	
	maven2 group 
	
	
	Need to configure group repo url in POM.XML  and settings.xml

<repositories>
			<repository>
			      <id>nexus</id>
			      <name>Group Repo</name>
			      <url>	http://3.110.165.154:8081/repository/mss-group-repo/</url>
			</repository>  
	</repositories>
	
	aslo
	
	change in settings.xml
	
	   <mirrors>
    <mirror>
      <id>nexus</id>
      <mirrorOf>*</mirrorOf>
      <name>Group repo</name>
      <url>	http://3.110.165.154:8081/repository/mss-group-repo/</url>
    </mirror>
  </mirrors>
  
  
  
  mvn clean package   ------- build
      [INFO] Building war: /root/app/maven-web-application/target/maven-web-application.war
   
  mvn clean install - /root/.m2/repository

       Installing /root/app/maven-web-application/pom.xml to 
            /root/.m2/repository/com/mt/maven-web-application/1.0.0/maven-web-application-1.0.0.pom

 mvn clean deploy


Uploading to nexus: http://3.110.165.154:8081/repository/Walmart-release/com/mt/maven-web-application/maven-metadata.xml

 -------
 
 401
 
 credentails should same
 id , username , password
 

administration:

   Security
     
	 Privileges
	 
	  
      Roles:
	     privileges:  
		 
		  add, delete ,,..
  	    nx-admin
	     nx-anonymous
	 
	 Users:
	    
		  admin
		  anonymous
	 
     anonymous access
          enable by default   --acces repo
	
	 LDAP
	 
	 
	 
Cleanup Polices:

cleanup policies can be used to remove content from your repositories. These policies will execute at the configured frequency. 
	 

System
Task

Run 
   
  	   
	http://3.110.165.154:8081/service/rest/v1/tasks
   
   
   conf
tomcat --> server.sml

conf
sonarqube --> sonar.properties

etc
nexus --> nexus-default.properties


SQE
 2 repos
   SQE-release
   SQE-snapshot


<dimst>


remote-repo
group-repo/
proxy-repo/


\=========Jenkins


Bamboo:


Jenkins is an OS, Java Based, CI tool.


Oracle - Husdon -CI-2004 
 
 Jenkins Community  -2011
 
 CI-
 
 Benefits:
 
 Immediate bug detection
 No integration 
 
 
 
 JaCoCo Pluging-- generate report
 
Installation--

Login as a root user
sudo su -

Install Jenkins

cd /opt/

wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo

sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key

yum install jenkins -y --nobest

Enable and start the jenkins service

systemctl enable jenkins

systemctl start jenkins


admin
admin



--
PAT-

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

freestyle project

Build:

invoke top level maven projects

mvn clean package

Jenkins Home Directory  -- /var/lib/jenkins/


------

Global Tool Configuration

--

Sonar deatils in POM.xml
nexus credentails  in /conf/settings.xml
nexus repo in POM.xml

Nexus Credentials
-----------------
in  maven HD conf/settings.xml 

<servers>
..
...
.
.
 <server>
      <id>nexus</id>
      <username>admin</username>
      <password>admin</password>
    </server>
	


  </servers>
  

repositories>
			<repository>
			      <id>nexus</id>
			      <name>Group Repo</name>
			      <url>	http://3.110.165.154:8081/repository/mss-group-repo/</url>
			</repository>  
	</repositories>
	
	aslo
	
	change in settings.xml
	
	   <mirrors>
    <mirror>
      <id>nexus</id>
      <mirrorOf>*</mirrorOf>
      <name>Group repo</name>
      <url>	http://3.110.165.154:8081/repository/mss-group-repo/</url>
    </mirror>
  </mirrors>
  
  
  /var/lib/jenkins/workspace/wallmart-dev/target
  
  Building war: /var/lib/jenkins/workspace/wallmart-dev/target/maven-web-application.war
  
  --------
  
  to deploy in tomcat need to use pluging called deploy to container 
  
  Pluging Manager
  
  
Post-build Actions


give credentails of user whoch we added in tomcat-users.xml


Build Triggers


POLL SCM
      when there is an commit in the  git then only it will trigger job on scheduled  time 
	  jenkins 
Build Periodically
      Trigger a job at scheduled time irrespective of change in the code.
github Webhook
      github only push the changes to jenkins to trigger the job
	  
Settings;

   WebHook
http://13.234.66.155:8080/github-webhook/

Discard old builds:

Abort the build if it's stuck:

Add timestamps to the Console Output

JACOCO  Pluging
    stop the deployment if not met the condition like 80% of code  coverage
  
  
IN Configure Jenkins 
  
 will configure mail 
  
  smtp.gmail.com
  23:25:33 Email was triggered for: Always
23:25:33 Sending email for trigger: Always
23:25:33 Sending email to: harishkumarbr38@gmail.com harishkumarbr8@gmail.com
23:26:33 Finished: SUCCESS

------
Jenkins Home Directory

/var/lib/jenkins/

jobs/
  job info
  
  wallmart-dev/
        builds/  nextBuildNumber  config.xml 
		   12/
		   13/
		     log   
			 
			 
workspace/
    contains source code 
   	wallmart-dev/               wallmart-dev@tmp/
	src/  target/  pom.xml
	
	
tools/

  contains the software which installed on Global tool configuration

users/
  user info..
  
  users.xml

plugins/
   pluging info
   
   
 freestyle Project  vs Maven Project Type
 by default                    manul instal plugging- maven Project type ---- need to install  Pluging call 
                                                        Maven Intrgreation Pluging
 .any language               . java language
 .install maven in            . maven can install in  only Global tool configuration
   server or tool configuration 
   
   
 plugin Management:

Deploy  to Container  - tomcat
 Deploy to weblogic - Weblogic - oracle
 Websphere Deployer   -Websphere ()IBM
 

 
 Restart of Jenkins:
  3 ways:
  
  1)
   systemctl restart jenkins
   
   2)
     Through urls
	http://13.234.66.155:8080/safeRestart	 
  
   http://13.234.66.155:8080/restart  
   
   
  3) 
    Through Pluggin called SafeRestart 
  
    Restart Safely  option in UI
	
	It wil call --
	http://13.234.66.155:8080/safeRestart


Are you sure you want to restart Jenkins? Jenkins will restart once all running jobs are finished. 


  
 http://13.234.66.155:8080/restart          # force restart
 
     Are you sure you want to restart Jenkins? 

 
---
Next Build Number Pluggin
 
to change to nextbuild number 

1) through server 
 
vi /var/lib/jenkins/jobs/wallmart-dev/nextBuildNumber 

2) Pluggin call Next Build Number
 we suppose to give greater than the current build number 
     # can see this plugin in job level
	  
---
JaCoCo 
----
SSH Agent
#docker
----
Email Extension # Suggestion plugging

--

SonarQube Scanner
 # directly configure SonarQube deatils in Jenkins only without configure in POM.xml.
 
 ---
 Audit Trail:
 
 # can see this Plugin in configure Jenkins page
 
 Audit 
  
   https://plugins.jenkins.io/
	
	---
Job Configuration History:
	
maintain congutaion of the job 	 and restore the job 
	  # can see this plugin in job level and system level 
	
---
Schedule Build
 
RUn a job at scheduled time only one time .
---

Build name and Description Setter:

we can do Customer Build number and name setter while creating a job- build Env
			 -------
Blue Ocean

---

EXternal Plugin

UCDeploy

----			 

Jenkins_Port =8080


RHEL

/etc/sysconfig
jenkins  --file

Ubuntu
/etc/default 
jenkins  --file

------------

Jenkins logs  available  on -/var/log/jenkins
jenkins.log
---
Views:


create view for usage


Jenkins Security:

 manage jenkins -- for admin
 
 Creating users:
 ------

Security:
 Manage users:
    Create  User:
	
   # wecan see this users in /var/lib/jenkins/users/users.xml
   
    #by default if we create users in jenkins they will have  admin access .
	
	to remove  admin access and  give proper access  got to 	
   Dashboard
Configure Global Security

LDAP- leightweight Directory Access Protocol


In authentication process, the identity of users are checked for providing the access to the system. 
While in authorization process, person’s or user’s authorities are checked for accessing the resources.
 Authentication is done before the authorization process, 
 whereas authorization process is done after the authentication process.


	In authentication process, users or persons are verified.	
	    While in authorization process, users or persons are validated.
		
		---
		Configure Global Security
Role-based Authorization Strategy  Plugin for Authorization
------
Authorization:
 Matrix-based security
 Project-based Matrix Authorization Strategy
 Role-Based Strategy
		
	-----------
 Project-based Matrix Authorization Strategy
::Enable project-based security
 add individual users including  admin ad provide access accordingly.
 
 
 ---------
 Build with Parameters:
 
 
 */${BranchName}
 $RevsionNumber
		
echo "................"
echo "selected branch name is : "$BranchName
echo "................"
echo "selected branch name is : $BranchName"
echo "................"
echo "Entered Revsion  is : $RevsionNumber"
echo "................"
echo "Entered Revsion  is "$RevsionNumber	


-------
Pipeline project type:

  vs FreeStyle vs  maven Project Type
  
 Intrgreation is easy
 scripts ,code
 
 
 Always  whenb we run job in jenkins it will point to below path
 
 /var/lib/jenkins/workspace/JobName
 
 /var/lib/jenkins/workspace/SQE
                                     target/*.war


 a) Scripted Way:
 
 node respresent in which server it will run 
 by default  master 
 node()
 or
 node('master')
 
 --
 /var/lib/jenkins/tools/hudson.tasks.Maven_MavenInstallation/maven3.8.3

${mavenHome}

maven-3.8.2
maven3.8.3

rsync

ssh agent plugin to connect the server with credentails , with user name and pemfile or paswd .
scp command: copy war file to to another  server  

ec2user is copying the file to webapps/ directory but ec2-user has ro access to write to webapps directory	
chmod -R 777 webapps/
 --
 node()
{ 
    def mavenHome = tool name: "maven-3.8.2"
    
    stage('Clone the Code'){
     
        sh 'echo "---cloning the code from Github---"'
        
        git branch: '${BranchName}', credentialsId: '45ea0ad7-34b3-4ae2-8be8-0b1432262ead',
        url: 'https://github.com/harishgowdabr/maven-web-application.git'

    }
        
        stage('Build the code'){
		// mvn clean package -Dmaven.test.skip=true
		//mvn -Dmaven.test.failure.ignore=true clean package
        sh "${mavenHome}/bin/mvn clean package -Dmaven.test.skip=true"
        
    }//Build
	
	 stage('Execute Sonar Report'){
        
        sh "${mavenHome}/bin/mvn sonar:sonar"
        
    }//Sonar
    stage('Depoy to remote Repo'){
         sh "${mavenHome}/bin/mvn deploy"
    }// to nexus
	
	stage('Depoying to Tomcat'){
        sshagent(['74499b02-e88e-440d-aaaa-0dd9c916e074']) {

      sh "scp -o strictHostKeyChecking=no target/maven*.war 
	  ec2-user@3.109.202.21:/opt/apache-tomcat-9.0.56/webapps/"
	  
}//ssh

        
    }//tomcat   scp: /opt/apache-tomcat-9.0.56/webapps//maven-web-application.war: Permission denied
	
	stage('mail'){
	}
	
}


####

Jenkinsfile  by default file we can have jenkins pipeline

//echo "GitHub BranhName ${env.BRANCH_NAME}"
  //echo "Jenkins Job Number ${env.BUILD_NUMBER}"
  echo "Jenkins Node Name ${env.NODE_NAME}"
  
  echo "Jenkins Home ${env.JENKINS_HOME}"
  echo "Jenkins URL ${env.JENKINS_URL}"
  echo "JOB Name ${env.JOB_NAME}"
  
----
 b)Declartive Way
 
 
 agent any means - any node to run this script
agent{
 label: 'dev'
}

---

pipeline{

agent any
/*
agent{
label 'NodeName'
}
*/


pipeline{

agent any
tools {
maven 'maven-3.8.2'
}

options()
{
timestamps() // it will add timestamps
buildDiscarder(logRotator(artifactDaysToKeepStr: '', artifactNumToKeepStr: '', daysToKeepStr: '6', numToKeepStr: '6'))
 // delete the old builds
}
triggers{
//Poll SCM
pollSCM('* * * * *')
//BuildPeriodically
cron('* * * * *')
//GitHub WebHook
githubPush()
}

    stages{
        stage('Cloning the code'){
            steps{
                 sh 'echo "---cloning the code from Github---"'
        
        git branch: '${BranchName}', credentialsId: '45ea0ad7-34b3-4ae2-8be8-0b1432262ead',
        url: 'https://github.com/harishgowdabr/maven-web-application.git'

            
            }//step
        }//stage

        stage('Building the war file'){
        steps{
        sh "mvn clean package -DMaven.test.skip=true"
        //mvn clean package -Dskip.Tests
        }
        }
        
        stage('sonar Report'){
        steps{
        sh "mvn  sonar:sonar"
     
        }
        }
        
        stage('INTO Remot Repo'){
        steps{
        sh "mvn clean deploy"
     
        }
        }
        stage  ('Depoying to Tomcat'){
       steps{
        sshagent(['74499b02-e88e-440d-aaaa-0dd9c916e074']) {

      sh "scp -o strictHostKeyChecking=no target/maven*.war 
	  ec2-user@3.109.202.21:/opt/apache-tomcat-9.0.56/webapps/"
     }//shh
     }//step
     }//stage
    }//stages
	
	post{
	
	success{
	
	}
	
	failure{
	  
	  }
	  abort{
	  
	  }
	  always{
	  
	  
	  }
	}
	
	
}//pipeline
------------------------


Example -2
----------
pipeline {
    agent any
stages {    
  stage('CreateDirsandFiles'){
  steps{
  sh "touch test.py"
  dir('/tmp/pipeline/'){
  sh "touch pipeline.txt"
  sh "touch test.sh"
  }
  }
  }

}
}

Example -3

pipeline {
    agent any
stages {    
  stage('run Java'){
  steps{

  sh "java -jar jarname"
  }
  }
  }

}
}

---

Build with parameter with declartive :

pipeline {
    agent any
    
    parameters {
    choice(name: 'BranchName', choices:['master','development','dev','qa'],
	description: 'Using this we can pass the branch names' )
	
	string defaultValue: 'NA', description: 'Please enter RevisionNumber', name: 'RevisionNumber', trim: true
	
	
    string(name: 'PersonName',  defaultValue: 'Bhaskar Reddy', 
	description: 'This Parameter, will use to pass the persona name')
    }
    stages {
        stage('CheckoutCode') {
            steps {
               git branch: '${BranchName}', credentialsId: '45ea0ad7-34b3-4ae2-8be8-0b1432262ead',
        url: 'https://github.com/harishgowdabr/maven-web-application.git'
             sh "echo The persona name is: ${params.PersonName}"
			 
			 sh "echo  RevsionNumber is : ${params.RevisionNumber}"
			  sh "echo  Branch is : ${params.BranchName}"
             }
        }
    }
}

--------------

pipeline {
    agent any

 
    parameters {
    choice(name: 'BranchName', choices:['master','development','dev','qa'],
	description: 'Using this we can pass the branch names' )
	
	string defaultValue: 'NA', description: 'Please enter RevisionNumber', name: 'RevisionNumber', trim: true
	
	
    string(name: 'PersonName',  defaultValue: 'Bhaskar Reddy', 
	description: 'This Parameter, will use to pass the persone name')
    }

    stages {
        stage('Hello') {
            steps {
                echo 'Hello World'
                
                
                git branch: '${BranchName}', credentialsId: '45ea0ad7-34b3-4ae2-8be8-0b1432262ead',
        url: 'https://github.com/harishgowdabr/maven-web-application.git'
             sh "echo The persona name is: ${params.PersonName}"
			 
			 sh "echo  RevsionNumber is : ${params.RevisionNumber}"
			  sh "echo  Branch is : ${params.BranchName}"
            }
        }
        
          stage('CreateDirsandFiles'){
  steps{
  sh "touch test.py"
  dir('/tmp/pipeline/'){
  sh "touch pipeline.txt"
  sh "touch test.sh"
  }
  }
  }
        
     
     
        
    }
}

--
Multi Branch pipeline Project Type

create 50 Branches at one shot .. if we have common file 

..

FileName - Jenkinsfile  -- 

Go to git repo-- search for Jenkinsfile-Declarative-Pratice -- 
file name in all the branches, for those branches it wil create a Job.

Scan Multibranch Pipeline Triggers--- feature act li poll scm chmages if found triigers build

Scan Multibranch Pipeline Now -- will trigger build manually.


Blue Ocean will use this MUlti branch pipeline concept


--------

Jenkins Backup:

Thin backup pluggings:
Backup:


  -- we can seee in manage Jenkins -Uncategorized
 
 Thin backup 
 
 
 settings
    path to store the backup:
	/var/lib/jenkins/
	                   mkdir /var/lib/jenkins/jenkinsbackup
					    it will create as Root user
						so change owner n group
						
						chown -R jenkins:jenkins backup
						
						
	/var/lib/jenkins/backup  in jenkins UI  in setting s ThinBackup

Max number of backup sets -- 100
  -1 number of backup 	
  
  
  
  Backup schedule for full backups *** 1hr
  
  * * * * *
Backup schedule for differential backups   # updated backup



Jenkins Migration:

Jenkins 2.289.3  version

1) acces too jenkins:
 Install jenkins in new server with same version as in old server .
 It will create /var/lib/jenkins/ directory in new server  rename it to jenkins_bak
  stop it 
 copy the configure file jenkins file from old server (/etc/sysconfig   , /etc/default) to new server 
 
 start it 
  
 2)  Job Import  Plugin  
 
 install in new server
    give credentails of jenkins url of old server
	
	---
	
	
	Master Slave Architecture:***
	
TCP /IP Protocal

Connection Type is SSH	
	
	
	job info is maintain in master only.
	
	
	Source code is maintain in Slave.
		
labelname is collection of nodes avilable node it wil run he job

use lables so that it wil act as Load balancing..
	
	------------
	
	Manage Nodes and Clouds
	
	instal java and  Git in Nodes /slaves 
	
	 
	create  path to have node directory in node server
	mkdir node1
	
	/home/ec2-user/node1


3.110.179.182 -- node1
	
	
	
	INFO: Both error and output logs will be printed to /home/ec2-user/node1/remoting
<===[JENKINS REMOTING CAPACITY]===>channel started
Remoting version: 4.7
This is a Unix agent
Evacuated stdout
Agent successfully connected and online


/home/ec2-user/node1/remoting

remoting.jar  # with this jar only nodes n master will communicate

to run the job in nodes -- go to job n configure in genaral -Restrict where this project can be run

label expression (select node name r label name)

	For Pipeline project type


 node(nodeName/LabelName)
{ 

--
 node('wallmart-node')
{ 


 node('nodes')
{ 
------

pipeline {
    agent{
	label : 'nodes'
	}
	
	-----
	
	pipeline {
    agent{
	label : 'wallmart-node'
	}
	----
	Number of Executors
	
	in node configure page
	
	------------
	
	
	Jenkins shared Libs:
	
	
	Reuse of pipeline scripts
	
	
	1:create scripts and store in git
	
	 steps 2  Add GitHub Shared Library Repository to JenkinsManage 
	 
	 Jenkins- Configure System -Global Pipeline Libraries

sharedLibs

    3 :  to use this  shared scripts
	
	
To access the shared libraries, in the Jenkinsfile (declarative pipeline)
	   in  pipeline  job only
	   
	@Library('sharedLibs') _
pipeline{

agent{
label : 'wallmart-node'
}
tools {
maven 'maven-3.8.2'
}
         agent {
		  label: 'nodes'
		 }
             stages{
                 stage('Gettimg code from Git'){
                     steps{
                         git branch: 'dev', credentialsId: '45ea0ad7-34b3-4ae2-8be8-0b1432262ead', url: 'https://github.com/harishgowdabr/maven-web-application.git'
                     }
                 }//
                 
                 stage('Building file'){
                     steps{
                         //sh "mvn clean package"
                         stages('Build')// from sharedLibs
                     }
                 }//
                 
                 stage('Sonar report')
                 {
                     steps{
                         //sh mvn sonar:sonar
                         stages('SonarQube Report')
                     }
                 }
             }
               }	
			   
-----------

Jenkins CLI:


jenkins-cli.jar  download 

ip:8080/cli 
/home/mobaxterm/Desktop/DevOps-Cloud/jenkinscli


java -jar jenkins-cli.jar -auth admin:admin -s http://13.234.66.155:8080/ -webSocket help

			   
	JENKINS_USERNAME=admin
	
			   11754699d36ef07df95344c0ad78dea078
			   
			 jenkinsUserName=`grep JENKINS_USERNAME jenkinsCredentails.properties.sh | cut -d "=" -f2`
jenkinsPasswordToken=`grep JENKINS_TOKEN jenkinsCredentails.properties.sh | awk -F = '{ print $2 }'`  
			   
java -jar jenkins-cli.jar -s $jenkinsUrl -auth $jenkinsUserName:$jenkinsPasswordToken -webSocket build 
		          $jobName -s -v -p BranchName=$branchName  -p RevisionNumber=$RevisionNumber   
				  
------------				  
	CI/CD for Node js Project

	nodeJS  Pluggin
	
	
	GLobal tool Configuration --
	  
	   NodeJS - NodeJs17.40
	   
	   
	   
	 
node{

stage('CheckOutCode')
{
git credentialsId: '45ea0ad7-34b3-4ae2-8be8-0b1432262ead', 
url: 'https://github.com/harishgowdabr/nodejs-app-mss.git'
}

stage('Build')
{
nodejs(nodeJSInstallationName:'NodeJs17.40'){
sh "npm install"
}
}

stage('ExecuteSonarQubeReport')
{
nodejs(nodeJSInstallationName:'NodeJs17.40'){
sh "npm run sonar"
}
}

/*stage('UploadArtifcatsIntoNexus')
{
nodejs(nodeJSInstallationName:'NodeJs17.40'){
sh "npm publish"
}
}
*/
stage('RuntheApp')
{
//sh "node app.js"
sh "npm start &"
}

}






Maven Java         Node Js
----------         -------
pom.xml            package.json

mvn sonar:sonar     npm run sonar (OR) node sonar-project.js

sonar details in          sonar details in sonar-project.js
 Pom.xml
 
mvn deploy         npm publish

nexus details      nexus details in
in POM.xml            .npmrc


to get nexus token for node js app
echo -n "username:password" | openssl base64

echo -n "admin:admin"  | openssl base64

------------------------------------------------


 node_modules
  
  
  
  
  ---sonar-project.js
   Sonar details
    <sonar.host.url>http://13.234.66.155:9000/</sonar.host.url>
		<sonar.login>7039883a8c91a3ca3ad24493e12ea32590421a12</sonar.login>
		
		
		-----------------
		
		
		Resume:
		
		HarishKumarBR_5Years_DevOps.doc
		
		
		----------------------
		Monitoring tools:
		
Application Monitoring tools
----------------------------

NewRelic
AppDynamics
DataDog
Nagios
Zabbix
Grafna and Prometheous --> K8s 

Cloudwatch --> Service --> AWS


Log Monitoring tools
--------------------

Splunk
Logentries

ELK  Stack --> Elastic Search, Logstash and Kibana




wallmart

wallmart.log

ERROR


NewRelic + PagerDuty


newrelic

harishkumarbr8@gmail.com

H@r!$h1234




Very useful video for Jenkins Admin user password reset.

https://youtu.be/TnbzCae--X0

Very useful video for Jenkins Pipeline Parallel Stage execution.

https://youtu.be/KOJXR8CHpKI

---------------------

AWS:

requires to hostour apps

servers
databases
Storage(disk)
Networking
Routers
Switches
Cables
Firewalls.....

	
Application
-->
Database
-->
OS
-->
Haraware	

Data Center    physical Machines

all dev , operation teams connected  to server to host, maintain ,manage apps 

challengers:

cost
maintainace
Scalability

power supply
coolling system

capital expenditure -capex

investment
physical security
data security

support engineers --

Maintainces effort


Scaling:

 On primise Infrastructure :
 
 
 takes time to delivery the required infrastructure
  
  Datacenter Infrastructure management
------------------------------------------------------
1.	Dedicated space
2.	High bandwidth
3.	Redundant power supply
4.	Support Availability
5.	Leadership Experience
6.	Time consuming
7.	Higher maintenance Effort
8.	Capacity Planning

Business requirement
-------------------------------
1.	High Availability
         Data and application should be accessable in anytime/ allways avilable
2.	Fault Tolerant
          Ability to withstand the failures
3.	Scalability:
          Increases r deceases the capacity of the infrastructure via statically
4.	Elasticity
         grow r Shrink the capacity of the infrastructure resources dynamically
Big Billion offers:

Cloud Service:

  Services on demand, pay as you go model
  Any Service made available to the users on demand via internetfrom a cloud computing providers servers

example : Zomato , swiggy  

Cloud Computing
------------------------
Delivery of comuting services like servers, storage, databases etc over internet hosted 
  remote data centers manages by Cloud Service Providers CSP		
		
	Advantage of cloud
---------------------------
1.	cost-Effectiveness - Pay as you Go
      eliminates CAPEX
2.	scalability and elasticity
        vertically and horizontal scalability
3.	Reliable and High Availability
4.	speed or Agility
      get infrastructure in fraction of minutes
5.	Deploy globally in minutes
          expand the business globally
6.	Security	


Types of cloud
---------------------
1.	public cloud - AWS, GCP and Microsoft Azure
        Owned and operated by    3 rd party CSP
2.	private cloud - Openshift and IBM Cloud
       used by single business organisation , physically located
	      
3.	Hybrid cloud
      combination public and private
        some r own data center and some are cloud.
4.	Multi cloud
     organization depends on multicloud  
	 appls on AWS, application on Azure
	 
	
Popular Cloud providers
---------------------------------
•	AWS
•	Microsoft Azure
•	GCP
•	VMware
•	IBM Cloud
•	Oracle Cloud
•	Rackspace
•	Redhat
•	Salesforce
	 
	Cloud Service Models:

•	On Premises - Networking, Storage, servers, Virtualization , O/s, Middleware, Runtime, Data , Applications(By own)
•	IAAS - Networking, Storage, servers, Virtualization
        EC2  (Os , java install , )
•	PAAS - Networking, Storage, servers, Virtualization , O/s, Middleware, Runtime
        EKS, Elastic beans stack
•	SAAS - Networking, Storage, servers, Virtualization , O/s, Middleware, Runtime, Data , Applications	
         github, sonar Cloud, office 365
		 
	
	
	Software --just use application
	 Platform - development of application and  use application n managing data
	Infrastructure - integration of infra n development of application and  use application
	
	
	-----
	
	Car  
	
	On premise ---Own  car
	
	IAAS -- Car leased
	
	PAAS - Car Hired
	
	SAAS - Taxi
	
	
	Amazon Web Service:
	
	
	Retail  , E-commence
	
	Sub organisation of Amazon.com 
	
AWS Global Infrastructure
------------------------------------
                         old-   Region -            25,    planning 8 more
                            Availability Zones - 81, planning 24 more	
	
	84 Availability Zones within 
	26 geographic regions around the world
	
	24 more Availability Zones and 8 more AWS Regions in Australia, Canada, 
	         India, Israel, New Zealand, Spain, Switzerland, and United Arab Emirates (UAE).
Regions:

Geographical Locations
 logical name 

AvailabilityZones -AZ:

Acutal Data Centre
Isolated Locations	
	Phyiscal
	

How to choose the right region?
-------------------------------------------
•2	pricing
•1	End User/Customer Location
•3	Latency (To check the latency https://www.cloudping.info/ or https://ping.psa.fun/ )
         access time of ur application
•4	Security and Compliance Requirement
•5	Service Availability
	
	
	1a -A.Z  --250KM ---	1b - A.Z--- 250Km - 1c A.Z
	
	Multi Region Applications
	
	
	Asia Pacific (Mumbai)
ap-south-1
	Europe (Frankfurt)
eu-central-1



EC2

Elastic Compute Cloud


Computer in Cloud
as many server u required
scale up , down to handle changes




Computer:

OS
CPU/Memory
HardDisk
Network Card
Firewalls

EC2 Instance components
-----------------------------------
•	AMI's  - Amazon Machine Images - pre configured package that contains OS and some application
•	Instance type
•	EBS(local storage)
•	IP Addressing
•	Security Groups
•	KEY pair


Once we  create account in AWS -we can create max 20 instances per EC2 region 

8 cpu and 32gb Memory   -- application
8 cpu and 16gb Memory   --- Jenkins, k8s, Docker Swarm
4 cpu and 8gb Memory  -- LB ,Apache/Weblogic

Instance type:

General Purpose
          Balanced Memory , CPU
		  
		  A, M, T Series
		  A2
		  M4 , M5 , M5a 
          t2 , t3, t4
Compute Optimized
         More CPU & RAM  , batch process ,High performance computing 
	    C series
	     C4, C5
Memory Optimized
        More RAM  , in memory DB  in-memory caches, and real time big data analytics
         R , X ,Z 
		 R5, R4, X1 z1d
   High memory Optimized Instances

      U  series
Accelerated Computing
        Graphical processing , Machine learning
		P , G , F Series
		P3, P2, P4, G5, G3, F1
         
Storage Optimized
        better read and write opertaion to disk, low latency sequential read and write access to 
		               very large data sets on local storage
	    I D H Series
		I3 D2 D3 H1

Instance Family & Type
---------------------------------
1.	General purpose – balanced memory and CPU (t2, m3, m4)
2.	Compute optimized – More CPU then RAM (c3, c4, cc2) – C types
3.	Memory optimized – More RAM – M type and R type
4.	Storage optimized – Low latency (d2, i2, i3) – D and I type
5.	Accelerated computing (GPU) – Graphics optimized
6.	High memory Optimized – High RAM, Nitro system
7.	Previous generation
8.	EBS Optimized (Option for higher IOPS performance)



EC2 Purchasing Option
----------------------------------


On Demand: -
------------------
•	Most expensive purchasing option
•	Most flexible purchasing option
•	You are charged only when instance is Running(billed by hour)
•	You can provision/terminate an instance anytime
•   Irregular Workloads,Pricing is per instance hour

Reserved: -
-----------------
  Scheduled Reserved Instances
•	Allows us to purchase an instance for a set time period (1/3 yrs
•	Significant price discount
•	Once you buy a reserved instance, we are responsible for the entire price  
       regardless of how often we use it

Spot: -
-----------
•	Amazon sells the unused instances, for short amount of time at lower price
•	We can Bid on an instance type & only use when the spot price is equal to or below your bid price
     Charged by hour
•	Spot price fluctuate based on supply & demand in market



Dedicated Instance
--------------------------
•	Instances running on hardware that’s dedicated to you. 
    If you stop/start instance, you can get some other hardware somewhere else.

Dedicated host
---------------------
•	Physical dedicated server for your use. It’s always the same physical machine for as long as you are paying.

Instance Family & Type
---------------------------------
1.	General purpose – balanced memory and CPU (t2, m3, m4)
2.	Compute optimized – More CPU then RAM (c3, c4, cc2) – C types
3.	Memory optimized – More RAM – M type and R type
4.	Storage optimized – Low latency (d2, i2, i3) – D and I type
5.	Accelerated computing (GPU) – Graphics optimized
6.	High memory Optimized – High RAM, Nitro system
7.	Previous generation
8.	EBS Optimized (Option for higher IOPS performance)

Instance Type Components
---------------------------------------
•	Family: Categorizing instance types based on what they are optimized for
•	Type: subcategory for each family type
•	vCPUs: number of virtual CPUs the instance type uses
•	Memory: Amount of RAM the instance type uses
•	Instance Storage(GB): local instance storage volume(hard drive)
•	EBS Optimized Available: Indicates if EBS optimization is an option for the instance type
•	Network Performance: Rating based on its data transfer rate(bandwidth)


How are we charaged for using EC2?

1>  Purchasing options
2> Instance family and type								
3> EBS Optimized
4> AMI Type
5> Data Transfer																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																									fer
6> Regions


 1>>>>>>AMI:
Amazon Machine Images
Pre configured package required to launch Ec2 Instances including os , software packages and other settings,
Virtual Servers in Cloud

Thing you do to select an AMI
-------------------------------------------
AMIs come in 2 main categories: -
1) Community AMIs:
- Free to use
- Generally it contains only the OS, basic free software

2) AWS Marketplace AMIs:
- pay to use
contains OS and 
- Generally comes packaged with additional licensed software

3) My AMIs:
- AMIs that you can create yourself
---------------------------------------------

2>>>>>>>> Instance Types 

vCPUs , memory , EBS.


EC2 -- CPU and memory

Instance Storage --

a)Instance store - Local Storage  , temporory storage- storage from EC2 box -- 
                     loss of storage/date  when it is stop/terminate
                    underlyimg drive fails -- DAS
 
b)EBS  --SAN - Storage Area Network  , storage from  another box  ec2 or 	
 consitence disk attached to the box where it is running  -- Root Volume- NAS
 
 
 
3>>>>>Configure Instance details:

 Number of Instances ,
 Network :  
          VPC  - custom VPC
		  Subnet
     ap -south -1
     Asia pacific 
  Auto assign pulblic IP:
  
  Protect against Termination
  
  Userdata:
    Boot Strap Script  , we can execute some commands , while creating server.
	
4>>>>>>>>> Add Storage:

 EBS -Disk,
 Volume
Root volume -- booting OS , instal software  /dev/sda1

additional volume --  db , jib ,jenkins job- mount to addtional storage 


EBS volume type:

IOPS - Input output operations per sec per GB

Speed 

i)Solid State Drive Backed Volumes - SSD
. General Purpose SSD
     gp2 # default
	 gp3
. Provisioned IOPS SSD
    io1
	io2

ii) Hard Disk Drive Backed Volume - HDD
. Throughput Optimized HDD -st1
. cold HDD - sc1

iii) Magnetic Standard

5>>>>>>>>>>> add tags

meta data-- key value

6>>>> Security Groups

Security Groups-SG
-------------------------
1. Act as a firewall for any EC2 Instance
2. Control the traffic by Inbound and Outbound rules
3. SG is stateful means if u allow rule in inbound it automactically allwoed in outbound 
Network security 
Inbound rules and Outbound rules

5 SG per EC2  instance
can only have permit Rules, cant have deny rules
Operates in Instance level 
statefull - return traffic is automactically allowed

--
NACL 
Firewall at Subnet level
Stateless
It permit allow as well deny
Number listof Rules -- 32766

 1 subnect - NACL
 1 NACL = multi Subnet
 
 Once we craete VPC by default , NACL will create n associated to  all subnets in VPC

7>>>>>Key Pair
ssh key 
public key (inside server wil  have in ~/.ssh/authorized_keys) and
Private  key  (Pem file which we have)

public access aenied
timeout issue

CloudPing.info
https://www.cloudping.info

AWS latency test
https://ping.psa.fun


IP Address
-----------
1. Public IP - Accessible from Internet and will change after restart.
2. Private IP - Not able to access through Internet. It should be from any class, A, B, C 
                Access Ec2 within the network -VPN
3. Elastic IP - Fixed Public ip and will be chargable if we not used it.
            If we associcate it to EC2 instance nas that instance is running it will not charge.

ping IP

telnet Ip port

curl -v telnet://Ip:22

We can have only private IP 

2/2

system check- under lying h|w
instance state check

/dev/sda1  -- root volume

lsblk  	- list  block storage
df -kh 

Meta data once u login :

curl http://--ip--/latest/meta-data
GET http://--ip--/latest/meta-data

Storage - services:
S3
EFS
Glacier
SnowBall
EBS



default - Delete on Termination


user -data verication -- sudo cat /var/log/cloud-init.log


Ec2 - cpu and memory


Storage services
--------
1. Local storage - temperory - Instance store
2. EBS Storage
3. EFS storage
4. S3


Storage types
---------------
1. Block storage - Data store in multiple Block and each block have their inode value - 
              Ex EBS - SSD(GP2/GP3/IO2/IO3) or Magnetic type - root volume - Good performnace for I/O
2. File storage - Data store in a file - Ex EFS
3. Object storage - Data store as an object. Each object have their end point - Ex S3




.Block storage:
	Data equally divided into block ,each block has address called index
	not conatin  metadata in block , performace is good - writing and reading 
    Ex: EBS	
.File Storage -System:
	Traditional file system , path , metadata
	
	To read and write the data we need mount the block and file storage to server 
    Ex: EFS , NFS
.Object Store:
	No need to mount this storage to server to access the data from anywhere 
	Each and every object which we are uploading to object storage whil have unique object id and object url
	using this end point we can anywhere
    ex: dropbox , s3, drive 	


EBS:

1 EBS storage can  be attached  1 ec2 Instance at given point time.
   we can dettach and attach to any other ec2 instance.
   and ec2 instance and EBS volume are should  be in same Az with region 
   
 EBS cant be mounted /shared with  multiple ec2 Instance at a time but 
 we can have 2 EBS storage  attached to 1 ec2 instance.



max size of EBS volume
1600GB /16TB

min size of EBS volume
1GB

Note:-
------
1. We can access or read/write on EBS and EFS after mounting it.
2. EBS volume and EC2 will be in same AZ if you want to mount EBS to EC2.
3. The min size value of EBS volume is 1 GB and Max 16384 GB.

---------------
Note:-  
--------
1. We can not mount one EBS volume simultaneously to multiple EC2.
2. You can increase the size of the created volume on fly but not able to decrease.

Usecase

Attach EBS storage to  Jenkins Server:

lsblk

df -kh 

Volume - created - attached to ec2 instance - format - mount to the path 

Elastic Block Store 

.Create Volume ->
  .. volumn type(SSD/IOPS) ->
      ...Size (1GB/1634GB) ->
	     ... AZ (same as Ec2 Instance) ->
			....Encryption (CMK)
			
			Created Volume
			
			
Volume -->Action --> Attach Volume -- Ec2 Instance 
			
			device name -- /dev/sdf -## additional volume
			
lsblk

#sudo file -s /dev/-xvda2 ## root volume already formated
sudo file -s /dev/xvdf ## addtional volume need to be formated  as it raw storage

# formats
sudo mkfs.ext4 /dev/xvdf

sudo mount /dev/xvdf  /var/lib/jenkins/  ## temperory mounting

-------------------

	[root@ip-172-31-94-136 opt]# lsblk
	NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
	xvda    202:0    0  10G  0 disk
	├─xvda1 202:1    0   1M  0 part
	└─xvda2 202:2    0  10G  0 part /

	only  one volume 

	ap-south-1b

	raw storage

	[ec2-user@ip-172-31-87-30 ~]$ sudo file -s /dev/xvdf
	/dev/xvda: DOS/MBR boot sector, extended partition table (last)

	[ec2-user@ip-172-31-87-30 ~]$ lsblk
	NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
	xvda    202:0    0  10G  0 disk
	├─xvda1 202:1    0   1M  0 part
	└─xvda2 202:2    0  10G  0 part /
	xvdf    202:80   0   5G  0 disk

	[ec2-user@ip-172-31-87-30 ~]$ sudo file -s /dev/xvdf
	/dev/xvdf: data

	attched now  2 volumes in AWS


	TO format: file system

	[ec2-user@ip-172-31-87-30 ~]$ sudo mkfs.ext4 /dev/xvdf

	after fomatted:

	sudo file -s /dev/xvdf
	/dev/xvdf: Linux rev 1.0 ext4 filesystem data, UUID=9c15add5-f815-4536-b657-91c7f7fc6691 (extents) (64bit) (large files) (huge files)
	[ec2-user@ip-172-31-87-30 ~]$

	to temporary mount:

	sudo mount /dev/xvdf /var/lib/jenkins/   ##after restart machine it will unmounted

	to permanent mount :

	take backup:
	sudo cp /etc/fstab /etc/fstab.orig

	sudo blkid 

	 ## take uui here 
	dev/xvda2: UUID="c9aa25ee-e65c-4818-9b2f-fa411d89f585" BLOCK_SIZE="512" TYPE="xfs" PARTUUID="b3824610-751c-49f8-a4a9-068fa13d9460"
	/dev/xvda1: PARTUUID="3e18b896-4879-4ede-8711-a58017aff81c"
	/dev/xvdf: UUID="9c15add5-f815-4536-b657-91c7f7fc6691" BLOCK_SIZE="4096" TYPE="ext4"


	##add uuid in etc/fstab 

	sudo vi  /etc/fstab

	UUID=b074e53e-e0f0-4dc8-876d-e6093f3d8404  /var/lib/jenkins/  ext4  defaults,nofail  0  2


  to validate entries are correct in fstab
  
  sudo unmount /var/lib/jenkins/
  sudo mount -a   # mounted back as there  an entry in fstab
  
  if any error revert sudo mv /etc/fstab.orig /etc/fstab 
  
	later need to change owner

	sudo chown jenkins:jenkins -R /var/lib/jenkins
	service jenkins restart

we can modify the EBS Volume while using it we can increase and change volume type aslo

	Volume Created and attched and formated and mounted
	
NO DISK SPACE LEFT ON THE DECIVE:


Lab: -
-------
Create an EC2 Instance and create Volume in same AZ and attach the volume to EC2.Create filesystem on 
the mounted Volume by mkfs.ext4 /dev/xvdf and then mount 
any directory to the mounted volume by mount /dev/xvdf /var/lib/jenkins.
----

	g4ad.2xlarge  - 16 32
	
	
EBS Snapshot
-------------
Snapshot is nothing backup of your EBS Volumes.

.Snapshot are usefull to recover /restore  the date  in case  of any failure in EBS volume.
.useful to migrate our data from one AZ to  another AZ  within region or we can move from 1 region to another region 

volume - snapshot -volume
5000 EBS volumes per account
upto 1000 snapshot per account
 Snapshot stores in s3 cant access it direclty , throgh APIs only
  EBS volume - Az specific
  Snapshot - region specific
  Incremental Snapshot
  
  take snapshot of non - root volume while running also
    take snapshot of  root volume need to stop it 


server migration to one Az to another Az , or 1 region to another  region means data migration

one Az to another Az  of volume::

create snapshot in 1 az of volume 
action -create volume in another AZ using this snapshot
         	attach volume to the server  in same AZ
			mount 

dev/sda1 --root

1 region to another  region of volume

create snapshot in 1 az /region of volume 
action -copy snapshot to another region 
        create volume  from snapshot
		attach volume to the server in same AZ
		mount (permanent/temperory)
		

We  cant attach snapshot direclty to the server
Volume created from snapshot not required ot formatting if  volume direclty created and  attached this need formated 
 like sudo  mkfs.ext4 /dev/xvdf
 


snapshots can be taken two types: -

1. Manually using AWS GUI, CLI, APIS etc

Volume snapshot or Instance snapshot

   check mark on the volume -> Actions -> Take a snapshot (no need to stop the server)

2. By creating Lifecycle manager (For periodically based on policy)
   Life cycle manager -> Specify settings -> On the basis of tags on that volume
   Target resource type -> volume
   Target resource tags -> Name and value
   Schedule details
   Retention Type   - 6 it wil maintain recent 6 snapshots
   
   
   
 
Restoring: -
-------------
In same region , different A-Z -> 
       -->  Create Snapshot 
		 
             Actions -> create volume

In different region - > use Actions -> copy -> Choose Region and then convert the Snapshot to volume in that region.

Lab1:- Take a snapshot of the Jenkins Volume and copy that snapshot to Singapore Region. create an Instance in Singapore Region and install Jenkins, and mount the snapshot after converting to volume to Jenkins EC2

Lab2: - Recovery of Key file

1) Stop Server Which you lost pem file
2) Create a server with new pem file in same AZ where you have server for which you lost pem file.
3) Detach root volume from the server for which u lost pem file and attach to new server as a additinal volume.
4) In New Server mount the volume and copy latest public key & un mount
 
 sudo mkdir -p /mnt/tempvol/

 sudo mount /dev/xvdf2 /mnt/tempvol/


 cat /home/ec2-user/.ssh/authorized_keys >> /mnt/tempvol/home/ec2-user/.ssh/authorized_keys

 umount /mnt/tempvol/
 
 
 
5) Detach volume from new server and attach back to existing server as a root volume(Root Device Type
/dev/sda1)


6) We can start the server and we can access using new pem file.

In Recovery Server
====================


Root Device Type
/dev/sda1

Additinal Volume Type

/dev/sdf
===================
  Where do EBS snapshots will be stored/maintained by AWS?--S3

EBS snapshots will be maintained in S3 buckets by AWS. But not available in our account.
  -----------
 ------------------
 Attach same volume/storage to 2 diff server
 
 shared volume/storage
 
 NFS - Network File System
 
 EFS -- AWS Service
 Its a manage NFS 
 managed Fle Stored for EC2
 Root volume will be EBS not EFS
 
 hared storage (EFS)
-------------------------------
We can mount this storage simultaneously.

EFS - Managed NFS -> you no need to configure NFS, managed by AWS. There is no fixed capacity,
 it can grow and shrink automatically.

Lab: - Create two EC2 in different AZ. Create EFS storage and share between two.

It can be shared to different AZ, Region or different Account.

Search EFS -> Create file system -> Name, VPC, Availability and Durability (Regional,(Multiple AZ) One zone -(1 AZ)) - create

It need NFS client or EBS client installed on the system to mount.

NFS Port -2049

Port open in SG which attach to EFS:

Install NFS client on both system
# yum install nfs-utils -y

Open the security group port 2049 of shared volume. You can get the Security Group id from netwrk section of EFS volume.
and add network range of VPC Cidr 


2 diffrent server same storage/sharesd volume
Mount the volume by below command

sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 
fs-01088cdc7ba63835e.efs.ap-south-1.amazonaws.com:/ <opt/one/two>
--

sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 
fs-01088cdc7ba63835e.efs.ap-south-1.amazonaws.com:/ <anotherPath>
							  
							  
							  
Amazon FSX client - For windows 
FSx - For windows
 
 S3: -
--------
 simple storage Service
Object Storage

Without mounting we can read and write on Object Storage from anywhere.
S3 use cases:

S3 is perfect place to maintain application static files like images, docs, audios, video file
S3 is perfect place to maintain application logs and backups.

state file in terraform will maintain in S3.

Data/object in S3 which we maintain wil have unique Id and Endpoint we can accces  the  oobject (file ) usimng HTTP and rest API

S3 Bucket 

Using AWS GUI ,CLI, API, SDK
S3 global service but its a region specific.
No OS in S3
S3 Bucket name is unique accoss all regions and accounts

from GUI max size upload is 160GB in S3.

How many s3 buckets we can create in one AWS account by default?
100 (Soft Limit). Can be extended this limit by working with AWS team.

What is the max limit in s3 ?
5TB/5000GB-- single file not s3 bukcet size


Can we do s3 replication without versioning enabled?
No


Security in S3:
Bucket level
Object Level

Public Access --> It's access for the objects in s3
				by default Block Public access
				
				
				
Bucket policies-
				json format

ACL --> Access Controll Lists is can be used to grant/share buket with other AWS Accounts. 

What is ARN in AWS?

Amazon Resource Name --> It's unique way of identified each resource(service- AWS , ELB, VPC, S3) in AWS.

arn:aws:<service>:<region>:<a.z>:<Id/NameOftheAWSResource>

arn:aws:s3:::<nameofbucket>

Prinipal (Who is  to access)- * means public, anyone 

 Action -  what he will do
 Resource ---AWS , ELB, VPC, S3

arn:aws:s3:::julymithuntech1234/*
here * means all the files in that bucket

In Bucket Policy:


{
  "Id":"PolicyId2",
  "Version":"2012-10-17",
  "Statement":[
    {
      "Sid":"AllowIPmix",
      "Effect":"Allow",
      "Principal":"*",
      "Action":"s3:*",
      "Resource": [
        "arn:aws:s3:::julymithuntech1234/*"
      ],
      "Condition": {
        "IpAddress": {
          "aws:SourceIp": [
            "54.240.143.0/24"
          ]
        }
      }
    }
  ]
}


versioning in S3:
Keep object versions as its changes once we enable it .
  to protect against accidental object/data deletionn or overwrites
  Once we enable versioning we cant be disabled 
bucket level versioning

StorageClass and Obejct life cycle

 charge in S3 :
 Size of the  obejct -file and storageClass in which we are maintain object.
 
 Object action  -->edit storageClass
 
 StorageClass:
 
.Amazon Standard               Default  -costly faster , durability ,performace, 3 copies in different AZ
.Amazon  S3 Standard Infrequent Access IA  -- 
.RRS - Reduced  Redundancy Storage //
.Amazon  S3 One -zone IA	
.Amazon  S3 Glacier
.Amazon  S3 Glacier Deep Archieve 
.Amazon  S3 Intelligent Tiering


Storage Classes
----------------------
By default each object which is upload to S3 maintained in standard Storage class.
S3 charges is based on the object size and storage class.

1. Standard class ->
    The files/data  which we are going to access frequently. 3 AZ
2. IA (Infrequent Access) ->
    The data is not going to access frequently for cost saving and less performance.3 AZ
3. RRS/One zone (Reduced Redundancy Storage) -> 
  Non critical $ Reproducible data. We can store in RSS.1 AZ
 Chances to loss of data.
4. Glacier->
  Archival data we can store in glacier with very less price. When we retrieve the data from 3 AZ
   Glacier it will be chargeable high. It take up to 4 hrs to read the data from Glacier. It is like a historical data.


Object Life Cycle -

 By creating Object Life Cycle rules we can transit objects from one storage class to another storage class and aslo
 set an expiration(delete) object


our S3 bucket  -Mangament - lifecycle rule 
 
 
Object Transition & Expiration can be automate using Object Life Cycle Rules.
--------------------------------------------------------------------------------------------------------

Object Lifecycle -> we can transit & move objects from one SC to another SC and also we can set 
expiration (delete) for the object using Object Life Cycle Rules.

Management -> Lifecycle Rules -> create Lifecycle rules
1. Name
2. Choose a rule scope
3. prefix
4. tags
5. Lifecycle rule actions

We can move from one class to another from any specific days or we can delete after some specific days.

To save the cost we implement Life cycle management.

Replication Rules:
Replication in S3 (Duplicate Copy) 
Replication of obejct.

already  uploaded files  in  the bucket   before  the replication rule files  wil not replicate  .
files will replicate    only after creating the replication rule.


Enable of  versioning is must for Replication in S3 in both soruce and destination bucket
----------------------------------------------
management -> replication rules -> create replication rules ->
   Name, status, source Bucket and scope, Destination and Bucket Name , IAM Role , storage class of replication

Replication can  be done in same region as well as in different region.
Same account same region
Different Region Same account
Same Region Different Account
Different Region Different Account

Max file size which we can upload in S3?
5 TB

How many buckets we can create in one account by default?
100 (Soft limit)


Snow Ball (Device)
--------------------------
Data transfer of huge data from in-premises to AWS.

Write a request to AWS or job, they will send device to your data center, you connect that device and 
copy all data and then the device will be move to AWS data center and then they connect and copy your data.


check metadata 
curl http://..../latest/meta-data
curl ifconfig.me 


=================
compute - EC2
storage -  EBS
           EFS
		   S3
		   SnowBall
Networking - VPC

VPC:

Virtual Private Cloud:

Secure servers , Isolated the servers

------


VPC: - Virtual Private Cloud (Private network)
Network -> Groups of devices which connected with each other some or the other way using network cables, 
routers, switches... etc.

LAN -> Local Area Network (Intranet)
WAN -> Wide Area Network (Internet)
		   
		   
Subnets - Sub networks
IGW- internet gateway
Routetable
NAT  - Network Address Translater
     NAT gateway /NAT Instance
NACL - Network Access Control List



In Default VPC all the  subnets are Public Subnets.

Subnets- 
	Public Subnets - Subnets have acccess to Internet.
		               if the  subnets has a route to IGW  in routetable (mention as 0.0.0/0 to IGW)
					   Subnets which are associated with IGW 
					   Servers in these subnets will have  accces to Internet
	Private Subnets - These subnets doesnt have access to Internet.
			           Subnets which doesnt have route to IGW in route Table
					   If the subnets which has route not linked to IGW in RouteTable
					   Servers in these subnets will doesnt have accces to Internet
					   
					   
	
	
1 VPC - Upto 200 Subnets	

1 region - 5 VPCs can create(Softlimit)					


****VPC Tool Box***********
-------------------
**1. VPC (max 5 VPC in one Account)
**2. Subnets (max 200 subnets in One VPC)
3. NACL (Network Access Control List)
**4. Routing Table 
        how to traffic should be routed from/to each subnets
		set of  rules
		
		1 route table  - as many Subnets
		1 subnets have 1 route table a time 
**5. Internet Gateway
        Virtual router   it will enable the internet to our network/ vpc
		logical device enabling traffic to be routed to/from the public internet
		Without IGW resource can talk to each other but not io internet 
6. NAT Gateway


1 IGW is attached to 1 VPC at any time 

VPC - region
Subnets  - Az


ELB -> Loadbalancer as a Service
--------------------------------------------

RDS -> Relational Data Base as a Service
------------------------------------------------------


CIDR - classless Inter-Domain Routing 
  method for alloactimg IP address and IP Routing
  CIDR range 0-32
  
  172.31.0.0/16
  
  /16  - siderblock /subnet mask
  
  32-n
  2
  
  32-16
  2
          16
         2     ==65536  IPv4 32 bit Ips addresss



Sider block is should be between 16- 28

Never see server  of  public  ip below  range
		RFC1918 Subnets
		The RFC1918 address space includes the following networks:
******Below are will be in Private Ips ***

		10.0.0.0 – 10.255.255.255  
		172.16.0.0 – 172.31.255.255  
		192.168.0.0 – 192.168.255.255 	
		
	*******************************************	
Private Ips can be same  as in intranet 
no public ips in below ranges
IP addresss Classes:

Class A ---0.0.0.0 - 127.0.0.0 --  (1.0.0.0 to 126.0.0.0)  - N H H H 

Class B --- 128.0.0.0 - 191.255.0.0                         - N N H H 

Class C --- 192.0.0.0 - 223.255.255.0                       - N N N H 

Class D --- 

Class E-


Priavte ips can same 


Network Bit  Host bit
----------------
Subnet CIDR should be Subset of VPC Cidr



https://www.davidc.net/sites/default/subnets/subnets.html




VPC Types: -

1)	Default VPC
2)	Custom VPC

Default VPC: -
•	Created in each AWS Region when an AWS account is created.
•	Has default CIDR, Security Group, NACL and Route table settings.
•	Has an Internet Gateway by default.

Custom VPC: -
•	Is a VPC on AWS Account owner creates.
•	AWS user creating the custom VPC can decide the CIDR
•	Has its own default security group, Network ACL and Route Tables.
•	Does not have an Internet Gateway by default, one needs to be created if needed.

Steps to create a VPC: -
1)	VPC
2)	Subnet
3)	Internet Gateway
4)	Route Table configuration

Public Subnet: - If a subnet traffic is routed to an Internet Gateway, the subnet is known as a public Subnet. 
If you want your instance in a public subnet to communicate with the internet over IPV4, 
it must have a public IPV4 address or an Elastic IP Address.

Private Subnet: - If a subnet does not have a route to the internet Gateway, the subnet is known as a Private Subnet. 
When you create a VPC, you must specify an IPV4 CIDR Block for the VPC.
 The allowed block size is between /16 to /28 netmask. The first four and last IP address of Subnet cannot assigned.

For ex: - for Network 10.0.0.0/16

10.0.0.0		Network Address     
10.0.0.1		Reserved by AWS for the VPC Router
10.0.0.2		Reserved by AWS for IP address of DNS server
10.0.0.3		Reserved for Future Use
10.0.0.255		Broadcast Address


32-n
2 
curl -v telnet://65.0.125.11:22


Note: - AWS do not support Broadcast in a VPC but reserve this Address.

Implied or logical Router and Router table: -

	It is the central Routing function
	It connects the different AZ together and connects the VPC to the Internet gateway.
	You can have upto 200 Route tables per VPC.
	You can have upto 50 Routes entries per route table.
	Each subnet must be associated with only one route table at any given time.
	If you do not specify a subnet to route table association, the subnet will be associates with the VPC route table.
	You can also edit the main route table if you need, but you cannot delete main route table.
	However you can make a custom route table manually become the main Route table then you can delete the former main as it is no longer a main route table
	**  You can associate multiple subnets with the same Route table.

Internet Gateway: -

	The Internet Gateway is a virtual router that connects a VPC to the internet.
  **	Default VPC is already attached with an Internet gateway.
	If you create a new VPC then you must attach the Internet Gateway in order to access the Internet.
	Ensure that your public subnet route table points to the internet gateway.
	It performs NAT between your private and public IPV4 address.
	It supports both IPV4 and IPV6.


A Virtual Private Cloud is a Virtual Network that closely resembles a traditional Networking 
that you operate in your own data center, with the benefits of using the scalable infrastructure of AWS.

OR

VPC is a virtual network or datacenter inside AWS for one client.

•	It is logically isolated from other virtual N/W in the AWS cloud.
•	Max 5 VPC can be created in one account and 200 subnets in one VPC.
•	We can allocate max 5 Elastic IP.
•	Once we created VPC- DHCP, NACL and security group will be automatically created.
•	A VPC is confined to an AWS region and does not extend between regions.


VPC will create in a Region

•	Subnet will create in AZ and in two AZ subnet will not same
•	Two VPC may have same CIDR because they are isolate from each other.
•	Once the VPC is created, you cannot change its CIDR block Range.
•	If you need a different CIDR size, create a new VPC.
•	The different subnets within  VPC cannot overlap.
•	You can however expond your VPC CIDR by adding new/Extra IP address Ranges (Except Gov-cloud and AWS-China)

Components of VPC: -

•	CIDR and Ip Address, Subnets
	Implied Router and Routing table 
	Internet Gateway
	Security Group
•	Network ACL
•	Virtual Private gateway
•	Peering connection
•	Elastic IP


.Create VPC-
10.0.0.0/24

Action -->  Edit DNS names
        -->     Edit resolution
		
		
		
.Create Subnets:
10.0.0.0/24

  1subnet:
		1a AZ
		Public_1a_Subnet
		10.0.0.0/26
   2subnet:
		1a AZ
		Private_1a_Subnet
		10.0.0.64/26
   3subnet:
		1b AZ
		Public_1b_Subnet
	    10.0.0.192/26
   4subnet:
		1b AZ
		Private_1b_Subnet
         10.0.0.128/26

		Auto assign pulblic IP:

		====================
		10.0.0.0/24
		32-24			8
		2            = 2  = 256 IPS
		total IPS - 5* Number of subnets
		 256    - 5* 4
		 256-20
		 236 --Available Ips to use
		 
------------
https://www.google.com/search?q=ip+addresses+classes&oq=ip+addresses+class&aqs=chrome.1.69i57j0i512j0i10j0i512j0i10l4.3613j0j9&client=ms-android-oppo-rvo3&sourceid=chrome-mobile&ie=UTF-8#imgdii=CCiPqI6qjv6G1M&imgrc=m_ayDQiEyzXrSM



https://www.davidc.net/sites/default/subnets/subnets.html


-----------


	.Create Subnets:
10.0.0.0/24

  1subnet:
		1a AZ
		PublicSubnet-1a
		10.0.0.0/26
   2subnet:
		1a AZ
		PrivateSubnet-1a
		10.0.0.64/26
   3subnet:
		1b AZ
		PublicSubnet-1b
	    10.0.0.192/26
   4subnet:
		1b AZ
		PrivateSubnet-1b
         10.0.0.128/26

		Auto assign pulblic IP:	disabled means assign public ips to server is disabled
	
Auto assign pulblic IP:  in subnet level -  

		select public subnet - action -Edit subnet settings-Auto-assign IP settings--Enable auto-assign public IPv4 address
	
Once we create VPC Default route Table wil create	

route - local

1  IGW to  1 VPC

ISP -Internet Service provider  - is the  Responsilbe  for assign public Ips 
--
	VPC:
Subnet
IGW - create and attach to VPC
Route Table- local (default) -attched  to subnets (default)
	 
create Route Table- #Public - route  --edit -  add route -0.0.0.0/0 --target - IGW
                     Subnet Assoication  -- #explictly add subnet -- ## Public subnet


create Route Table- #Private - route  --edit -  add route -0.0.0.0/0 --target - NAT Gateway
                     Subnet Assoication  -- #explictly add subnet -- ## Private subnet


Public server means- - Route table - IGW - 0.0.0.0/0 --Public --Subnet Assoication

Private Server means - Route table - NAR - 0.0.0.0/0  -- Private -Subnet  assoication

curl  -v telnet://Ip:port

Jump server/Batison Server- Public	server


In Public server  (source) we  can access Private server (destination)if they are in SAme VPC - range intranet


http://docs.gcc.rug.nl/hyperchicken/ssh-agent-forwarding-mobaxterm/

ssh-agent-forwarding:


sudo ssh-add -k ~/Desktop/Privatekey/MumbaiKey.pem

ssh-add -L

ssh -A ec2user@jumpserver

shh ec2-user@privateip

not worked agent-forwarding

so  copied pem in jump server/Bastion Server/Public server 

n ssh

  21/10/2021   23:18.36   /home/mobaxterm/Desktop/Privatekey  ssh -i "MumbaiKey.pem" ec2-user@65.0.125.11 
Last login: Thu Oct 21 17:41:46 2021 from 157.45.200.68
[ec2-user@ip-10-0-0-167 ~]$ sudo vi 1.pem
[ec2-user@ip-10-0-0-167 ~]$ sudo chmod 400 1.pem
[ec2-user@ip-10-0-0-167 ~]$ ls
1.pem

[ec2-user@ip-10-0-0-167 ~]$ ssh -i "1.pem" ec2-user@10.0.0.87     #Private server
[ec2-user@ip-10-0-0-87 ~]$ ls
[ec2-user@ip-10-0-0-87 ~]$


------------

Private server to connect  Internet  to install something
1..  conenct  to jump sever download required file 
		scp to private server
		
2..Enable internet access to Private server using NAT instance/gateway

		create a NAT Gateways  in public subnet- in samw VPC
		Elastic Ip

    .. server can access to internet .
			 not internet to access this private server 
			 
			 
NAT - Network Address Translater
is one network	device which enable access to internet fro private subnets.

private server to access API services (AWS resoruces)	

	

NACL	:

Network Access Control List:

Firewall

;


Each ENV there will be a separate network means

Dev - VPC , subnets 
Prod - VPC , subnets
Management - VPC , subnets  --jenkins , sonarQube , nexus

---

VPC Peering:

Servers in 1 vpc to talk to servers in another VPC in a private channel.

IF CIDR of VPCs are overlapping we cant do VPC peering

10.0.0.0/16 , 10.0.0.0/24 -- overlapping  - doesnt know where to route the  traffic, causes ambigute

10.0.0.0/16 , 10.1.0.0/24 --not overlapping 

we can do VPC peering 

Account
My account
Another account

Region
This Region (ap-south-1)
Another Region


Requestor;
same region
same account
different region 
different account


Acceptor;


same region
same account
different region 
different account


Peerring connection - Requestor -----Acceptor ----(Once Accept)  
add routes in both the sides  -- connected Privately 

In routes table: in private routes
10.0.0.0/24 -- Peering Connection
173.31.0.0/16   -- Peering Connection

to test ping  -- ICMP Protocal


VPC- 5 in one region

Subnet in 1 VPC -200





Day8: - 

Lab: -
NAT
NACL Farewall  at subnet level
vpc peering

Day9: -

ELB: -

Min 2 public subnet in diff AZ for implementing ELB

Types: -
1.	Application Load Balancer (Layer 7 Load balancer)- support HTTP & HTTPS
2.	Network Load Balancer (Layer 4 Load Balancer)
3.	Classic Load Balancer
Network Load balancer: -

Network Load Balancer cannot intercept the request. It simply forward the request and it’s not matter what url clients have hit.  

From Network load balancer only one application target group created because it does not intercept the request.

The request goes to ELB and the listener will forward the request to the target group. Routes based on IP Protocol.

Listener will not able to intercept for different two application.

For this we have to create different-different NLB for different-different Application.

Application LB:-

We can route host based from single ALB. We can route to multiple target Group as per the requested multiple application url.

Layer1: - Physical Layer
Layer2: - Datalink Layer
Layer3: - Network Layer
Layer4: - Transport Layer
Layer5: - Session Layer
Layer6: - Presentation Layer
Layer7: - Application Layer


Lab: -

Step 1: - Create two EC2 with User data in two different Private Subnet. This will download through NAT from public network. Open port 8080 also for Network CIDR. If you are create in one subnet then create in different AZ.
 

Step2: - Create war of maven-web-application and copy to jump server and then scp to both EC2 in private subnet under tomcat webapps.

Step3: - Configure Target Group , Load Balancer and listener for different-different application

 
 
Load-balancer: -

Load balancer -> create load balancer -> Network load balancer type -> name -> Scheme - Internet facing -> choose VPC - Mapping AZ and subnet (Min two) -> Listener (protocol, port and default action to target group) -Create a target group in another session protocol tcp port 8080 -> Health check settings -> create and add the servers check mark on include as pending

Loadbalancer - add target group -> create load balancer

Check all parameters of Load balancer and Target group

Access the loadbalancer url. Please allow your ip in SG of EC2 instances.






-------------------------------


#!/bin/bash
sudo su -

cd /opt
yum install wget unzip -y

##download java
wget -c --header "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.rpm
yum install jdk-8u131-linux-x64.rpm -y

java -version


wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.54/bin/apache-tomcat-9.0.54.zip

unzip apache-tomcat-9.0.54.zip
cd /opt/apache-tomcat-9.0.54/bin
chmod u+x *.sh
ln -s /opt/apache-tomcat-9.0.54/bin/startup.sh /usr/bin/startTomcat
ln -s /opt/apache-tomcat-9.0.54/bin/shutdown.sh /usr/bin/stopTomcat
startTomcat





http://3.84.66.154:8080/  --jenkins viriginia 


[INFO] Packaging webapp
[INFO] Assembling webapp [maven-web-application] in [/var/lib/jenkins/workspace/sample/target/maven-web-application]
[INFO] Processing war project
[INFO] Copying webapp resources [/var/lib/jenkins/workspace/sample/src/main/webapp]
[INFO] Webapp assembled in [165 msecs]
[INFO] Building war: /var/lib/jenkins/workspace/sample/target/maven-web-application.war
[INFO] WEB-INF/web.xml already added, skipping


[INFO] Installing /var/lib/jenkins/workspace/sample/target/maven-web-application.war to 
/var/lib/jenkins/.m2/repository/com/mt/maven-web-application/0.0.4-SNAPSHOT/maven-web-application-0.0.4-SNAPSHOT.war

[INFO] Installing /var/lib/jenkins/workspace/sample/pom.xml to 
/var/lib/jenkins/.m2/repository/com/mt/maven-web-application/0.0.4-SNAPSHOT/maven-web-application-0.0.4-SNAPSHOT.pom


http://mymumbai-nlb-9e84f36787a935a1.elb.ap-south-1.amazonaws.com/maven-web-application/


Press the Windows key.
Type Notepad in the search field.
In the search results, right-click Notepad and select Run as administrator.
From Notepad, open the following file: c:\Windows\System32\Drivers\etc\hosts.
Make the necessary changes to the file.


nslookup  mymumbai-nlb-9e84f36787a935a1.elb.ap-south-1.amazonaws.com

 http://mymumbai-nlb-9e84f36787a935a1.elb.ap-south-1.amazonaws.com/maven-web-application/

take ip here

52.66.132.194 raithadinachari.com

Select File > Save to save your changes.



raithadinachari.com/maven-web-application/
                     -->> DNS -- IP  --Public ip
                                                        ------Load Balancer --to private Ip
														--------Application server

http://mavenwebapp.mithuntechdevops.co.in/maven-web-application


http://javawebapp.mithuntechdevops.co.in/java-web-app

-------------------------

NLB-Java-web-app-8e438c6202429203.elb.ap-south-1.amazonaws.com/java-web/

http://nlb-java-web-app-8e438c6202429203.elb.ap-south-1.amazonaws.com/java-web/

raithadinachari.nat.com/java-web/



Application LB:


http://applb-433928400.ap-south-1.elb.amazonaws.com/maven-web-application

http://applb-2037253127.ap-south-1.elb.amazonaws.com/java-web/



http://mithuntechdevops.co.in/java-web-app/

http://mithuntechdevops.co.in/maven-web-application/

--ALLB domain

http://raithadinachari.ckm.com/maven-web-application/ 

http://raithadinachari.ckm.com/java-web


52.66.132.194 raithadinachari.com #maven NLB
3.109.253.62 raithadinachari.nat.com #java NLB
13.232.84.106 raithadinachari.ckm.com # Maven , Java APPLB


CLI:

https://awscli.amazonaws.com/AWSCLIV2.msi



What is IAM?
--------------------
•	IAM allows you to manage users and their level of access to the AWS Console.
•	IAM user limit is 5000 per AWS account and at a time you can add 10 Users.
•	1000 IAM roles are limited in one account
•	IAM User can be a member of 10 groups
•	Maximum 2 access key can provide to one User.
•	Permission is assigned to any IAM users by the JSON Policies.

•	Programmatically access (access key and secret access key) – service user to access any service
•	Interactive login access (user and password) – AWS mgmt. Console login access
 

Identity Access Management (IAM) offers the following features:-
-------------------------------------------------------------------------------------------
•	Centralized control of your AWS account
•	Shared Access to your AWS account 
•	Granular Permissions
•	Identity Federation (including Active Directory, Facebook, Linkedin etc.)
•	Multifactor Authentication
•	Provide temporary access for users/devices and services where necessary
•	Allows you to set up your own password rotation policy
•	Integrates with many different AWS services
•	Supports PCI DSS Compliance
•	 Your whole AWS security is there:
•	Users
•	Groups
•	Roles
•	Policies
•	Root account should never be used (and shared)
•	Users must be created with proper permissions
•	IAM is at the center of AWS
•	Policies are written in JSON (JavaScript Object Notation)
•	IAM has a global view
•	Permissions are governed by Policies (JSON)
•	MFA (Multi Factor Authentication) can be setup
•	IAM has predefined “managed policies”
•	It’s best to give users the minimal amount of
•	Big enterprises usually integrate their own repository of users with IAM
•	This way, one can login into AWS using their company credentials
•	Identity Federation uses the SAML standard (Active Directory)

Users: - End Users such as people, employees of an organization etc.

Groups: - A collection of users. Each user in the group will inherit the permission of the group.

Policies: - Policies are made up of documents. These documents are in a format called JSON and they give permission as to what a User/Group Role is able to do.

Roles: - You create roles and then assign them to AWS Resources.
Roles is the way by which you can use one AWS service to another AWS service. 
Example that if you want to give access to any EC2 instance to any S3 bucket.




Tips:-
----------
•	IAM is universal. It does not apply to regions at this time.
•	The "root account" is simply the account created when first setup your AWS account. It has complete Admin access.
•	New Users have NO permissions when first created.
•	New Users are assigned Access Key ID & Secret Access Keys when first created.
•	These are not the same as a password. You cannot use the Access key ID & Secret Access Key to Login in to the console. You can use this to access AWS via the APIs and command line, however.
•	You only get to view these once. If you lose them, you have to regenerate them. So, save them in a secure location. 
•	Always setup Multifactor Authentication on your root account.
•	You can create and customize your own password rotation policies.
•	One IAM User per PHYSICAL PERSON
•	One IAM Role per Application
•	IAM credentials should NEVER BE SHARED
•	Never, ever, ever, ever, write IAM credentials in code. EVER.
•	And even less, NEVER EVER EVER COMMIT YOUR IAM credentials
•	Never use the ROOT account except for initial setup.
•	Never use ROOT IAM Credentials


users

harish
mithun 

Pwsd:
H@r!$h12341

bhaskar:
H@r!$h1234





------

Docker:



Day1: -

Traditinal Deployment
Virtualized Deployment
Conatiner Deployment

Difference between container and virtualization

Cgroup
Namespaces

Day2: -

Containerization Tools
-----------------------
Docker - It has very good CLI, API, GUI(community and Enetrprise Edition)
Container-D
Podman
CoreOS
CIR-O
Rocket

Container Orchestration Tools
-----------------------------
Docker Swarm
K8s
Openshift
Mesios

Docker EE
---------
DTR - > Docker Trusted Registry(Repository)
UCP -> Universal Control Pane (GUI for Docker and Docker swarm)

Why Containers?
---------------
QUickly create, ready to run
Automate testing, integration and packaging
Support microservices
Less time to start and light weight

Mirantis Acquired Docker

Docker Desktop
----------------
1. WSL2 backend
2. Hyperv Enabled

Win10+ 64 bit and 4GB ram

Benifits of Docker
1. Portable
2. Cost saving
3. Scalable
4. Quick stop and Start
5. Quick deploy
6. Light weight
7. Easy to monitor
8. Secure

Docker Architecture
---------------------
1. Docker client - Docker cli - To send commands/instructions to docker
2. Docker daemon/engine/server/host/service - take input from User and execute that instructions/commands.
3. Docker Registry- place to maintain docker images
   1. Public registory - Docker Hub( central repository for most the softwraes as docker images)- http://hub.docker.com
   2. Private registory: - Docker registry, Nexus, jfrog, ECR(managed Registry- AWS), DTR, ACR(Azure), GCR(Google cloud) etc.
Run Docker container

Container: - Container is run time process of your image where your application will be running

Docker Image: - Image is a package (App code+, soft+lib, + configuration)

DockerFile: - Dockerfile is a file having instructions or steps to create an Image.

Lab1: - Create an Linux Instance(Amazon Linux, Centos, Ubuntu/Debian) and install Dokcer Engine

Community Edition officially it wont support Redhat.
Redhat officially doces not support Docker CE officially.


-- Daye 2:----------------

Docker Is a containerization software using which we can create build and deploy our applications as containers.

Docker is for Devlepors, Admins(DevOps) to build,ship and run applications as contianers.

Docker Editions:

Docker CE --> Comunity Edition --> Open Source(Free)
 
Docker EE --> Enterprise Edition --> Commercial

Type: Containerization
Vendor: Docker INC

O.S --> Cross Platform(Docker can be installed in any O.S)

Docker Can Be Installed in Linux, Windows OS ,mac OS  

Docker CE Can be installed in Most of the linux except redhat.

Docker EE can installed in all O.S including redhat.

Docker CE --> OpenSource Free

Docker EE --> Commerical
   DTR --> Docker Trusted Registry(Private Repo to main docker images)
   UCP --> Universal Controll Pane --> It's GUI for managing Docker Machines

Docker,CoreOS,Rocket,CRI-O,Podman --> Containerzation Platforms/Softwares.



Dockerfile --> Dockerfile is file which contains instructions to create an image. Which contains 
               Docker Domain Specific Key Words to build image.
			
		   
DockerImage --> It's a package which contains everything(Softwares+ENV+Application Code) to run your application.

DockerContainer --> Run time instance of an image.If you run docker image container will be created
                    that's where our application(process) is running.
DockerRegistriy/Repository

DockerRepo/Registry. --> We can store and share the docker images.

Public Repo --> Docker hub is a public reposotiry. Which contains all the open source softwares as 
a docker images. We can think of docker hub as play store for docker images.


Private Repo(Nexus,JFrog,D.T.R(Docker Trusted Registory)),AWS ECR  --> We can store and share the docker images with in our company
network using private repo

Docker Enigine/Daemon/Host --> It's a software or program using which we can create images & contianers.

Docker is cross platform.

Docker CE
   Docker CE will not be supported by Redhat.
   
 
Docker EE
  Docker EE will be support most of the os including redhat.

      
First Create Account in docker hub
https://hub.docker.com

What is docker hub?
It's a public repository for docker images. You can think as play store for
docker images.

Install Docker on AWS Ubuntu
############################
sudo apt update -y
sudo apt install docker.io -y
sudo service docker start

sudo usermod -aG docker ubuntu
sudo docker info

# Check docker is installed or not
   docker info

# You will get permison denied error as regular user dosn't have permisions to execute docker commands.Add user to docker group.

sudo usermod -aG docker $USER 
     or 
sudo usermod -aG docker ubuntu

# Exit From Current SSH Terminal & SSH(Login) again .Then execute 
docker ps


# Amazon Linux
================
sudo yum update -y		
sudo yum install docker -y
sudo service docker start

Add Regural user to dockergroup
sudo usermod -aG docker  <username>

ex:
sudo usermod -aG docker ec2-user

Once you add user to group exit from the server and login again.
# Get docker information
docker info

#Install Docker in Linux (Works for most of linux flavors).
sudo curl -fsSL get.docker.com | /bin/bash


Docker Home/Working Dir: 
/var/lib/docker


Install Docker on AWS RHEL  (Offcially No Support)
############################
sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo dnf install docker-ce-3:18.09.1-3.el7 -y
sudo systemctl enable docker
sudo systemctl start docker


sudo docker info

# Check docker is installed or not
docker info

# You will get permison denied error as regular user dosn't have permisions to execute docker commands.Add user to docker group.

sudo usermod -aG docker $USER 
or 
sudo usermod -aG docker ec2-user

# Exit From Current SSH Terminal & SSH(Login) again .Then execute 
docker ps




How many containers we can run in on system/server?
It dependes on your system resources(CPU,RAM).


# List Images

docker images

Day 3


Image Commands
=============

# List Images

docker images

docker image ls

# Will return only ids.
docker images -q


# Sample DockerFile Content

FROM tomcat:8-jdk8-corretto
COPY target/maven-web-application*.war /usr/local/tomcat/webapps/maven-web-application.war


# Build Image
Defautl Docker file Name: Dockefile
docker build -t <imageName> .

If you have docker file with custom name using -f <fileName> while building docker image.
docker build -f DockerfileMaven -t <imageName> .

Note: Image name should have repository details along with name and version.

Public Repo (Docker Hub)

docker build -t <registryName>/<RepoName>:<version> .

Note: If we don't mention version information. By defualt it will use 'latest' as version

ex:
docker build -t dockerhandson/maven-web-application:1 .

Private Repo (Nexus/JFrog/DTR)

docker build -t <imageName> .

docker build -t <IP/HostNameOfRepo>:<RepoPort>/<RepoName>:<version> .

ex:
docker build -t 178.90.34.12:8083/maven-web-application:1 .

Authenticate with repo

# Public Repo
docker login -u <userName> -p <password>
ex:
docker login -u dockerhandson -p password


Priavate Repo
docker login -u <username> -p <password>  <URL>
ex:
docker login -u admin -p admin123 178.90.34.12:8083


Push Docker Image to Repo
docker push <imageName>

Public Repo
docker push dockerhandson/maven-web-application:1

Private Repo
docker push 178.90.34.12:8083/maven-web-application:1

# Downlod Image from repo
docker pull <imageName>

Public Repo
docker pull dockerhandson/maven-web-application:1

Private Repo
docker pull 178.90.34.12:8083/maven-web-application:1



Docker Registry

harishkumarbr
harishkumarbr8@gmail.com
H@r!$h1234


Application
war -package
build --image
push --registory

1 clone
2 create App Package
3 create Docker Image
 harishkumarbr/javawebapp:2
 
 def buildNumber =BUILD_NUMBER
4 Push image  to registry

5 Delete old Conatiner and create new containers


Day  5:


Inspect Docker Image
==================
docker image inspect <imageId/Name>

docker inspect <imageId/Name>

How to list only layers of an image?
docker history <imageId/Name>



Delete Image

docker rmi <imageId/Name>

docker rmi -f <imageId/Name>




Note: We cann't remove images if there are running container for the image.We cann't force delete images if there is running container.

If container is in stopped(exited) state we can force delete image for the stopped container.

what is dangling images in docker?
The image which doesn't have repository mapping or tag.

How to delete all the images?
docker rmi -f imageId imageId imageId

docker rmi -f $(docker images -q)

docker system prune 
Will delete all stopped containers , unused docker networks and dangling images.

docker image prune

Will delete angling images.

We can tag image with repo.

# We can use docker tag to tag images with multiple repo.
docker tag <ImageId/ExistingImageName> <ImageName>


What is working directory of docker?
/var/lib/docker


How can we move/copy images from one server to another server with out repo?

In Source Server(where you have image)
# It save image(All the layers) as a tar file

docker save -o <fileName>.tar <imageName/Id>


Then SCP tar file from Source Server to Destination Server

# In destination server
docker load -i <fileName>.tar


List Dangling images

docker images -f dangling=true

Remove Dangling Images
docker rmi $(docker images -f dangling=true -q)


docker system prune 

This will remove:
  - all stopped containers
  - all networks not used by at least one container
  - all dangling images

docker image prune
This will remove:
- all dangling images

docker container prune
This will remove:
  - all stopped containers

docker network prune
This will remove:
  - all networks not used by at least one container


---------


day 6:


Container Commands:
===================
How to create a contianer?

docker run or docker create

docker create --name <containerName> -p <hostPort>:<containerPort> <imageName>


docker run --name <containerName> -p <hostPort>:<containerPort> <imageName>

# Create a container in dettached mode
docker run -d --name <containerName> -p <hostPort>:<containerPort> <imageName>

what is the difference b/w docker run and docker create?

docker create will only create a container but it will not start the container.
docker run will create a container and start the container.

what is port publish or port mapping in docker ?
If We have to access application which is running as container from out side of docker we can't access using continerIP & ContainerPort. We can publish contianer port using host port using -p or --publish.
So that we can access using HostIP(docker server IP) and Host Port from outside docker.


docker run -d -p 8080:8080 --name mavenwebapp  dockerhandson/maven-web-application

Access Application which is running Using  Docker Server IP & Host Port.

http://<DOCKERSERVERPUBLICIP>:<HOSTPORT>/maven-web-application


# How to create container in interactive mode?

docker run -it --name <nameofthecontainer>  <image>

List Running Containers
=======================

docker ps 
docker container ls

List All Containers
==================

docker ps -a

docker container ls -a

List only running container ids
==============================

docker ps -q

docker container ls -q


List all container ids
==============================

docker ps -aq

docker container ls -aq


Start the container
===================
docker start <containerId/Name>


Restart Container

docker restart <containerId/Name>

Stop Container

docker stop <containerId/Name>




Kill container

docker kill <containerId/Name>

What is the difference b/w docker stop & docker kill?

docker stop will first send SIGTERM then SIGKILL it will kill process with grace period. Docker kill send SIGKILL it will kill process with out any grace period.

Can we have/run more than one process in a container?
Yes Can we have. But it's not suggestable. 


Pause contaier process.

docker pause <containerId/Name>

docker unpuase <containerId/Name>

Inspect container
docker inspect <containerId/Name>
docker container inspect <containerId/Name>


It will  container if it is stopped.
docker rm  <containerId/Name>

Force Remove If container is runing we can force remove
docker rm -f <containerId/Name>


How to delete only stopped containers
docker rm $(docker ps -aq --filter  status="exited")

How to delete all containers
docker rm -f $(docker ps -aq)



How to trouble shoot or debug application which is running as a container?

docker logs <containerId/Name>
docker logs --tail <NoOflines> <containerId/Name>

# It will display process details which is runing inside a container.
docker top <containerId/Name>

# It will display resource(RAM,CPU) consumtion details.
docker stats <containerId/Name>

# Execute commands on a runinging container.
docker exec <containerId/Name> <cmd>

ex:
docker exec javawebapp ls
docker exec javawebapp pwd

How to go inside a container?

docker exec -it <containerId/Name> /bin/bash
 
       or

docker exec -it <containerId/Name> /bin/sh

# Docker attach will attach container process or shell to host server
docker attach <containerId/Name>

If we have to come out with out stoping the process cntl p+q.

How to copy files from container to host system or host system to container?

docker cp

Container to the system

docker cp <containerName>:</pathOftheContainerFile>  <SystemPath>/<fileName>

docker cp javawebappone:/usr/local/tomcat/logs/catalina.2020-04-23.log  javawebappone.log

system to the Container 

docker cp  <SystemPath>/<fileName><containerName>:</pathOftheContainerFile> 

docker cp  /home/ubunut/test.log javawebappone:/usr/local/tomcat/logs/test.log

docker rename <ContainerId/NameOld> <NewName>

What is docker commit?
Using docker commit we can create image from the continer.

docker commit <containerId/Name> <imageName>

Can we set CPU,RAM limit for the containers while creating?
Yes We set using options while creating a container.

  
  

Jenkins -Docker

sudo apt  update
    sudo apt install openjdk-8-jdk -y
    java -version
    javac -version


wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add -

sudo sh -c 'echo deb https://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list'

sudo apt update

sudo apt install jenkins

sudo usermod -aG docker jenkins

systemctl enable jenkins

systemctl restart jenkins

systemctl status jenkins
http://65.2.125.76:8080/

Admin
admin

stage('docker login and push') {
         withCredentials([string(credentialsId: 'Docker_hub_login_passowrd', variable: 'DockerHubPassword')]) {
             sh "docker login -u mohit1998 -p ${DockerHubPassword}"
           }
             sh "docker push mohit1998/maven-web-application:${buildNumber}"
        }

 
http://13.233.125.150:7071/java-web-app/ --harishkumarbr/java-web-app-1.0:1

http://13.233.125.150:7072/java-web-app/  harishkumarbr/java-web-app-docker:13

http://13.233.125.150:7070/maven-web-application/  
____________



FROM tomcat:8.0.20-jre8
COPY target/maven-web-app*.war /usr/local/tomcat/webapps/maven-web-application.war





node {
    def mavenHome = tool name : "maven3.8.3"
    //def buildNumber = BUILD_NUMBER
    stage('Take Code -Git') { // for display purposes
        // Get some code from a GitHub repository
        git credentialsId: '42f3f839-714c-4cc1-bffd-f1bf3676ffde', url: 'https://github.com/harishgowdabr/java-web-app-docker.git'
        
    }
    
    stage ('clean-package'){
        sh "${mavenHome}/bin/mvn clean package"
    }
    
    stage ('dockerbuild'){
        sh "docker build -t harishkumarbr/java-web-app-docker:${BUILD_NUMBER} ."
    }
     stage ('Docker login and push'){
         withCredentials([string(credentialsId: 'Docker_hub_pwd', variable: 'Docker_Hub_pwd')]) {
    sh "docker login -u harishkumarbr -p ${Docker_Hub_pwd}"
}
  sh "docker push harishkumarbr/java-web-app-docker:${BUILD_NUMBER}"
         
     }
     
     stage ('Deploy App As DockerConatiner in Docker Dev Server')
     {
         sshagent(['Docker_Server']) {
   sh "ssh -o strictHostKeyChecking=no ubuntu@172.31.40.238 docker rm -f javawebapp || true"
   sh "ssh -o strictHostKeyChecking=no ubuntu@172.31.40.238 docker run -d -p 7072:8080 --name javawebapp harishkumarbr/java-web-app-docker:${BUILD_NUMBER}"
}
     }
  
}

---------------------------
Day 7





Image is package (AppCode+ Softwares)
Cotainer is a running process of an image.

If you have to crate docker image we need a dockerfile.


Dockerfile --> Dockerfile is file which contians instructions(Docker Domain Specific KeyWords) to create an image. 
Docker Daemon  will process these instruction from top to bottom.



EX:

FROM tomcat:8.0.20-jre8
COPY target/java-web-app*.war /usr/local/tomcat/webapps/java-web-app.war


DockerImage --> It's package which contains application code + all it's dependencies(Software+ENV Varibles + Config Files) together.


Dockerfile keywords
===================

FROM

MAINTAINER

COPY

ADD

RUN

CMD

ENTRYPOINT

WORKDIR

ENV

EXPOSE

USER

VOLUME

LABEL

ARG


FROM --> FROM indicates the image base image which we are using to build our own image.


Syntax: 
FROM  <ImageName>

Ex:

FROM tomcat:8.0.20-jre8(Software)

FROM openjdk:8-alpine


MAINTAINER --> It's will be used as commnets to describe author/owner who is maintaning docker file.

MAINTAINER MithunTech <devopstrainingblr@gmail.com>


COPY --> Using COPY we can copy files/folders to the image. Files/Folders will be copied while creating an image.
It will copy local files from host server(docker server)from where we are building image to the image while creating a image.

SYTNAX:
======
COPY <source>                <destination> 
      ServerFile/FolderPath   PathInsideImage
EX:
COPY target/java-web-app.war /usr/local/tomcat/webapps/java-web-app.war

# Below also valid it will copy all the files/folder from HOST Machine current working
directory to Image working dirctroy.
COPY . .

ADD --> ADD also can copy files to the image while creating image. 
ADD can copy local files from host server and also it can download files from remote HTTP/S locations while creating a image.
		
ADD <URL> <destination>

ADD <source> <destination>

It can handle remote URLs
It can also auto-extract tar files.
EX:

# File from http(s) location
ADD https://downloads.apache.org/tomcat/tomcat-8/v8.5.54/bin/apache-tomcat-8.5.54.zip /opt/ 

# Local file
ADD target/java-web-app.war  /usr/local/tomcat/webapps/java-web-app.war

Note: If it's tar file ADD will copy file and also it will extract tar file.

RUN ,CMD, ENTRYPOINT instruncations can be used to execute commands.

RUN --> RUN instruncation  will  execute commands .RUN commands or instructions will be executed while creating an image. Next to run you can mention any command based on base os of image.
We can have n number RUN instructions in a docker file all the RUN instructions will be exectued one after the other from top to bottom.

Syntax: 
#Shell Form
RUN <commond with args> 
#Executable Form
RUN ["commond" , "Arg1","Arg2"]

EX:
RUN mkdir -p /opt/app				
RUN tar -xvzf /opt/apache-tomcat-8.5.54.tar.gz 


CMD  --> CMD instruncation will execute commands. CMD commands or instructions will be executed while creating a container.CMD insturction can be used to start the process inside the container.

#Shell Form
CMD <commond with args> 
#Executable Form
CMD ["command" , "Arg1","Arg2"]

# Shell Form
CMD java -jar springapplication.jar. 
# Executable form
CMD ["java", "-jar" , "springapplication.jar"] 


What is difference b/w RUN & CMD?

RUN instructions will be executed while creating a image. CMD Instructions will be executed while creating a
container.We can have more than one RUN keyword in a docker file. All the RUN keywords will be processed while creating an image in the defined order(top to bottom).

Can we have more than one CMD in dockerfile?
Yes you can have. But only the last one/recent one in the order will be proccessed while creating a container.


Can we have both CMD & ENTRYPOINT in docker file? 

Yes we can have both in a docker file. CMD instructions will not be executed if we have both CMD & ENTRYPOINT.CMD instructions will be passed as an arguments for ENTRYPOINT.

FOR Example:
CMD ls
ENTRYPOINT ["echo", "Hello"]

IT Will be executed as below
/bin/echo HELLO ls 

# Out Put
Hello ls

Requirement always we have to execute sh catalina.sh . But argument by default it has to execute "start". But dynamically i should a option to pass different argument while creating a container.

CMD start
ENTRYPOINT ["sh", "catalina.sh"]







Shell vs Exec form From
EXEC:
<instruction> ["executable", "param1", "param2", ...]

When the exec form of the CMD instruction is used the command will be executed without a shell.

CMD ["executable","param1","param2"]

This is the preferred form for CMD and ENTRYPOINT instructions.
Whether you’re using ENTRYPOINT or CMD (or both) the recommendation is to always use the exec form so that, it’s obvious which command is running as PID 1 inside your container and it can listen to SIGNALS.

Examples:
FROM ubuntu:trusty  
CMD ["/bin/ping","localhost"]


Examples:
FROM ubuntu:trusty  
ENTRYPOINT ["/bin/ping","localhost"]

When instruction is executed in exec form it calls executable directly, and shell processing does not happen.
For example, the following snippet in Dockerfile-
ENV name John Doe 
ENTRYPOINT ["/bin/echo", "Hello, $name"]

When the container runs as docker run -it <image> , it will produce output- Hello, $name
Note that the variable name is not substituted.

SHELL form-
<instruction> <command>
CMD executable param1 param2

When using the shell form, the specified binary is executed with an invocation of the shell using /bin/sh -c

Examples:
FROM ubuntu:trusty 
ENTRYPOINT echo "Hello world"


FROM ubuntu:trusty 
CMD echo "Hello world"

When instruction is executed in shell form it calls /bin/sh -c <command> under the hood and normal shell processing happens.






ENV --> ENV instruction sets the environment variable and this sets the environment for the subsequent build instructions. It takes two forms: one with a single variableENV <key> <value> and another with multiple variables ENV <key> =<avlue> <key> = <value>.



ARG -> ARG Instruction defines a variable that can be passed at build time. Once it is defined in the Dockerfile you can pass with this flag --build-arg while building the image. We can have multiple ARG instruction in the Dockerfile. ARG is the only instruction that can precede the FROM instruction in the Dockerfile.

ARG values are not available after the image is built. A running container won’t have access to an ARG variable value


EX:

ARG TAG=latest
FROM centos:$TAG
docker build -t <image-name>:<tag> --build-arg TAG=centos8 .



WORKDIR --> WORKDIR  is used to define the working directory of a Docker container at any given time.
 The command is specified in the Dockerfile.It is optional (default is / , but base image might have set it), 
 but considered a good practice. Subsequent instructions in the Dockerfile, such as RUN , CMD and ENTRYPOINT will operate in this dir.

Ex:

WORKDIR /app



USER

LABEL

  
  
USER 

The USER instruction sets the user name (or UID) and optionally the user group (or GID) to use when running the image and for any RUN, CMD and ENTRYPOINT instructions that follow it in the Dockerfile


LABEL

The LABEL instruction adds metadata to an image. A LABEL is a key-value pair. To include spaces within a LABEL value, use quotes and backslashes as you would in command-line parsing. A few usage examples:


LABEL branch=develop

LABEL description="This text illustrates"


An image can have more than one label. You can specify multiple labels on a single line.

LABEL label1="value1" label2="value2" other="value3"
  
  
EXPOSE

The EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime.

he EXPOSE instruction does not actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published. To actually publish the port when running the container, use the -p flag on docker run to publish and map one or more ports.

EXPOSE 8080


Jenkins as Container

RUN yum install java

RUN curl ___.war //  ADD __.war /path


CMD ["java","-jar","jenkins.war"]

push registory/repository

Session 10

Multi Stage Docker file
======================

Multi-stage Docker builds let you write Dockerfiles with multiple FROM statements. This means you can create images which derive from several bases, which can help cut the size of your final build.

Docker images are created by selecting a base image using the FROM statement. You then add layers to that image by adding commands to your Dockerfile.

With multi-stage builds, you can split your Dockerfile into multiple sections. Each stage has its own FROM statement so you can involve more than one image in your builds. Stages are built sequentially and can reference their predecessors, so you can copy the output of one layer into the next.

Example
=======
#Maven
FROM maven:3.5-jdk-8-alpine as build
WORKDIR /app
COPY . .
RUN mvn install

#Tomcat
FROM tomcat:8.0.20-jre8
COPY --from=build /app/target/maven-web-application*.war /usr/local/tomcat/webapps/maven-web-application.war




Session 11




Docker Networks

What is network ?
Group of servers/devices connected to each other in a specific network. If Servers
are in same network each one can talk to another server.

Docker network

If One Container has to talk to another Container in Docker. Both has to created under
same docker network.

If Containers are in two different networks. They can't accees each other.


In which docker network the container will be created if we don't mention network name while
creating a container ?

Containers will be created in a default bridge network.
If we don't mention network name while creating a container.

How to list networks in docker?

docker network ls

Docker will have 3 networks by default.
bridge(default)
host
none/null


docker run -d --name javawebapp -p 8080:8080 dockerhandson/java-web-app

docker run -d --name mavenwebapp dockerhandson/maven-web-app


If containers are created in a default bridge network. Communcation will happen only
with IP Address of container. Communcation will not happen using containerName(hostName).

To Check Go inside javawebapp container and ping mavenwebapp container using name & ip. When we ping using ip it will work it will not able to communicate using name.


Developers should not code the connectivity based on the IP in case of contianers. Since IP address of cotnainers will be dynamic.
IP will keep changing.


How to create a custom bridge network ?

# Create Network
Syntax: docker network create -d <driver> <networkName>

Ex: 
docker network create -d bridge flipkartnetwork

# Inspect network
docker network inspect <networkNameOrId>

If containers are created in custom bridge network. Each container can access other using containerName/ContainerIP.

# Delete Containers which are running in default bridge or create container with different name.

docker run -d --name javawebapp -p 8080:8080  --network flipkartnetwork dockerhandson/java-web-app

docker run -d --name mavenwebapp  --network flipkartnetwork dockerhandson/maven-web-app

Create both containers in same network and try to ping mongo contaner with name & IP  from sprinapp container or vice versa it will work with both.



# Remove unused networks
docker network prune 

# Remove Network
docker network rm <networkNameOrId>

Docker Host Network.

If we create contaienrs in host network. Container will not have IP Address. Container will be created
in a system network.

But we can't create more than one cotnainer with same container port in host network.We no need to do port publish to access
containers.


Docker none/null network

If we create contaienrs in none/null network. Container will not have IP Address.We can't 
accees these contianers from out side or from any other cotnainer.


We connect container to more than one network by using docker connect.

docker network connect <networkName/Id>  <containerName/Id>

docker network disconnect <networkName/Id>  <containerName/Id>



  


Another Application

Ex: 
docker network create -d bridge springappbridge

# Create Network if not exists
docker run -d --name mongo -e MONGO_INITDB_ROOT_USERNAME=devdb -e MONGO_INITDB_ROOT_PASSWORD=devdb@123  --network springappbridge mongo

docker run -d -p 8080:8080 --name springapp  -e MONGO_DB_HOSTNAME=mongo -e MONGO_DB_USERNAME=devdb -e MONGO_DB_PASSWORD=devdb@123 --network springappbridge  dockerhandson/spring-boot-mongo







Docker Volumes

Volumes:
=======

Docker containers are used to run applications in an isolated environment. By default, all the changes(data) inside the container are lost when the container is removed/recreated. If we want to keep data between runs, Docker volumes and bind mounts can help. 

Docker volumes are file systems mounted on Docker containers to preserve data generated by the running container.


 Create docker network using below commond(If it's not created already)

     docker network create  -d bridge springappbridge

  Create a mongo contianer with out volume in above network
	 	 
	 docker run -d --name mongo -e MONGO_INITDB_ROOT_USERNAME=devdb -e MONGO_INITDB_ROOT_PASSWORD=devdb@123  --network springappbridge mongo

 Create Spring Application Container in above network & which will talk to mongo data base container

       docker run -d -p 8080:8080 --name springapp  -e MONGO_DB_HOSTNAME=mongo -e MONGO_DB_USERNAME=devdb -e MONGO_DB_PASSWORD=devdb@123 --network springappbridge dockerhandson/spring-boot-mongo
    
	 
 Access Spring application & insert data it will be inserted to mongo db. Delete and recreate mongo container 
   what ever you have inserted will no longer be availbale. As once we delete contaienr data also will be deleted
   in container.
   
   To take maitain state (date) of container we have to use volunmes
   
   
   
Bind Mounts:

Bind mounts may be stored anywhere on the host system. They may even be important system files or directories.
 Non-Docker processes on the Docker host or a Docker container can modify them at any time.




# Volumes Using bind mount
mkdir mongodbdata(If not exists)
# Delete container if exists then create
docker run -d --name mongo -v ~/mongodbdata:/data/db 
 -e MONGO_INITDB_ROOT_USERNAME=devdb -e MONGO_INITDB_ROOT_PASSWORD=devdb@123  --network springappbridge mongo




  

The docker-compose stop command will stop your containers, but it won't remove them.
 The docker-compose down command will stop your containers, 
but it also removes the stopped containers as well as any networks that were created






Docker Peristent Volumes

Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure and OS of the host machine, volumes are completely managed by Docker. Volumes have several advantages over bind mounts:

Volumes are easier to back up or migrate than bind mounts.
You can manage volumes using Docker CLI commands.
Volume drivers let you provsion volumes on remote hosts or cloud providers.

Volumes are stored in a part of the host filesystem which is managed by Docker (/var/lib/docker/volumes/ on Linux).
Non-Docker processes should not modify this part of the filesystem. Volumes are the best way to persist data in Docker.




# To list volumes
docker volume ls
	 
create a volume a Local Volume(Execute docker volume ls to check existing volumes)

docker volume create mongodb
	
   
 Use above volume while creating container.

     docker run -d --name mongo -v mongodb:/data/db   
	 -e MONGO_INITDB_ROOT_USERNAME=devdb -e MONGO_INITDB_ROOT_PASSWORD=devdb1234  --network springappnetwork mongo
   
Access Spring application & insert data it will be inserted to mongo db. Delete and recreate mongo container 
   with same volume mapping. You can see the data back.
   
   
Volume Driver Plugin --> It's a piece of code or software which is responsible for creating a 
storage and attaching the storage to the container.   
    
 ===== Network Volumes Using AWS EBS==========
 
 1) Create IAM User with EC2 Full Access and user access key & Secret Key of the same. Replace your access key & secret below. Or Use Your root aws account access Key & Secret Key.

 docker plugin install rexray/ebs EBS_ACCESSKEY=<ACCESSKEY> EBS_SECRETKEY=<SECRETKEY>
 
 EX:
 
 docker plugin install rexray/ebs EBS_ACCESSKEY=AKIAJRVS26WY3UKXG57Q EBS_SECRETKEY=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

 docker volume create --driver rexray/ebs --name ebsvolume

 docker run -d -p 27017:27017 -v ebsvolume:/data/db  --name mongo -e MONGO_INITDB_ROOT_USERNAME=devdb -e MONGO_INITDB_ROOT_PASSWORD=devdb1234  --network springappnetwork mongo


Map Volumes As Read Only using below option>
-v <volumeName/BindMount>:<containerPath>:ro





Docker Compose
==============

Docker Compose is a tool for defining and running multicontainer applications.


With out compose to deploy above applications which has only 2 images we executed below commnads.


docker network create -d bridge springappnetwork

docker volume create -d local mongobkp

 docker run -d --name mongo -v mongobkp:/data/db -e MONGO_INITDB_ROOT_USERNAME=devdb -e MONGO_INITDB_ROOT_PASSWORD=devdb1234  --network springappnetwork mongo
 
 docker run -d -p 8080:8080 --name springapp  -e MONGO_DB_HOSTNAME=mongo -e MONGO_DB_USERNAME=devdb -e MONGO_DB_PASSWORD=devdb1234 --network springappnetwork dockerhandson/spring-boot-mongo
 

With Docker Compose we deploy/create all of the above 4 with single command using compose file.


With Compose

Install docker compose using below command:

sudo apt install docker-compose

We will define all the serivces(cotainers) details in compose file using compose file we can deploy multi container applications.

Defautl name : docker-compose.yml or docker-compose.yaml


Example 1: (Volumes & Networks also will be created by docker compose)


version: '3.1'

services:
  springboot:
    image: dockerhandson/spring-boot-mongo:latest
    restart: always # This will be ignored if we deploy in docker swarm
    container_name: springboot
    environment:
    - MONGO_DB_HOSTNAME=mongo
    - MONGO_DB_USERNAME=devdb
    - MONGO_DB_PASSWORD=devdb1234
    ports:
      - 8080:8080
    working_dir: /opt/app
    depends_on:
      - mongo
    deploy:  # This will be considered only in docker swarm.
      replicas: 2
      update_config:
        parallelism: 1
        delay: 20s
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 5
    networks:
    - springappnetwork

  mongo:
    image: mongo
    container_name: springboot-mongo
    environment:
    - MONGO_INITDB_ROOT_USERNAME=devdb
    - MONGO_INITDB_ROOT_PASSWORD=devdb1234
    volumes:
      - mongobkp:/data/db
    restart: always
    networks:
    - springappnetwork
    
volumes:
  mongobkp:
    driver: local
    
networks:
  springappnetwork:
    driver: bridge
	  
  
 
Commands
# Syntax Check
docker-compose config 
# Create Services/Contianers
docker-compose up -d  
  
# Remove Services/Contianers 
docker-compose down	  




Example 2: (Volumes & Networks will not be created by docker compose.As we set volumes and networks as external)
==========
 
version: '3.1'

services:
  springboot:
    image: dockerhandson/spring-boot-mongo:latest
    restart: always # This will be ignored if we deploy in docker swarm
    container_name: springboot
    environment:
    - MONGO_DB_HOSTNAME=mongo
    - MONGO_DB_USERNAME=devdb
    - MONGO_DB_PASSWORD=devdb1234
    ports:
      - 8080:8080
    working_dir: /opt/app
    depends_on:
    - mongo
    deploy:  # This will be considered only in docker swarm.
      replicas: 2
      update_config:
        parallelism: 1
        delay: 20s
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 5
    networks:
    - flipkartnetwork
  mongo:
    image: mongo
    container_name: springboot-mongo
    environment:
    - MONGO_INITDB_ROOT_USERNAME=devdb
    - MONGO_INITDB_ROOT_PASSWORD=devdb1234
    volumes:
      - mongodb:/data/db
    restart: always
    networks:
    - flipkartnetwork
    
volumes:
  mongodb:
    external: true
    
networks:
  flipkartnetwork:
    external: true
	
	
If docker compose file with custom name.	
docker-compose -f <CustomeComposeFileName>.yml <command>

Ex:
docker-compose -f docker-compose-springapp.yml config
docker-compose -f docker-compose-springapp.yml up -d

docker-compose -f docker-compose-springapp.yml down


Docker Compose Commands:

  config             Validate and view the Compose file
  create             Create services
  down               Stop and remove containers, networks, images, and volumes
  exec               Execute a command in a running container
  help               Get help on a command
  images             List images
  kill               Kill containers
  logs               View output from containers
  pause              Pause services
  port               Print the public port for a port binding
  ps                 List containers
  pull               Pull service images
  push               Push service images
  restart            Restart services
  rm                 Remove stopped containers
  run                Run a one-off command
  scale              Set number of containers for a service
  start              Start services
  stop               Stop services
  top                Display the running processes
  unpause            Unpause services
  up                 Create and start containers
  version            Show the Docker-Compose version information





# In Normal(Standalone) Docker Server We can use below command to create a containers.
docker-compose up

# In docker swarm we will use below command to deploy services using docker compose.
docker stack deploy --compose-file docker-compose.yml <stackName>




Containerization Tools: docker,rocker(rkt),coreos

Container Orchestration Tools: docker swarm,kubernetes,openshift ..etc



H.A --> HighAvailability
F.T --> Fault Tolarence
Scalability
L.B


Create 3 Ubuntu Linux Systems in AWS Execute following commands 

#!/bin/bash
sudo apt-get update
sudo apt-get install curl -y
sudo curl -fsSL get.docksal.io | bash
sudo usermod -aG docker ubuntu

Logout from the the terminal and login again

Note: Make Sure You Open Required/All Ports in AWS Security Groups.

======================================================================
# Initialize docker swarm cluster by exeuting below command on docker server which you want make it as Manager

docker swarm init 

# Initialyze Docker swarm with Public IP
Note: Don't use below(If restart your systems public ip will change will break your cluster) use above commond to initilaze cluster.

docker swarm init  --listen-addr=eth0 --advertise-addr $(curl http://169.254.169.254/latest/meta-data/public-ipv4) (Only run in manager node)



docker swarm join-token worker (Get the token in manager & exeute in nodes)


======================================================================


Docker Swarm has two modes

Global   --> All the nodes (3 servers 1 Manager + 2 Workers)
Replicas --> It will deploye based on replicated number.



What is serivce in docker or docker swarm?

Serivce is nothing but a collection of one or more replicas(contianers) of same type(Image).


What is stack in docker or docker swarm?

Stack is nothing but a collection of one or more serivces of some application.

# Default Mode Replica and it will create a service with 1 replica
docker service create  -p 8080:8080 --name javawebapp dockerhandson/java-web-app

docker service ls

docker service ps <servcieNAme>
ex:
docker service ps javawebapp

docker service scale <serviceName>=<noOfReplicas>
docker service scale javawebapp=3

docker service scale javawebapp=2


# While creating service we can mention number of replicas as below
docker service create  -p 5000:5000 --name pythonapp --replicas 2 dockerhandson/python-flask-app:1

# List contianers running in the given node managed by swarm
docker node ps <nodeName>


# Global Mode
docker service create  --name nginx --mode global   -p 80:80 nginx




# User constriants to create containers in specific docker hosts based on condtion
docker service create  -p 8080:8080 --name javawebapp --replicas 2 --constraint 'node.role==worker' dockerhandson/java-web-app


# Add labels to node
docker node update --label-add key=value <nodeid>
Ex: docker node update --label-add type=appServer qmdh9tgvdef99sryhbezswfl9

#Use above label in constrainsts
docker service create  -p 8080:8080 --name javawebapp --replicas 1 --constraint 'node.labels.type==appServer' dockerhandson/java-web-app

# Drain Nodes in Cluster(Swarm will not create containers in drained nodes)
docker node update --availability drain <NodeID>

# Make Node Active in Cluster
docker node update --availability active <NodeID>


# Create a service with a rolling update policy
docker service create --replicas 2  --name javawebapp  --update-delay 30s --update-parallelism 1  dockerhandson/java-web-app:1

# Update service image without down time.
docker service update --image dockerhandson/java-web-app:2 javawebapp





docker stack deploy --compose-file docker-compose.yml springmongo

docker stack ls

docker stack rm <stackName>

docker stack services <stackName>

docker stack ps <stackName>


version: '3.1'

services:
  springboot:
    image: dockerhandson/spring-boot-mongo:latest
    restart: always # This will be ignored if we deploy in docker swarm
    container_name: springboot
    environment:
    - MONGO_DB_HOSTNAME=mongo
    - MONGO_DB_USERNAME=devdb
    - MONGO_DB_PASSWORD=devdb1234
    ports:
      - 8080:8080
    working_dir: /opt/app
    depends_on:
    - mongo
    deploy:  # This will be considered only in docker swarm.
      replicas: 2
      update_config:
        parallelism: 1
        delay: 20s
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 5
    networks:
    - flipkartnetwork
  mongo:
    image: mongo
    container_name: springboot-mongo
    environment:
    - MONGO_INITDB_ROOT_USERNAME=devdb
    - MONGO_INITDB_ROOT_PASSWORD=devdb1234
    volumes:
      - mongodb:/data/db
    restart: always
    networks:
    - flipkartnetwork
    
volumes:
  mongodb:
  
    
networks:
  flipkartnetwork:
    external: true


# To come out of swarm execute below commond in worker node
docker swarm leave

# Remove node from Manager
docker node rm <nodename>



# Use private ECR As Private Repo repo's.
#Send registry authentication details to swarm agents using --with-registry-auth	



Create ECR in AWS.
  
Note: Replcae your ECR URL when with 935840844891.dkr.ecr.ap-south-1.amazonaws.com when u create and push.
ECR
===
docker build -t 935840844891.dkr.ecr.ap-south-1.amazonaws.com/maven-web-app

# Authentication with ECR(Install AWS CLI And execute below command after attaching IAM Role)
aws ecr get-login-password --region ap-south-1 | docker login --username AWS --password-stdin 935840844891.dkr.ecr.ap-south-1.amazonaws.com  


Note: Create IAM Role with required policy and attach to EC2 Servers.

# IAM Policiy to autheticate and pull & Push image.
AmazonEC2ContainerRegistryFullAccess

# IAM Policiy to autheticate and pull image.
AmazonEC2ContainerRegistryReadOnly


#While Creating a Service Or Stack send registry authentication details to swarm agents using --with-registry-auth	
	
docker service create  -p <hostPort>:<containerPort> --name <serviceName> --replicas 1  --with-registry-auth <imageName>

docker stack deploy --compose-file docker-compose.yml  --with-registry-auth <stackName>


	
# Use private Nexus As Private Repo repo's.
#Send registry authentication details to swarm agents using --with-registry-auth	

If you are using insecure(http) registry. Make sure you have flow below step in all servers (jenkins,docker swarm master,workers). Below Steps from 1 to 6 are not requried in real time since we will have secure(https) repositories like ECR or nexus with https.

1.	Login as root user
2.	Go to /etc/docker
      cd /etc/docker
3.	Then create a file called daemon.json 
      vi /etc/docker/daemon.json
4.	Write these script in daemon.json

{
  "insecure-registries": [ "<IPOfPrivateRepo>:<dockerRepoPort>" ]
}

ex:

{
  "insecure-registries": [ "172.31.45.81:8083" ]
}

Note: Replace with your nexus ip instead of 172.31.45.81. Make Sure You Opened 8083 port in nexus server security
group.

(Here we are allowing our docker daemon to access the Nexus Hosted Repo)

5.	Save the file 

6.	Restart docker
sudo systemctl restart docker

Note: Step 1 to 6 is not required if we are using secure repo's.Like ECR which is already confifured with https.

Before Pushing Image From Jenins execute docker login

docker login -u <username> -p <password>  <URL>
Ex:
docker login -u admin -p admin123 172.31.45.81:8083	
	
Execute docker login only in master

docker login -u <username> -p <password>  <URL>

docker login -u admin -p admin123 172.31.45.81:8083	

# While Creating a Service Or Stack send registry authentication details to swarm agents using --with-registry-auth	
	
docker service create  -p <hostPort>:<containerPort> --name <serviceName> --replicas 1  --with-registry-auth <imageName>

docker stack deploy --compose-file docker-compose.yml  --with-registry-auth <stackName>




  -------
  Docker Swarm
  
  
  Create 3 Ubuntu Linux Systems in AWS Execute following commands 

#!/bin/bash
sudo apt-get update
sudo apt-get install curl -y
sudo curl -fsSL get.docksal.io | bash
sudo usermod -aG docker ubuntu

Logout from the the terminal and login again

Note: Make Sure You Open Required/All Ports in AWS Security Groups.

======================================================================
# Initialize docker swarm cluster by exeuting below command on docker server which you want make it as Manager

docker swarm init 

# Initialyze Docker swarm with Public IP
Note: Don't use below(If restart your systems public ip will change will break your cluster) use above commond to initilaze cluster.

docker swarm init  --listen-addr=eth0 --advertise-addr $(curl http://169.254.169.254/latest/meta-data/public-ipv4) (Only run in manager node)



docker swarm join-token worker (Get the token in manager & exeute in nodes)


======================================================================



docker run  imageName --> It will create/deploy one application  in single machine  --> docker service create

docker-compose up     --> To create/deploy mutiple applications in single mahcine   --> docker stack deploy --composefile docker-compose.yml



Docker Swarm has two modes

Global   --> All the nodes (3 servers 1 Manager + 2 Workers)
Replicas --> It will deploye based on replicated number.




docker service create  -p 8080:8080 --name javawebapp --replicas 2 dockerhandson/java-web-app

# User constriants to create containers in specific docker hosts based on condtion
docker service create  -p 8080:8080 --name javawebapp --replicas 2 --constraint 'node.role==worker' dockerhandson/java-web-app

# Create a service with a rolling update policy
docker service create --replicas 2  --name nginxservice  --update-delay 10s --update-parallelism 1  nginx:alpine
docker service update --image nginx:latest nginxservice



# Create a service with Volume mapping
docker service create  -p <hostPort>:<containerPort> --name <serviceName> --replicas 1  --mount type=volume,source=<volumeName>,destination=<containerfolderPath> <imageName>

# List Services
docker service ls

# List Services process
docker service ps <servicenName>

# Scale Services 
docker service scale javawebapp=3

docker service rm javawebapp


# Add labels to node
docker node update --label-add key=value <nodeid>
Ex: docker node update --label-add server=nodeone qmdh9tgvdef99sryhbezswfl9

#Use above label in constrainsts
docker service create  -p 8080:8080 --name javawebapp --replicas 1 --constraint 'node.labels.server==nodeone' dockerhandson/java-web-app


# Drain Nodes in Cluster(Swarm will not create containers in drained nodes)
docker node update --availability drain <NodeID>


# Stack Deploy

version: '3.1'

services:
  springboot:
    image: dockerhandson/spring-boot-mongo:latest
    restart: always
    container_name: springboot
    ports:
      - 8182:8080
    working_dir: /opt/app
    depends_on:
      - mongo
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s      
      restart_policy:
        condition: on-failure   

  mongo:
    image: mongo
    container_name: mongo
#    ports:  # for demo/debug purpose only
#      - 27018:27017
    volumes:
      - data:/data/db
      - data-bkp:/data/bkp
    restart: always
    
volumes:
    data:
    data-bkp:
	
=================================================================


docker stack deploy --compose-file docker-compose.yml springmongo

docker stack ls

docker stack rm <stackName>

# To come out of swarm execute below commond in worker node
docker swarm leave

# Remove node from Manager
docker node rm <nodename>



version: '3.1'
  
services:
  springboot:
    image: dockerhandson/spring-boot-mongo:latest
    restart: always
    container_name: springboot
    ports:
      - 8182:8080
    working_dir: /opt/app
    depends_on:
      - mongo
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure

  mongo:
    image: mongo
    container_name: mongo
    volumes:
      - data:/data/db
      - data-bkp:/data/bkp
    restart: always

volumes:
    data:
      external: true
    data-bkp:
      external: true
	  
networks:
  default:
    external:
      name: flipkartoverlay


